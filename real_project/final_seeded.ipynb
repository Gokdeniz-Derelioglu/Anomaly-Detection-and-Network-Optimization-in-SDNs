{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24f81820",
   "metadata": {},
   "source": [
    "# Final Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cb8d36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, numpy as np, torch\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "# Python\n",
    "random.seed(SEED)\n",
    "\n",
    "# NumPy\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Torch\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28159035",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: derelioglugokdeniz (derelioglugokdeniz-bilkent-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\gokde\\OneDrive\\Masaüstü\\Anomaly-Detection-and-Network-Optimization-in-SDNs\\real_project\\wandb\\run-20250902_114617-gniy7pjk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/AnomalyDetectionInSDNs/runs/gniy7pjk' target=\"_blank\">experiment_run</a></strong> to <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/AnomalyDetectionInSDNs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/AnomalyDetectionInSDNs' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/AnomalyDetectionInSDNs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/AnomalyDetectionInSDNs/runs/gniy7pjk' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/AnomalyDetectionInSDNs/runs/gniy7pjk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/derelioglugokdeniz-bilkent-university/AnomalyDetectionInSDNs/runs/gniy7pjk?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x29e53930690>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "import os\n",
    "\n",
    "# Set your API key here\n",
    "os.environ[\"WANDB_API_KEY\"] = \"4cac7a348a78f711d2890b70c3252efbe9c16fe5\"\n",
    "\n",
    "wandb.init(\n",
    "    project=\"AnomalyDetectionInSDNs\",\n",
    "    name=\"experiment_run\",\n",
    "    reinit=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df655de2",
   "metadata": {},
   "source": [
    "# Hopefully the FINAL notebook I'll need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7b528c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "151b693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_csv_files(folder_path, output_file=\"merged.csv\"):\n",
    "    csv_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "    dfs = []\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)\n",
    "        df.columns = [col.strip() for col in df.columns]\n",
    "        dfs.append(df)\n",
    "    merged_df = pd.concat(dfs, ignore_index=True)\n",
    "    merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "# Use correct relative path for CICIDS2017 folder\n",
    "merge_csv_files(\"../CICIDS2017\", \"merged.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c96479",
   "metadata": {},
   "source": [
    "need to get this part to only include the top 50 features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcbb6e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 50 selected features:\n",
      "['Destination Port', 'Init_Win_bytes_forward', 'Max Packet Length', 'Average Packet Size', 'Bwd Packet Length Max', 'Init_Win_bytes_backward', 'Min Packet Length', 'Bwd Packet Length Min', 'Packet Length Std', 'Subflow Fwd Bytes', 'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Total Length of Fwd Packets', 'Avg Fwd Segment Size', 'Avg Bwd Segment Size', 'Packet Length Mean', 'Packet Length Variance', 'Fwd Packet Length Mean', 'Fwd Header Length.1', 'Fwd Header Length', 'Total Length of Bwd Packets', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Subflow Bwd Bytes', 'Bwd Header Length', 'act_data_pkt_fwd', 'Fwd IAT Min', 'Total Fwd Packets', 'Bwd Packets/s', 'min_seg_size_forward', 'Subflow Fwd Packets', 'Fwd IAT Std', 'URG Flag Count', 'ACK Flag Count', 'Flow IAT Max', 'Flow Bytes/s', 'Subflow Bwd Packets', 'Fwd IAT Max', 'Bwd IAT Total', 'Total Backward Packets', 'Flow Packets/s', 'Bwd IAT Max', 'Flow IAT Mean', 'PSH Flag Count', 'Flow IAT Std', 'Fwd Packet Length Std', 'Fwd IAT Total', 'Idle Min', 'Fwd IAT Mean', 'Flow IAT Min']\n",
      "\n",
      "Final shape: (1162445, 51)\n",
      "Number of features: 50\n",
      "First few feature names: ['Destination Port', 'Init_Win_bytes_forward', 'Max Packet Length', 'Average Packet Size', 'Bwd Packet Length Max', 'Init_Win_bytes_backward', 'Min Packet Length', 'Bwd Packet Length Min', 'Packet Length Std', 'Subflow Fwd Bytes']\n",
      "Label distribution:\n",
      " Label\n",
      "0    664425\n",
      "1    498020\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# ----------------------------\n",
    "# SETTINGS\n",
    "# ----------------------------\n",
    "file_path = \"merged.csv\"\n",
    "fraction = 0.85     # % of data to sample overall (keeps earliest fraction, preserves timeline)\n",
    "seq_length = 10     # default LSTM sequence length (adjust as needed)\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Load in chunks (preserve order)\n",
    "# ----------------------------\n",
    "chunks = []\n",
    "chunk_size = 10_000\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "    # Strip whitespace from column names\n",
    "    chunk.columns = chunk.columns.str.strip()\n",
    "\n",
    "    # Downcast numerics\n",
    "    for col in chunk.select_dtypes(include=['int', 'float']).columns:\n",
    "        if pd.api.types.is_integer_dtype(chunk[col]):\n",
    "            chunk[col] = pd.to_numeric(chunk[col], downcast='integer')\n",
    "        else:\n",
    "            chunk[col] = pd.to_numeric(chunk[col], downcast='float')\n",
    "\n",
    "    # Replace NaN/Inf inside chunk\n",
    "    chunk = chunk.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "    chunks.append(chunk)\n",
    "\n",
    "data = pd.concat(chunks, ignore_index=True)\n",
    "del chunks  # free memory\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Label to binary\n",
    "# ----------------------------\n",
    "if \"Label\" not in data.columns:\n",
    "    raise KeyError(\"Expected a 'Label' column.\")\n",
    "\n",
    "data[\"Label\"] = data[\"Label\"].apply(\n",
    "    lambda x: 0 if str(x).strip().upper() == \"BENIGN\" else 1\n",
    ").astype(np.int32)\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Optional overall fraction (preserves order)\n",
    "# ----------------------------\n",
    "if fraction < 1.0:\n",
    "    n_sample = max(1, int(len(data) * fraction))\n",
    "    data = data.iloc[:n_sample].copy()\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Time-aware benign downsampling (keeps chronology)\n",
    "# ----------------------------\n",
    "def time_aware_downsample_benign(df, label_col=\"Label\", max_benign_ratio=0.55):\n",
    "    keep_mask = np.zeros(len(df), dtype=bool)\n",
    "    benign_kept = 0\n",
    "    attack_kept = 0\n",
    "\n",
    "    label = df[label_col].to_numpy()\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        if label[i] == 1:\n",
    "            keep_mask[i] = True\n",
    "            attack_kept += 1\n",
    "        else:\n",
    "            proposed_benign = benign_kept + 1\n",
    "            proposed_total  = proposed_benign + attack_kept\n",
    "            if proposed_total == 0:\n",
    "                keep_mask[i] = True\n",
    "                benign_kept += 1\n",
    "            else:\n",
    "                if (proposed_benign / proposed_total) <= max_benign_ratio:\n",
    "                    keep_mask[i] = True\n",
    "                    benign_kept += 1\n",
    "\n",
    "    kept = df.loc[keep_mask]\n",
    "    return kept\n",
    "\n",
    "def time_aware_stratified_downsample(df, label_col=\"Label\", max_benign_ratio=0.55, \n",
    "                                   time_window_size=1000):\n",
    "    \"\"\"\n",
    "    Downsample benign samples while maintaining both chronology AND \n",
    "    better class balance across time windows.\n",
    "    \"\"\"\n",
    "    keep_mask = np.zeros(len(df), dtype=bool)\n",
    "    label = df[label_col].to_numpy()\n",
    "    \n",
    "    # Process in time windows to ensure stratification across time\n",
    "    for window_start in range(0, len(df), time_window_size):\n",
    "        window_end = min(window_start + time_window_size, len(df))\n",
    "        window_labels = label[window_start:window_end]\n",
    "        \n",
    "        # Count attacks in this window\n",
    "        attack_count = np.sum(window_labels == 1)\n",
    "        benign_count = np.sum(window_labels == 0)\n",
    "        \n",
    "        # Calculate how many benign samples to keep in this window\n",
    "        if attack_count == 0:\n",
    "            # If no attacks, keep some benign samples\n",
    "            benign_to_keep = min(benign_count, int(time_window_size * (1 - max_benign_ratio)))\n",
    "        else:\n",
    "            # Calculate based on desired ratio\n",
    "            total_attacks_kept = attack_count\n",
    "            max_benign_to_keep = int(total_attacks_kept * max_benign_ratio / (1 - max_benign_ratio))\n",
    "            benign_to_keep = min(benign_count, max_benign_to_keep)\n",
    "        \n",
    "        # Keep all attacks in this window\n",
    "        attack_indices = np.where(window_labels == 1)[0] + window_start\n",
    "        keep_mask[attack_indices] = True\n",
    "        \n",
    "        # Keep chronologically first benign samples in this window\n",
    "        benign_indices = np.where(window_labels == 0)[0] + window_start\n",
    "        if len(benign_indices) > 0 and benign_to_keep > 0:\n",
    "            keep_benign = benign_indices[:benign_to_keep]  # Keep first ones chronologically\n",
    "            keep_mask[keep_benign] = True\n",
    "    \n",
    "    return df.loc[keep_mask].reset_index(drop=True)\n",
    "\n",
    "# Apply chronology-safe downsampling\n",
    "data = time_aware_stratified_downsample(data, label_col=\"Label\", max_benign_ratio=0.55)\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Cap extreme values (exclude the label)\n",
    "# ----------------------------\n",
    "numeric_cols = [c for c in data.select_dtypes(include=[np.number]).columns if c != \"Label\"]\n",
    "for col in numeric_cols:\n",
    "    cap_value = data[col].quantile(0.999)\n",
    "    data[col] = np.clip(data[col], a_min=None, a_max=cap_value)\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Feature selection: keep only TOP 50 features\n",
    "# ----------------------------\n",
    "X = data.drop(columns=[\"Label\"])\n",
    "y = data[\"Label\"]\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    class_weight=\"balanced\"\n",
    ")\n",
    "rf.fit(X, y)\n",
    "\n",
    "importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "top_features = importances.sort_values(ascending=False).head(50).index.tolist()\n",
    "\n",
    "print(\"\\nTop 50 selected features:\")\n",
    "print(top_features)\n",
    "\n",
    "data = data[top_features + [\"Label\"]]\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Dtypes for ML (features float32, label int32)\n",
    "# ----------------------------\n",
    "for col in top_features:\n",
    "    data[col] = data[col].astype(np.float32)\n",
    "data[\"Label\"] = data[\"Label\"].astype(np.int32)\n",
    "\n",
    "# ----------------------------\n",
    "# Save final reduced dataset\n",
    "# ----------------------------\n",
    "data.to_csv(\"new_merged.csv\", index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# Final assignment for downstream code\n",
    "# ----------------------------\n",
    "datadf = data.copy()\n",
    "ftnames = [c.strip() for c in datadf.columns if c.strip() != \"Label\"]\n",
    "\n",
    "print(f\"\\nFinal shape: {datadf.shape}\")\n",
    "print(f\"Number of features: {len(ftnames)}\")\n",
    "print(\"First few feature names:\", ftnames[:10])\n",
    "print(\"Label distribution:\\n\", datadf['Label'].value_counts())\n",
    "\n",
    "# ======================================================================\n",
    "# Chronology-safe sequence utilities (NO shuffling, NO stratify anywhere)\n",
    "# ======================================================================\n",
    "\n",
    "def create_sequences(X, y, sequence_length=10, label_strategy='last'):\n",
    "    \"\"\"\n",
    "    Create overlapping sequences without breaking chronology.\n",
    "    label_strategy: 'last' | 'majority' | 'any_attack'\n",
    "    \"\"\"\n",
    "    if len(X) < sequence_length:\n",
    "        sequence_length = len(X)\n",
    "        print(f\"[create_sequences] Adjusted sequence_length to {sequence_length}\")\n",
    "\n",
    "    X_sequences, y_sequences = [], []\n",
    "    for i in range(len(X) - sequence_length + 1):\n",
    "        X_seq = X[i:i+sequence_length]\n",
    "        y_seq = y[i:i+sequence_length]\n",
    "        X_sequences.append(X_seq)\n",
    "\n",
    "        if label_strategy == 'last':\n",
    "            y_sequences.append(y_seq[-1])\n",
    "        elif label_strategy == 'majority':\n",
    "            y_sequences.append(1 if np.sum(y_seq) > (len(y_seq) // 2) else 0)\n",
    "        elif label_strategy == 'any_attack':\n",
    "            y_sequences.append(1 if (y_seq == 1).any() else 0)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown label_strategy: {label_strategy}\")\n",
    "\n",
    "    X_sequences = np.asarray(X_sequences, dtype=np.float32)\n",
    "    y_sequences = np.asarray(y_sequences, dtype=np.float32)\n",
    "\n",
    "    return X_sequences, y_sequences\n",
    "\n",
    "def chrono_split_train_val_test(df, label_col=\"Label\", train_ratio=0.7, val_ratio=0.15):\n",
    "    \"\"\"\n",
    "    Chronological split on rows (NO shuffle). Test gets the tail.\n",
    "    \"\"\"\n",
    "    n = len(df)\n",
    "    n_train = int(n * train_ratio)\n",
    "    n_val = int(n * val_ratio)\n",
    "    n_test = n - n_train - n_val\n",
    "\n",
    "    train_df = df.iloc[:n_train]\n",
    "    val_df   = df.iloc[n_train:n_train+n_val]\n",
    "    test_df  = df.iloc[n_train+n_val:]\n",
    "\n",
    "    def xy(d):\n",
    "        X = d.drop(columns=[label_col]).to_numpy(dtype=np.float32)\n",
    "        y = d[label_col].to_numpy(dtype=np.int32)\n",
    "        return X, y\n",
    "\n",
    "    return xy(train_df), xy(val_df), xy(test_df)\n",
    "\n",
    "def build_chrono_sequences(df, label_col=\"Label\", sequence_length=10, label_strategy='last'):\n",
    "    \"\"\"\n",
    "    Split chronologically on raw rows, then build sequences inside each split so\n",
    "    no sequence crosses split boundaries.\n",
    "    \"\"\"\n",
    "    (X_tr, y_tr), (X_va, y_va), (X_te, y_te) = chrono_split_train_val_test(df, label_col=label_col)\n",
    "\n",
    "    X_train_seq, y_train_seq = create_sequences(X_tr, y_tr, sequence_length, label_strategy)\n",
    "    X_val_seq,   y_val_seq   = create_sequences(X_va, y_va, sequence_length, label_strategy)\n",
    "    X_test_seq,  y_test_seq  = create_sequences(X_te, y_te, sequence_length, label_strategy)\n",
    "\n",
    "    # Quick sanity print\n",
    "    def _dist(y):\n",
    "        c = Counter(y.astype(int).tolist())\n",
    "        total = len(y)\n",
    "        if total == 0:\n",
    "            return {}\n",
    "        return {k: f\"{v} ({v/total:.2%})\" for k, v in sorted(c.items())}\n",
    "\n",
    "    print(\"\\n=== Chronological split (sequence-level) ===\")\n",
    "    print(f\"Train seq: {X_train_seq.shape}, dist: { _dist(y_train_seq) }\")\n",
    "    print(f\"Val   seq: {X_val_seq.shape}, dist: { _dist(y_val_seq) }\")\n",
    "    print(f\"Test  seq: {X_test_seq.shape}, dist: { _dist(y_test_seq) }\")\n",
    "\n",
    "    return X_train_seq, y_train_seq, X_val_seq, y_val_seq, X_test_seq, y_test_seq\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "def build_chrono_kfold_sequences(df, label_col=\"Label\", sequence_length=10, \n",
    "                                label_strategy='last', n_splits=5):\n",
    "    \"\"\"\n",
    "    K-fold cross-validation that maintains chronological order.\n",
    "    Each fold uses progressively more past data for training.\n",
    "    \"\"\"\n",
    "    # Convert to numpy arrays first\n",
    "    X_full = df.drop(columns=[label_col]).to_numpy(dtype=np.float32)\n",
    "    y_full = df[label_col].to_numpy(dtype=np.int32)\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    fold_data = []\n",
    "    \n",
    "    print(f\"\\n=== Chronological K-Fold ({n_splits} splits) ===\")\n",
    "    \n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(tscv.split(X_full)):\n",
    "        # Maintain chronological order (TimeSeriesSplit already does this)\n",
    "        X_train_fold = X_full[train_idx]\n",
    "        y_train_fold = y_full[train_idx]\n",
    "        X_val_fold = X_full[val_idx]\n",
    "        y_val_fold = y_full[val_idx]\n",
    "        \n",
    "        # Create sequences within each fold (no cross-contamination)\n",
    "        X_train_seq, y_train_seq = create_sequences(X_train_fold, y_train_fold, \n",
    "                                                   sequence_length, label_strategy)\n",
    "        X_val_seq, y_val_seq = create_sequences(X_val_fold, y_val_fold, \n",
    "                                               sequence_length, label_strategy)\n",
    "        \n",
    "        # Distribution info\n",
    "        def _dist(y):\n",
    "            c = Counter(y.astype(int).tolist())\n",
    "            total = len(y)\n",
    "            if total == 0:\n",
    "                return {}\n",
    "            return {k: f\"{v} ({v/total:.2%})\" for k, v in sorted(c.items())}\n",
    "        \n",
    "        print(f\"Fold {fold_idx+1}: Train={X_train_seq.shape}, Val={X_val_seq.shape}\")\n",
    "        print(f\"  Train dist: {_dist(y_train_seq)}\")\n",
    "        print(f\"  Val dist: {_dist(y_val_seq)}\")\n",
    "        \n",
    "        fold_data.append({\n",
    "            'fold': fold_idx,\n",
    "            'X_train': X_train_seq,\n",
    "            'y_train': y_train_seq,\n",
    "            'X_val': X_val_seq,\n",
    "            'y_val': y_val_seq\n",
    "        })\n",
    "    \n",
    "    return fold_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59fa15c",
   "metadata": {},
   "source": [
    "chunking -> dataset large, loading it all at once is not possible with our hardware\n",
    "preserving order -> this dataset is time-sensitive so we need to maintain the order\n",
    "labels -> string to int\n",
    "fractioning was something done to try out with less data at first (found out how much it can change later on in the project)\n",
    "downsampling -> too many benign samples\n",
    "RF used to determine most important 50 features\n",
    "\n",
    "\"chrono-sequences\" -> LSTM looks at past sequences. sequences must be split in order\n",
    "K-Fold -> splits data randomly into chunks but we needed chronological sequences. So we did that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79bb2633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, average_precision_score, roc_curve, precision_recall_curve,\n",
    "    classification_report, accuracy_score\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import optuna\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b267374b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_data_info(X, y, stage_name):\n",
    "    \"\"\"Print comprehensive data information\"\"\"\n",
    "    print(f\"\\n=== {stage_name} Data Info ===\")\n",
    "    print(f\"Feature matrix shape: {X.shape}\")\n",
    "    print(f\"Labels shape: {y.shape}\")\n",
    "    print(f\"Feature data type: {X.dtype}\")\n",
    "    print(f\"Label data type: {y.dtype}\")\n",
    "    \n",
    "    # Check for NaN/inf values\n",
    "    nan_count = np.isnan(X).sum()\n",
    "    inf_count = np.isinf(X).sum()\n",
    "    print(f\"NaN values in features: {nan_count}\")\n",
    "    print(f\"Inf values in features: {inf_count}\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"Feature matrix - Min: {X.min():.4f}, Max: {X.max():.4f}, Mean: {X.mean():.4f}\")\n",
    "    \n",
    "    # Label distribution\n",
    "    unique_labels, counts = np.unique(y, return_counts=True)\n",
    "    print(f\"Label distribution:\")\n",
    "    for label, count in zip(unique_labels, counts):\n",
    "        percentage = (count / len(y)) * 100\n",
    "        print(f\"  Class {int(label)}: {count} samples ({percentage:.2f}%)\")\n",
    "    \n",
    "    # Sample some labels\n",
    "    print(f\"First 20 labels: {y[:20]}\")\n",
    "    print(f\"Last 20 labels: {y[-20:]}\")\n",
    "\n",
    "def validate_data_splits(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    \"\"\"Validate that data splits preserve class distribution\"\"\"\n",
    "    print(\"\\n=== Data Split Validation ===\")\n",
    "    \n",
    "    # Check shapes\n",
    "    print(f\"Original total samples: {len(X_train) + len(X_val) + len(X_test)}\")\n",
    "    print(f\"Train: {X_train.shape[0]} ({X_train.shape[0]/(len(X_train) + len(X_val) + len(X_test))*100:.1f}%)\")\n",
    "    print(f\"Val: {X_val.shape[0]} ({X_val.shape[0]/(len(X_train) + len(X_val) + len(X_test))*100:.1f}%)\")\n",
    "    print(f\"Test: {X_test.shape[0]} ({X_test.shape[0]/(len(X_train) + len(X_val) + len(X_test))*100:.1f}%)\")\n",
    "    \n",
    "    # Check class distributions\n",
    "    datasets = [(\"Train\", y_train), (\"Val\", y_val), (\"Test\", y_test)]\n",
    "    \n",
    "    print(\"\\nClass distributions across splits:\")\n",
    "    for name, y_split in datasets:\n",
    "        unique_labels, counts = np.unique(y_split, return_counts=True)\n",
    "        print(f\"{name}:\")\n",
    "        for label, count in zip(unique_labels, counts):\n",
    "            percentage = (count / len(y_split)) * 100\n",
    "            print(f\"  Class {int(label)}: {count} samples ({percentage:.2f}%)\")\n",
    "    \n",
    "    # Check for data leakage indicators\n",
    "    print(f\"\\nFeature statistics consistency check:\")\n",
    "    print(f\"Train mean: {X_train.mean():.6f}\")\n",
    "    print(f\"Val mean: {X_val.mean():.6f}\")\n",
    "    print(f\"Test mean: {X_test.mean():.6f}\")\n",
    "\n",
    "def check_preprocessing_integrity(X_before, y_before, X_after, y_after, stage_name):\n",
    "    \"\"\"Check if preprocessing preserved data integrity\"\"\"\n",
    "    print(f\"\\n=== {stage_name} Preprocessing Integrity Check ===\")\n",
    "    \n",
    "    # Shape consistency\n",
    "    assert X_before.shape[0] == X_after.shape[0], f\"Sample count mismatch: {X_before.shape[0]} vs {X_after.shape[0]}\"\n",
    "    assert len(y_before) == len(y_after), f\"Label count mismatch: {len(y_before)} vs {len(y_after)}\"\n",
    "    print(\"✓ Sample counts preserved\")\n",
    "    \n",
    "    # Label consistency\n",
    "    assert np.array_equal(y_before, y_after), \"Labels were modified during preprocessing!\"\n",
    "    print(\"✓ Labels preserved\")\n",
    "    \n",
    "    # Feature scaling check\n",
    "    if stage_name == \"Scaling\":\n",
    "        print(f\"Before scaling - Min: {X_before.min():.4f}, Max: {X_before.max():.4f}\")\n",
    "        print(f\"After scaling - Min: {X_after.min():.4f}, Max: {X_after.max():.4f}\")\n",
    "        print(f\"After scaling - Mean: {X_after.mean():.6f}, Std: {X_after.std():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13ed8c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineEvaluator:\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "    \n",
    "    def add_dummy_classifier(self, X_train, y_train):\n",
    "        \"\"\"Add majority class predictor\"\"\"\n",
    "        print(\"Adding Majority Class Predictor...\")\n",
    "        self.models['majority_class'] = DummyClassifier(strategy='most_frequent', random_state=42)\n",
    "        self.models['majority_class'].fit(X_train, y_train)\n",
    "    \n",
    "    def add_logistic_regression(self, X_train, y_train):\n",
    "        \"\"\"Add logistic regression baseline\"\"\"\n",
    "        print(\"Adding Logistic Regression...\")\n",
    "        self.models['logistic_regression'] = LogisticRegression(\n",
    "            random_state=42, \n",
    "            max_iter=1000,\n",
    "            class_weight='balanced'\n",
    "        )\n",
    "        self.models['logistic_regression'].fit(X_train, y_train)\n",
    "    \n",
    "    def add_random_forest(self, X_train, y_train):\n",
    "        \"\"\"Add random forest baseline\"\"\"\n",
    "        print(\"Adding Random Forest...\")\n",
    "        self.models['random_forest'] = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            random_state=42,\n",
    "            class_weight='balanced',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        self.models['random_forest'].fit(X_train, y_train)\n",
    "    \n",
    "    def add_knn(self, X_train, y_train):\n",
    "        \"\"\"Add KNN baseline\"\"\"\n",
    "        print(\"Adding KNN...\")\n",
    "        # Use smaller sample for KNN if dataset is too large\n",
    "        if len(X_train) > 10000:\n",
    "            print(f\"Using subset of {min(5000, len(X_train))} samples for KNN training...\")\n",
    "            indices = np.random.choice(len(X_train), min(5000, len(X_train)), replace=False)\n",
    "            X_train_knn = X_train[indices]\n",
    "            y_train_knn = y_train[indices]\n",
    "        else:\n",
    "            X_train_knn = X_train\n",
    "            y_train_knn = y_train\n",
    "            \n",
    "        self.models['knn'] = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
    "        self.models['knn'].fit(X_train_knn, y_train_knn)\n",
    "    \n",
    "    def evaluate_all(self, X_test, y_test, model_name_prefix=\"Baseline\"):\n",
    "        \"\"\"Evaluate all baseline models\"\"\"\n",
    "        print(f\"\\n=== {model_name_prefix} Model Evaluation ===\")\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            print(f\"\\nEvaluating {name.replace('_', ' ').title()}...\")\n",
    "            \n",
    "            # Predictions\n",
    "            try:\n",
    "                y_pred = model.predict(X_test)\n",
    "                y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else y_pred\n",
    "            except Exception as e:\n",
    "                print(f\"Error with {name}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "            recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "            f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "            \n",
    "            # AUC calculation\n",
    "            try:\n",
    "                if len(np.unique(y_test)) > 1:\n",
    "                    auc = roc_auc_score(y_test, y_prob)\n",
    "                    pr_auc = average_precision_score(y_test, y_prob)\n",
    "                else:\n",
    "                    auc = pr_auc = 0.5\n",
    "            except:\n",
    "                auc = pr_auc = 0.5\n",
    "            \n",
    "            # Store results\n",
    "            self.results[name] = {\n",
    "                'accuracy': accuracy,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'auc': auc,\n",
    "                'pr_auc': pr_auc,\n",
    "                'y_pred': y_pred,\n",
    "                'y_prob': y_prob\n",
    "            }\n",
    "            \n",
    "            # Print results\n",
    "            print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "            print(f\"  Precision: {precision:.4f}\")\n",
    "            print(f\"  Recall:    {recall:.4f}\")\n",
    "            print(f\"  F1 Score:  {f1:.4f}\")\n",
    "            print(f\"  AUC:       {auc:.4f}\")\n",
    "            print(f\"  PR-AUC:    {pr_auc:.4f}\")\n",
    "    \n",
    "    def plot_confusion_matrices(self, y_test):\n",
    "        \"\"\"Plot confusion matrices for all models with robust error handling\"\"\"\n",
    "        n_models = len(self.results)  # Use results, not models\n",
    "        if n_models == 0:\n",
    "            print(\"No model results available for confusion matrix plotting\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Plotting confusion matrices for {n_models} models...\")\n",
    "        \n",
    "        # Try the subplot approach first\n",
    "        try:\n",
    "            cols = min(3, n_models)\n",
    "            rows = (n_models + cols - 1) // cols\n",
    "            \n",
    "            fig, axes = plt.subplots(rows, cols, figsize=(5*cols, 4*rows))\n",
    "            \n",
    "            # Convert to 2D array for consistent indexing\n",
    "            if n_models == 1:\n",
    "                axes = np.array([[axes]])\n",
    "            elif rows == 1:\n",
    "                axes = axes.reshape(1, -1)\n",
    "            elif cols == 1:\n",
    "                axes = axes.reshape(-1, 1)\n",
    "            \n",
    "            plot_idx = 0\n",
    "            for name, results in self.results.items():\n",
    "                try:\n",
    "                    row = plot_idx // cols\n",
    "                    col = plot_idx % cols\n",
    "                    \n",
    "                    ax = axes[row, col]\n",
    "                    \n",
    "                    cm = confusion_matrix(y_test, results['y_pred'])\n",
    "                    sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap='Blues')\n",
    "                    ax.set_title(f'{name.replace(\"_\", \" \").title()}\\nF1: {results[\"f1\"]:.3f}, AUC: {results[\"auc\"]:.3f}')\n",
    "                    ax.set_xlabel('Predicted')\n",
    "                    ax.set_ylabel('Actual')\n",
    "                    \n",
    "                    plot_idx += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error plotting confusion matrix for {name}: {e}\")\n",
    "                    plot_idx += 1\n",
    "                    continue\n",
    "            \n",
    "            # Hide empty subplots\n",
    "            for idx in range(n_models, rows * cols):\n",
    "                try:\n",
    "                    row = idx // cols\n",
    "                    col = idx % cols\n",
    "                    axes[row, col].axis('off')\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            print(\"✅ Confusion matrices plotted successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Subplot approach failed: {e}\")\n",
    "            print(\"Falling back to individual plots...\")\n",
    "            \n",
    "            # Fallback: individual plots\n",
    "            try:\n",
    "                for name, results in self.results.items():\n",
    "                    try:\n",
    "                        plt.figure(figsize=(6, 4))\n",
    "                        cm = confusion_matrix(y_test, results['y_pred'])\n",
    "                        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "                        plt.title(f'{name.replace(\"_\", \" \").title()}\\nF1: {results[\"f1\"]:.3f}, AUC: {results[\"auc\"]:.3f}')\n",
    "                        plt.xlabel('Predicted')\n",
    "                        plt.ylabel('Actual')\n",
    "                        plt.tight_layout()\n",
    "                        plt.show()\n",
    "                        print(f\"✅ Confusion matrix for {name} plotted\")\n",
    "                    except Exception as e2:\n",
    "                        print(f\"❌ Failed to plot confusion matrix for {name}: {e2}\")\n",
    "                        \n",
    "            except Exception as e3:\n",
    "                print(f\"❌ All plotting approaches failed: {e3}\")\n",
    "                print(\"Skipping confusion matrix plots...\")\n",
    "    \n",
    "    def get_results_summary(self):\n",
    "        \"\"\"Get summary of all baseline results\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No results available for summary\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "        summary_data = []\n",
    "        for name, results in self.results.items():\n",
    "            summary_data.append({\n",
    "                'Model': name.replace('_', ' ').title(),\n",
    "                'Accuracy': results['accuracy'],\n",
    "                'Precision': results['precision'],\n",
    "                'Recall': results['recall'],\n",
    "                'F1': results['f1'],\n",
    "                'AUC': results['auc'],\n",
    "                'PR-AUC': results['pr_auc']\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(summary_data).round(4)\n",
    "    \n",
    "    # Replace your plot_confusion_matrices method with this simple version\n",
    "    def plot_confusion_matrices(self, y_test):\n",
    "        \"\"\"Plot confusion matrices for all models - Simple version\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No results available for plotting\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Plotting confusion matrices for {len(self.results)} models...\")\n",
    "        \n",
    "        # Use simple individual plots to avoid matplotlib axes issues\n",
    "        for name, results in self.results.items():\n",
    "            try:\n",
    "                print(f\"Plotting confusion matrix for {name}...\")\n",
    "                \n",
    "                # Create a new figure for each model\n",
    "                plt.figure(figsize=(6, 5))\n",
    "                \n",
    "                # Calculate confusion matrix\n",
    "                cm = confusion_matrix(y_test, results['y_pred'])\n",
    "                \n",
    "                # Use matplotlib directly instead of seaborn to avoid axes issues\n",
    "                plt.imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "                plt.colorbar()\n",
    "                \n",
    "                # Add text annotations\n",
    "                for i in range(cm.shape[0]):\n",
    "                    for j in range(cm.shape[1]):\n",
    "                        plt.text(j, i, str(cm[i, j]), \n",
    "                                ha='center', va='center', \n",
    "                                color='white' if cm[i, j] > cm.max() / 2 else 'black',\n",
    "                                fontsize=14, fontweight='bold')\n",
    "                \n",
    "                # Labels and title\n",
    "                plt.title(f'{name.replace(\"_\", \" \").title()}\\nF1: {results[\"f1\"]:.3f}, AUC: {results[\"auc\"]:.3f}', \n",
    "                        fontsize=12, pad=20)\n",
    "                plt.xlabel('Predicted Label', fontsize=11)\n",
    "                plt.ylabel('True Label', fontsize=11)\n",
    "                \n",
    "                # Set tick labels\n",
    "                tick_labels = ['Benign', 'Attack']\n",
    "                plt.xticks(range(len(tick_labels)), tick_labels)\n",
    "                plt.yticks(range(len(tick_labels)), tick_labels)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error plotting confusion matrix for {name}: {e}\")\n",
    "                # Still try to show basic metrics\n",
    "                try:\n",
    "                    cm = confusion_matrix(y_test, results['y_pred'])\n",
    "                    print(f\"  Confusion Matrix for {name}:\")\n",
    "                    print(f\"    {cm}\")\n",
    "                except:\n",
    "                    print(f\"  Could not generate any visualization for {name}\")\n",
    "        \n",
    "        print(\"✅ Confusion matrix plotting completed\")\n",
    "    \n",
    "    def get_results_summary(self):\n",
    "        \"\"\"Get summary of all baseline results\"\"\"\n",
    "        if not self.results:\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "        summary_data = []\n",
    "        for name, results in self.results.items():\n",
    "            summary_data.append({\n",
    "                'Model': name.replace('_', ' ').title(),\n",
    "                'Accuracy': results['accuracy'],\n",
    "                'Precision': results['precision'],\n",
    "                'Recall': results['recall'],\n",
    "                'F1': results['f1'],\n",
    "                'AUC': results['auc'],\n",
    "                'PR-AUC': results['pr_auc']\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(summary_data).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62df0cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a7071be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# ==========================================================\n",
    "# Chronology-safe prepare_lstm_sequences_fixed\n",
    "# ==========================================================\n",
    "def prepare_lstm_sequences_fixed(X, y, sequence_length=SEQ_LENGTH, label_strategy='majority'):\n",
    "    \"\"\"\n",
    "    Convert tabular data to sequences for LSTM training with better label handling.\n",
    "    Chronology preserved (NO shuffling).\n",
    "    \"\"\"\n",
    "    print(f\"Creating sequences of length {sequence_length} with {label_strategy} labeling...\")\n",
    "\n",
    "    if len(X) < sequence_length:\n",
    "        sequence_length = len(X)\n",
    "        print(f\"Adjusted sequence length to {sequence_length} due to limited data\")\n",
    "\n",
    "    X_sequences = []\n",
    "    y_sequences = []\n",
    "\n",
    "    for i in range(len(X) - sequence_length + 1):\n",
    "        X_seq = X[i:i + sequence_length]\n",
    "        y_seq = y[i:i + sequence_length]\n",
    "        X_sequences.append(X_seq)\n",
    "\n",
    "        if label_strategy == 'last':\n",
    "            y_sequences.append(y_seq[-1])\n",
    "        elif label_strategy == 'majority':\n",
    "            y_sequences.append(1 if np.sum(y_seq) > len(y_seq) // 2 else 0)\n",
    "        elif label_strategy == 'any_attack':\n",
    "            y_sequences.append(1 if np.any(y_seq == 1) else 0)\n",
    "\n",
    "    X_sequences = np.array(X_sequences, dtype=np.float32)\n",
    "    y_sequences = np.array(y_sequences, dtype=np.int32)\n",
    "\n",
    "    print(f\"Created {len(X_sequences)} sequences\")\n",
    "    print(f\"Sequence shape: {X_sequences.shape}\")\n",
    "\n",
    "    # Distribution debug\n",
    "    original_dist = Counter(y)\n",
    "    sequence_dist = Counter(y_sequences)\n",
    "    print(f\"Original distribution: {dict(original_dist)}\")\n",
    "    print(f\"Sequence distribution: {dict(sequence_dist)}\")\n",
    "\n",
    "    return X_sequences, y_sequences\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# Chronology-safe create_stratified_sequences\n",
    "# (renamed internally but kept external name)\n",
    "# ==========================================================\n",
    "def create_stratified_sequences(X, y, sequence_length=SEQ_LENGTH, test_size=0.4, val_split=0.5, random_state=42):\n",
    "    \"\"\"\n",
    "    Chronology-safe split of sequences (replaces stratified split).\n",
    "    \"\"\"\n",
    "    print(\"Creating chronology-safe sequences (no stratify)...\")\n",
    "\n",
    "    # Step 1: build sequences\n",
    "    X_seq, y_seq = prepare_lstm_sequences_fixed(X, y, sequence_length, label_strategy='majority')\n",
    "\n",
    "    # Step 2: split chronologically\n",
    "    n_total = len(X_seq)\n",
    "    n_test = int(n_total * test_size)\n",
    "    n_val = int((n_total - n_test) * val_split)\n",
    "\n",
    "    train_end = n_total - n_test - n_val\n",
    "    val_end   = n_total - n_test\n",
    "\n",
    "    X_train_seq, y_train_seq = X_seq[:train_end], y_seq[:train_end]\n",
    "    X_val_seq,   y_val_seq   = X_seq[train_end:val_end], y_seq[train_end:val_end]\n",
    "    X_test_seq,  y_test_seq  = X_seq[val_end:], y_seq[val_end:]\n",
    "\n",
    "    validate_sequence_splits(X_train_seq, y_train_seq, X_val_seq, y_val_seq, X_test_seq, y_test_seq)\n",
    "\n",
    "    return X_train_seq, y_train_seq, X_val_seq, y_val_seq, X_test_seq, y_test_seq\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# validate_sequence_splits (unchanged, still works)\n",
    "# ==========================================================\n",
    "def validate_sequence_splits(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    \"\"\"Validate sequence splits maintain reasonable class distribution\"\"\"\n",
    "    print(\"\\n=== Sequence Split Validation ===\")\n",
    "    splits = [(\"Train\", y_train), (\"Val\", y_val), (\"Test\", y_test)]\n",
    "\n",
    "    for name, y_split in splits:\n",
    "        if len(y_split) > 0:\n",
    "            attack_ratio = np.sum(y_split) / len(y_split)\n",
    "            benign_count = len(y_split) - np.sum(y_split)\n",
    "            attack_count = np.sum(y_split)\n",
    "            print(f\"{name}: {benign_count} benign, {attack_count} attack (ratio: {attack_ratio:.3f})\")\n",
    "        else:\n",
    "            print(f\"{name}: Empty split!\")\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# get_balanced_class_weights (no change needed)\n",
    "# ==========================================================\n",
    "def get_balanced_class_weights(y_train):\n",
    "    \"\"\"Calculate balanced class weights with safety checks\"\"\"\n",
    "    class_counts = Counter(y_train)\n",
    "    print(f\"Training class counts: {dict(class_counts)}\")\n",
    "\n",
    "    if len(class_counts) < 2:\n",
    "        print(\"⚠️  WARNING: Only one class in training data!\")\n",
    "        return {0: 1.0, 1: 1.0}\n",
    "\n",
    "    total_samples = len(y_train)\n",
    "    n_classes = len(class_counts)\n",
    "    class_weights = {cls: total_samples / (n_classes * count) for cls, count in class_counts.items()}\n",
    "\n",
    "    print(f\"Calculated class weights: {class_weights}\")\n",
    "    return class_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82f5d8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, num_layers=2, dropout=0.3, \n",
    "                 use_attention=True, bidirectional=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.use_attention = use_attention\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        print(f\"Initializing AdvancedLSTM:\")\n",
    "        print(f\"  Input dim: {input_dim}\")\n",
    "        print(f\"  Hidden dim: {hidden_dim}\")\n",
    "        print(f\"  Num layers: {num_layers}\")\n",
    "        print(f\"  Dropout: {dropout}\")\n",
    "        print(f\"  Attention: {use_attention}\")\n",
    "        print(f\"  Bidirectional: {bidirectional}\")\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        \n",
    "        lstm_output_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        \n",
    "        # Attention mechanism\n",
    "        if use_attention:\n",
    "            self.attention = nn.MultiheadAttention(\n",
    "                embed_dim=lstm_output_dim,\n",
    "                num_heads=4,\n",
    "                dropout=dropout,\n",
    "                batch_first=True\n",
    "            )\n",
    "        \n",
    "        # Classification head with residual connection\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(lstm_output_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "        \n",
    "        # Count parameters\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        print(f\"  Total parameters: {total_params:,}\")\n",
    "        print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "        \n",
    "    def forward(self, x, return_features=False):\n",
    "        # LSTM forward pass\n",
    "        lstm_out, (hidden, cell) = self.lstm(x)\n",
    "        \n",
    "        if self.use_attention:\n",
    "            # Apply attention to all time steps\n",
    "            attended_out, attention_weights = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "            # Use mean of attended outputs\n",
    "            features = attended_out.mean(dim=1)\n",
    "        else:\n",
    "            # Use last time step\n",
    "            features = lstm_out[:, -1, :]\n",
    "\n",
    "        if return_features:\n",
    "            return features  # Return the hidden layer values (pre-classifier)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(features)\n",
    "        \n",
    "        return logits.squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfde727",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6475f94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_advanced_lstm_enhanced(X_train, y_train, X_val, y_val, params, model, epochs=30, min_delta=1e-4):\n",
    "    \"\"\"\n",
    "    Enhanced training with better overfitting prevention and chronology preservation.\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "    import numpy as np\n",
    "    from collections import Counter\n",
    "    from sklearn.metrics import roc_auc_score, f1_score\n",
    "\n",
    "    # Ensure float32 for GPU efficiency\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    X_val = X_val.astype(np.float32)\n",
    "    y_train = y_train.astype(np.float32)\n",
    "    y_val = y_val.astype(np.float32)\n",
    "\n",
    "    # Prepare datasets with NO shuffling (chronology preserved)\n",
    "    train_dataset = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "    val_dataset = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=params['batch_size'],\n",
    "        shuffle=False,  # CRITICAL: NO shuffling to preserve chronology\n",
    "        pin_memory=True,\n",
    "        num_workers=2,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=params['batch_size'],\n",
    "        shuffle=False,  # CRITICAL: NO shuffling\n",
    "        pin_memory=True,\n",
    "        num_workers=2,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "\n",
    "    # Optimizer with stronger weight decay if specified\n",
    "    weight_decay = params.get('l2_reg', 1e-5)\n",
    "    if params.get('optimizer') == 'adamw':\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(), \n",
    "            lr=params['lr'], \n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(), \n",
    "            lr=params['lr'],\n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "\n",
    "    # Conservative learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='max',  # Monitor validation F1 (maximize)\n",
    "        factor=0.5,  # Reduce LR by half\n",
    "        patience=params.get('reduce_lr_patience', 8),\n",
    "        verbose=True,\n",
    "        min_lr=params.get('min_lr', 1e-6)\n",
    "    )\n",
    "\n",
    "    # Loss function with class balancing\n",
    "    device = next(model.parameters()).device\n",
    "    class_counts = Counter(y_train)\n",
    "    \n",
    "    if len(class_counts) > 1:\n",
    "        pos_weight = torch.tensor([class_counts[0] / class_counts[1]], device=device)\n",
    "        print(f\"Using pos_weight: {pos_weight.item():.3f} (Benign/Attack ratio)\")\n",
    "    else:\n",
    "        pos_weight = torch.tensor([1.0], device=device)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    # Training monitoring\n",
    "    train_losses, val_aucs, val_f1s = [], [], []\n",
    "    best_val_f1 = 0\n",
    "    best_model_state = None\n",
    "    patience = params.get('early_stopping_patience', 20)\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # Overfitting detection variables\n",
    "    consecutive_loss_drops = 0\n",
    "    loss_drop_threshold = 0.9  # If loss drops by 90% in one epoch, flag it\n",
    "\n",
    "    print(f\"Training for max {epochs} epochs with early stopping (patience={patience})\")\n",
    "    print(f\"Batch size: {params['batch_size']}, Learning rate: {params['lr']}\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        batch_count = 0\n",
    "\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "\n",
    "            # Primary loss\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            # L2 regularization (if not using weight_decay in optimizer)\n",
    "            if params.get('l2_reg', 0) > 0 and params.get('optimizer') != 'adamw':\n",
    "                l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "                loss += params['l2_reg'] * l2_norm\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            clip_value = params.get('gradient_clip', 1.0)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_value)\n",
    "            \n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            batch_count += 1\n",
    "\n",
    "        avg_train_loss = epoch_loss / batch_count\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Overfitting detection: Check for suspiciously fast loss drops\n",
    "        if epoch > 0:\n",
    "            loss_drop_ratio = (train_losses[-2] - avg_train_loss) / train_losses[-2]\n",
    "            if loss_drop_ratio > loss_drop_threshold:\n",
    "                consecutive_loss_drops += 1\n",
    "                print(f\"WARNING: Large loss drop detected: {loss_drop_ratio:.1%}\")\n",
    "                if consecutive_loss_drops >= 2:\n",
    "                    print(\"ALERT: Multiple consecutive large loss drops - possible overfitting!\")\n",
    "            else:\n",
    "                consecutive_loss_drops = 0\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_probs, val_targets = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X = batch_X.to(device)\n",
    "                outputs = model(batch_X)\n",
    "                \n",
    "                # Check for NaN outputs\n",
    "                if torch.isnan(outputs).any():\n",
    "                    print(f\"WARNING: NaN detected in outputs at epoch {epoch+1}\")\n",
    "                    continue\n",
    "                \n",
    "                val_probs.extend(torch.sigmoid(outputs).cpu().numpy())\n",
    "                val_targets.extend(batch_y.numpy())\n",
    "\n",
    "        if len(val_probs) == 0:\n",
    "            print(f\"ERROR: No valid validation predictions at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "        val_probs = np.array(val_probs)\n",
    "        val_targets = np.array(val_targets)\n",
    "\n",
    "        # Calculate validation metrics\n",
    "        if len(np.unique(val_targets)) > 1:\n",
    "            val_auc = roc_auc_score(val_targets, val_probs)\n",
    "        else:\n",
    "            val_auc = 0.5\n",
    "            print(f\"WARNING: Only one class in validation set at epoch {epoch+1}\")\n",
    "\n",
    "        # Find best F1 threshold\n",
    "        thresholds = np.linspace(0.1, 0.9, 50)\n",
    "        f1_scores = []\n",
    "        for t in thresholds:\n",
    "            try:\n",
    "                f1 = f1_score(val_targets, (val_probs >= t).astype(int), zero_division=0)\n",
    "                f1_scores.append(f1)\n",
    "            except:\n",
    "                f1_scores.append(0.0)\n",
    "        \n",
    "        best_f1 = max(f1_scores) if f1_scores else 0.0\n",
    "        val_aucs.append(val_auc)\n",
    "        val_f1s.append(best_f1)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if best_f1 > best_val_f1 + min_delta:\n",
    "            best_val_f1 = best_f1\n",
    "            best_model_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        # Step the scheduler (monitoring validation F1)\n",
    "        scheduler.step(best_f1)\n",
    "\n",
    "        # Early stopping check\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1} (no improvement for {patience} epochs)\")\n",
    "            break\n",
    "\n",
    "        # Progress reporting\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch {epoch+1:2d}/{epochs} | \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "              f\"Val F1: {best_f1:.4f} | \"\n",
    "              f\"Val AUC: {val_auc:.4f} | \"\n",
    "              f\"LR: {current_lr:.2e}\")\n",
    "\n",
    "        # Additional overfitting warnings\n",
    "        if epoch >= 5 and best_f1 > 0.995:\n",
    "            print(\"WARNING: Suspiciously high validation F1 (>99.5%) - check for data leakage!\")\n",
    "        \n",
    "        if epoch >= 3 and avg_train_loss < 0.01:\n",
    "            print(\"WARNING: Very low training loss - possible overfitting\")\n",
    "\n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict({k: v.to(device) for k, v in best_model_state.items()})\n",
    "        print(f\"Loaded best model with validation F1: {best_val_f1:.4f}\")\n",
    "    else:\n",
    "        print(\"WARNING: No improvement found, using final epoch model\")\n",
    "    \n",
    "    # Save the best model to disk\n",
    "    torch.save(model.state_dict(), \"best_lstm_model.pth\")\n",
    "    print(\"✅ Best model saved to best_lstm_model.pth\")\n",
    "\n",
    "    return model, train_losses, val_aucs, val_f1s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cf59756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_objective(trial, X_train_seq, y_train_seq):\n",
    "    params = {\n",
    "        'hidden_dim': trial.suggest_categorical('hidden_dim', [64, 128, 256]),\n",
    "        'num_layers': trial.suggest_int('num_layers', 2, 4),\n",
    "        'dropout': trial.suggest_float('dropout', 0.2, 0.5),\n",
    "        'lr': trial.suggest_float('lr', 5e-5, 5e-3, log=True),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [32, 64, 128]),\n",
    "        'use_attention': trial.suggest_categorical('use_attention', [True, False]),\n",
    "        'bidirectional': trial.suggest_categorical('bidirectional', [True, False]),\n",
    "        'optimizer': trial.suggest_categorical('optimizer', ['adam', 'adamw']),\n",
    "        'l2_reg': trial.suggest_float('l2_reg', 1e-6, 1e-3, log=True),\n",
    "    }\n",
    "    \n",
    "    if params['optimizer'] == 'adamw':\n",
    "        params['weight_decay'] = trial.suggest_float('weight_decay', 1e-5, 1e-3, log=True)\n",
    "    \n",
    "    try:\n",
    "        # Use cross-validation for more robust evaluation\n",
    "        kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        cv_scores = []\n",
    "        \n",
    "        for train_idx, val_idx in kf.split(X_train_seq, y_train_seq):\n",
    "            X_cv_train, X_cv_val = X_train_seq[train_idx], X_train_seq[val_idx]\n",
    "            y_cv_train, y_cv_val = y_train_seq[train_idx], y_train_seq[val_idx]\n",
    "\n",
    "            # 🔹 Build a fresh model for each CV split\n",
    "            input_dim = X_train_seq.shape[2]\n",
    "            num_classes = len(np.unique(y_train_seq))\n",
    "            \n",
    "            model = AdvancedLSTM(\n",
    "                input_dim=input_dim,\n",
    "                hidden_dim=params['hidden_dim'],\n",
    "                num_layers=params['num_layers'],\n",
    "                dropout=params['dropout'],\n",
    "                use_attention=params['use_attention'],\n",
    "                bidirectional=params['bidirectional']\n",
    "            ).to(device)\n",
    "\n",
    "\n",
    "            _, _, _, val_f1s = train_advanced_lstm_enhanced(\n",
    "                X_cv_train, y_cv_train,\n",
    "                X_cv_val, y_cv_val,\n",
    "                params,\n",
    "                model,\n",
    "                epochs=40\n",
    "            )\n",
    "            \n",
    "            cv_scores.append(max(val_f1s))\n",
    "        \n",
    "        return np.mean(cv_scores)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Trial failed: {e}\")\n",
    "        return 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e628c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance(model, X_sample, feature_names):\n",
    "    \"\"\"Analyze which features are most important for predictions\"\"\"\n",
    "    # Store original model state\n",
    "    original_training_state = model.training\n",
    "    \n",
    "    # Set model to training mode for gradient computation\n",
    "    model.train()\n",
    "    \n",
    "    try:\n",
    "        # Use gradient-based feature importance\n",
    "        X_tensor = torch.FloatTensor(X_sample[:100]).to(device)  # Use subset for speed\n",
    "        X_tensor.requires_grad_(True)\n",
    "        \n",
    "        outputs = model(X_tensor)\n",
    "        \n",
    "        # Calculate gradients\n",
    "        outputs.sum().backward()\n",
    "        gradients = X_tensor.grad.abs().mean(dim=[0, 1]).cpu().numpy()\n",
    "        \n",
    "        # Create feature importance DataFrame\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': gradients\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        return importance_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in feature importance analysis: {e}\")\n",
    "        # Return dummy DataFrame in case of error\n",
    "        return pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': np.zeros(len(feature_names))\n",
    "        })\n",
    "    \n",
    "    finally:\n",
    "        # Restore original model state\n",
    "        model.train(original_training_state)\n",
    "\n",
    "def plot_advanced_results(train_losses, val_aucs, val_f1s, test_results):\n",
    "    \"\"\"Create comprehensive result plots\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    \n",
    "    # Training history\n",
    "    axes[0, 0].plot(train_losses)\n",
    "    axes[0, 0].set_title('Training Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[0, 1].plot(val_aucs, label='AUC', color='blue')\n",
    "    axes[0, 1].plot(val_f1s, label='F1', color='red')\n",
    "    axes[0, 1].set_title('Validation Metrics')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Score')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # ROC and PR curves\n",
    "    fpr, tpr, _ = roc_curve(test_results['y_true'], test_results['y_prob'])\n",
    "    axes[0, 2].plot(fpr, tpr, label=f'AUC = {test_results[\"auc\"]:.4f}')\n",
    "    axes[0, 2].plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "    axes[0, 2].set_title('ROC Curve')\n",
    "    axes[0, 2].set_xlabel('False Positive Rate')\n",
    "    axes[0, 2].set_ylabel('True Positive Rate')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    precision, recall, _ = precision_recall_curve(test_results['y_true'], test_results['y_prob'])\n",
    "    axes[1, 0].plot(recall, precision, label=f'PR-AUC = {test_results[\"pr_auc\"]:.4f}')\n",
    "    axes[1, 0].set_title('Precision-Recall Curve')\n",
    "    axes[1, 0].set_xlabel('Recall')\n",
    "    axes[1, 0].set_ylabel('Precision')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(test_results['y_true'], test_results['y_pred'])\n",
    "    sns.heatmap(cm, annot=True, fmt='d', ax=axes[1, 1], cmap='Blues')\n",
    "    axes[1, 1].set_title('Confusion Matrix')\n",
    "    axes[1, 1].set_xlabel('Predicted')\n",
    "    axes[1, 1].set_ylabel('Actual')\n",
    "    \n",
    "    # Threshold analysis\n",
    "    thresholds = np.linspace(0.1, 0.9, 100)\n",
    "    f1_scores = []\n",
    "    for t in thresholds:\n",
    "        try:\n",
    "            f1 = f1_score(test_results['y_true'], (test_results['y_prob'] >= t).astype(int))\n",
    "            f1_scores.append(f1)\n",
    "        except:\n",
    "            f1_scores.append(0.0)\n",
    "    \n",
    "    axes[1, 2].plot(thresholds, f1_scores)\n",
    "    axes[1, 2].axvline(x=test_results['best_threshold'], color='red', linestyle='--')\n",
    "    axes[1, 2].set_title('F1 Score vs Threshold')\n",
    "    axes[1, 2].set_xlabel('Threshold')\n",
    "    axes[1, 2].set_ylabel('F1 Score')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_model_comparison(baseline_results_df, lstm_results):\n",
    "    \"\"\"Plot comparison between baseline models and LSTM\"\"\"\n",
    "    # Add LSTM results to comparison\n",
    "    lstm_row = pd.DataFrame({\n",
    "        'Model': ['Advanced LSTM'],\n",
    "        'Accuracy': [lstm_results.get('accuracy', 0)],\n",
    "        'Precision': [lstm_results['precision']],\n",
    "        'Recall': [lstm_results['recall']],\n",
    "        'F1': [lstm_results['f1']],\n",
    "        'AUC': [lstm_results['auc']],\n",
    "        'PR-AUC': [lstm_results['pr_auc']]\n",
    "    })\n",
    "    \n",
    "    comparison_df = pd.concat([baseline_results_df, lstm_row], ignore_index=True)\n",
    "    \n",
    "    # Create comparison plots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    \n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1', 'AUC', 'PR-AUC']\n",
    "    colors = ['skyblue', 'lightgreen', 'lightcoral', 'gold', 'plum', 'lightpink']\n",
    "    \n",
    "    for i, (metric, color) in enumerate(zip(metrics, colors)):\n",
    "        row, col = i // 3, i % 3\n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        bars = ax.bar(comparison_df['Model'], comparison_df[metric], color=color, alpha=0.7)\n",
    "        ax.set_title(f'{metric} Comparison')\n",
    "        ax.set_ylabel(metric)\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Highlight best model\n",
    "        best_idx = comparison_df[metric].idxmax()\n",
    "        bars[best_idx].set_color('red')\n",
    "        bars[best_idx].set_alpha(0.9)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                   f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f9b38f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from tqdm import tqdm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba5f9384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 1: Data Loading and Initial Validation ===\n",
      "Loading CICIDS2017 dataset...\n",
      "Labels already appear to be binary\n",
      "Dataset loaded successfully!\n",
      "Total samples: 1162445\n",
      "Total features: 50\n",
      "Final class distribution after conversion:\n",
      "  Benign (label=0): 664425 samples\n",
      "  Attack (label=1): 498020 samples\n",
      "Attack ratio: 0.428\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# STEP 1: Data Loading and Initial Sanity Checks\n",
    "# ==============================\n",
    "print(\"\\n=== STEP 1: Data Loading and Initial Validation ===\")    \n",
    "try:\n",
    "    # Load data\n",
    "    print(\"Loading CICIDS2017 dataset...\")\n",
    "    df, feature_names = datadf, ftnames  # pre-loaded globals\n",
    "        \n",
    "    # Extract features and labels\n",
    "    if 'Label' in df.columns:\n",
    "        X = df.drop('Label', axis=1).values\n",
    "        y = df['Label'].values\n",
    "    elif 'label' in df.columns:\n",
    "        X = df.drop('label', axis=1).values\n",
    "        y = df['label'].values\n",
    "    else:\n",
    "        X = df.iloc[:, :-1].values\n",
    "        y = df.iloc[:, -1].values\n",
    "        \n",
    "    # Convert labels to binary - FIXED VERSION\n",
    "    if y.dtype == 'object' or len(np.unique(y)) > 2:\n",
    "        print(\"Converting labels to binary classification...\")\n",
    "        benign_labels = ['BENIGN', 'Normal', 'normal', 'benign', 0, '0']\n",
    "        \n",
    "        unique_labels, counts = np.unique(y, return_counts=True)\n",
    "        print(\"Original label distribution:\")\n",
    "        for label, count in zip(unique_labels, counts):\n",
    "            print(f\"  {label}: {count} samples\")\n",
    "            \n",
    "        # FIXED: Proper binary conversion using boolean indexing\n",
    "        y_binary = np.ones(len(y), dtype=int)  # Start with all as attack (1)\n",
    "        \n",
    "        # Create boolean mask for benign samples\n",
    "        benign_mask = np.isin(y, benign_labels)\n",
    "        y_binary[benign_mask] = 0  # Set benign samples to 0\n",
    "        \n",
    "        print(f\"Benign samples found: {np.sum(benign_mask)} out of {len(y)}\")\n",
    "        print(f\"Attack samples: {np.sum(~benign_mask)} out of {len(y)}\")\n",
    "        \n",
    "        y = y_binary\n",
    "        print(\"Converted to binary classification (0=Benign, 1=Attack)\")\n",
    "    else:\n",
    "        print(\"Labels already appear to be binary\")\n",
    "        \n",
    "    # Ensure numeric features\n",
    "    if X.dtype == 'object':\n",
    "        print(\"Converting features to numeric...\")\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        le = LabelEncoder()\n",
    "        for col in range(X.shape[1]):\n",
    "            if df.iloc[:, col].dtype == 'object':\n",
    "                X[:, col] = le.fit_transform(X[:, col].astype(str))\n",
    "        X = X.astype(float)\n",
    "        \n",
    "    # Data quality checks\n",
    "    nan_count = np.isnan(X).sum()\n",
    "    inf_count = np.isinf(X).sum()\n",
    "        \n",
    "    if nan_count > 0:\n",
    "        print(f\"Replacing {nan_count} NaN values with median...\")\n",
    "        from sklearn.impute import SimpleImputer\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        X = imputer.fit_transform(X)\n",
    "            \n",
    "    if inf_count > 0:\n",
    "        print(f\"Replacing {inf_count} infinite values...\")\n",
    "        X = np.nan_to_num(X, nan=0.0, posinf=1e10, neginf=-1e10)\n",
    "        \n",
    "    print(f\"Dataset loaded successfully!\")\n",
    "    print(f\"Total samples: {len(X)}\")\n",
    "    print(f\"Total features: {X.shape[1]}\")\n",
    "        \n",
    "    # FIXED: More robust class distribution check\n",
    "    unique_labels, counts = np.unique(y, return_counts=True)\n",
    "    print(f\"Final class distribution after conversion:\")\n",
    "    for label, count in zip(unique_labels, counts):\n",
    "        class_name = \"Benign\" if label == 0 else \"Attack\"\n",
    "        print(f\"  {class_name} (label={label}): {count} samples\")\n",
    "    \n",
    "    # Calculate attack ratio safely\n",
    "    if len(counts) >= 2:\n",
    "        benign_count = counts[unique_labels == 0][0] if 0 in unique_labels else 0\n",
    "        attack_count = counts[unique_labels == 1][0] if 1 in unique_labels else 0\n",
    "        attack_ratio = attack_count / len(y)\n",
    "    else:\n",
    "        # Only one class present\n",
    "        if unique_labels[0] == 0:\n",
    "            benign_count, attack_count = counts[0], 0\n",
    "        else:\n",
    "            benign_count, attack_count = 0, counts[0]\n",
    "        attack_ratio = attack_count / len(y)\n",
    "    \n",
    "    print(f\"Attack ratio: {attack_ratio:.3f}\")\n",
    "        \n",
    "    if attack_ratio < 0.01:\n",
    "        print(f\"⚠️  SEVERE CLASS IMBALANCE: Only {attack_ratio:.3%} attacks!\")\n",
    "    elif attack_ratio > 0.99:\n",
    "        print(f\"⚠️  SEVERE CLASS IMBALANCE: Only {(1-attack_ratio):.3%} benign!\")\n",
    "        \n",
    "    # 🔍 EXTRA: Feature-label correlation check\n",
    "    try:\n",
    "        corrs = [np.corrcoef(X[:, i], y)[0, 1] for i in range(X.shape[1])]\n",
    "        max_corr = np.nanmax(np.abs(corrs))\n",
    "        if max_corr > 0.95:\n",
    "            print(f\"🚨 POTENTIAL DATA LEAKAGE: Feature correlates with label at {max_corr:.3f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not compute feature-label correlation: {e}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44cf8a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 2: Chunk-Based Balanced Temporal Splitting ===\n",
      "🍭 'Candy Store' Approach: Distribute temporal chunks to achieve balance\n",
      "\n",
      "🔍 ANALYZING TEMPORAL CHUNKS...\n",
      "Chunk  1 (  0%-  5%): A=42483 B=15639 | Ratio=0.731 | attack_heavy\n",
      "Chunk  2 (  5%- 10%): A=50829 B= 7293 | Ratio=0.875 | attack_heavy\n",
      "Chunk  3 ( 10%- 15%): A=34722 B=23400 | Ratio=0.597 | mixed\n",
      "Chunk  4 ( 15%- 20%): A=24718 B=33404 | Ratio=0.425 | mixed\n",
      "Chunk  5 ( 20%- 25%): A=56092 B= 2030 | Ratio=0.965 | attack_heavy\n",
      "Chunk  6 ( 25%- 30%): A=56184 B= 1938 | Ratio=0.967 | attack_heavy\n",
      "Chunk  7 ( 30%- 35%): A=21931 B=36191 | Ratio=0.377 | benign_heavy\n",
      "Chunk  8 ( 35%- 40%): A= 1964 B=56158 | Ratio=0.034 | benign_heavy\n",
      "Chunk  9 ( 40%- 45%): A=    0 B=58122 | Ratio=0.000 | benign_heavy\n",
      "Chunk 10 ( 45%- 50%): A=    0 B=58122 | Ratio=0.000 | benign_heavy\n",
      "Chunk 11 ( 50%- 55%): A=    0 B=58122 | Ratio=0.000 | benign_heavy\n",
      "Chunk 12 ( 55%- 60%): A=   18 B=58104 | Ratio=0.000 | benign_heavy\n",
      "Chunk 13 ( 60%- 65%): A=   18 B=58104 | Ratio=0.000 | benign_heavy\n",
      "Chunk 14 ( 65%- 70%): A= 2180 B=55942 | Ratio=0.038 | benign_heavy\n",
      "Chunk 15 ( 70%- 75%): A= 7921 B=50201 | Ratio=0.136 | benign_heavy\n",
      "Chunk 16 ( 75%- 80%): A= 2991 B=55131 | Ratio=0.051 | benign_heavy\n",
      "Chunk 17 ( 80%- 85%): A=32628 B=25494 | Ratio=0.561 | mixed\n",
      "Chunk 18 ( 85%- 90%): A=54860 B= 3262 | Ratio=0.944 | attack_heavy\n",
      "Chunk 19 ( 90%- 95%): A=54753 B= 3369 | Ratio=0.942 | attack_heavy\n",
      "Chunk 20 ( 95%-100%): A=53723 B= 4399 | Ratio=0.924 | attack_heavy\n",
      "\n",
      "Chunk categories:\n",
      "  Attack-heavy: 7 chunks\n",
      "  Benign-heavy: 10 chunks\n",
      "  Mixed: 3 chunks\n",
      "\n",
      "🎯 ALLOCATING CHUNKS TO SPLITS...\n",
      "Target allocation: Train=12, Val=4, Test=4\n",
      "  train: Added benign-heavy chunk 6 (ratio=0.377)\n",
      "  train: Added benign-heavy chunk 7 (ratio=0.034)\n",
      "  train: Added attack-heavy chunk 1 (ratio=0.875)\n",
      "  train: Added benign-heavy chunk 8 (ratio=0.000)\n",
      "  train: Added benign-heavy chunk 9 (ratio=0.000)\n",
      "  train: Added benign-heavy chunk 10 (ratio=0.000)\n",
      "  val: Added attack-heavy chunk 19 (ratio=0.924)\n",
      "  val: Added benign-heavy chunk 11 (ratio=0.000)\n",
      "  test: Added benign-heavy chunk 13 (ratio=0.038)\n",
      "\n",
      "🔧 COMBINING CHUNKS INTO FINAL SPLITS...\n",
      "TRAIN:\n",
      "  Chunks: [0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 17, 18]\n",
      "  Samples: 697,464 | Benign: 323,646 (46.4%) | Attack: 373,818 (53.6%)\n",
      "VAL:\n",
      "  Chunks: [3, 11, 12, 19]\n",
      "  Samples: 232,488 | Benign: 154,011 (66.2%) | Attack: 78,477 (33.8%)\n",
      "TEST:\n",
      "  Chunks: [13, 14, 15, 16]\n",
      "  Samples: 232,488 | Benign: 186,768 (80.3%) | Attack: 45,720 (19.7%)\n",
      "\n",
      "============================================================\n",
      "🎯 FINAL CHUNK-BASED RESULTS\n",
      "============================================================\n",
      "Dataset size: 1,162,445 → 1,162,440 (100.0% retained)\n",
      "Split sizes: Train=697,464 | Val=232,488 | Test=232,488\n",
      "\n",
      "FINAL CLASS DISTRIBUTIONS:\n",
      "Train: Benign=46.4% | Attack=53.6% | Minority=46.4% ✅ USABLE\n",
      "Val  : Benign=66.2% | Attack=33.8% | Minority=33.8% ✅ USABLE\n",
      "Test : Benign=80.3% | Attack=19.7% | Minority=19.7% ⚠️  STILL IMBALANCED\n",
      "\n",
      "✅ Data integrity: 1,162,445 → 1,162,440 samples\n",
      "🍭 Temporal chunks successfully distributed!\n",
      "\n",
      "🚀 Ready for training with chunk-balanced splits!\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# STEP 2: CHUNK-BASED BALANCED TEMPORAL SPLITTING\n",
    "# ==============================\n",
    "print(\"\\n=== STEP 2: Chunk-Based Balanced Temporal Splitting ===\")\n",
    "print(\"🍭 'Candy Store' Approach: Distribute temporal chunks to achieve balance\")\n",
    "\n",
    "# ==============================\n",
    "# CHUNK ANALYSIS & PREPARATION\n",
    "# ==============================\n",
    "print(\"\\n🔍 ANALYZING TEMPORAL CHUNKS...\")\n",
    "\n",
    "# Create temporal chunks\n",
    "n_chunks = 20  # More chunks = finer control\n",
    "chunk_size = len(y) // n_chunks\n",
    "chunks_info = []\n",
    "\n",
    "for i in range(n_chunks):\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = min((i + 1) * chunk_size, len(y))\n",
    "    \n",
    "    chunk_X = X[start_idx:end_idx]\n",
    "    chunk_y = y[start_idx:end_idx]\n",
    "    \n",
    "    # Analyze chunk composition\n",
    "    unique_vals, counts = np.unique(chunk_y, return_counts=True)\n",
    "    total_samples = len(chunk_y)\n",
    "    benign_count = counts[unique_vals == 0][0] if 0 in unique_vals else 0\n",
    "    attack_count = counts[unique_vals == 1][0] if 1 in unique_vals else 0\n",
    "    \n",
    "    attack_ratio = attack_count / total_samples\n",
    "    \n",
    "    chunk_info = {\n",
    "        'id': i,\n",
    "        'start_pct': start_idx / len(y) * 100,\n",
    "        'end_pct': end_idx / len(y) * 100,\n",
    "        'X': chunk_X,\n",
    "        'y': chunk_y,\n",
    "        'total_samples': total_samples,\n",
    "        'benign_count': benign_count,\n",
    "        'attack_count': attack_count,\n",
    "        'attack_ratio': attack_ratio,\n",
    "        'chunk_type': 'attack_heavy' if attack_ratio > 0.6 else 'benign_heavy' if attack_ratio < 0.4 else 'mixed'\n",
    "    }\n",
    "    chunks_info.append(chunk_info)\n",
    "    \n",
    "    print(f\"Chunk {i+1:2d} ({chunk_info['start_pct']:3.0f}%-{chunk_info['end_pct']:3.0f}%): \"\n",
    "          f\"A={attack_count:5d} B={benign_count:5d} | Ratio={attack_ratio:.3f} | {chunk_info['chunk_type']}\")\n",
    "\n",
    "# Categorize chunks\n",
    "attack_heavy_chunks = [c for c in chunks_info if c['chunk_type'] == 'attack_heavy']\n",
    "benign_heavy_chunks = [c for c in chunks_info if c['chunk_type'] == 'benign_heavy']\n",
    "mixed_chunks = [c for c in chunks_info if c['chunk_type'] == 'mixed']\n",
    "\n",
    "print(f\"\\nChunk categories:\")\n",
    "print(f\"  Attack-heavy: {len(attack_heavy_chunks)} chunks\")\n",
    "print(f\"  Benign-heavy: {len(benign_heavy_chunks)} chunks\") \n",
    "print(f\"  Mixed: {len(mixed_chunks)} chunks\")\n",
    "\n",
    "# ==============================\n",
    "# SMART CHUNK ALLOCATION\n",
    "# ==============================\n",
    "print(f\"\\n🎯 ALLOCATING CHUNKS TO SPLITS...\")\n",
    "\n",
    "def allocate_chunks_for_balance(chunks_info, target_attack_ratio=0.5, train_ratio=0.6, val_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Allocate chunks to splits to achieve target balance\n",
    "    \"\"\"\n",
    "    total_chunks = len(chunks_info)\n",
    "    target_train_chunks = int(total_chunks * train_ratio)\n",
    "    target_val_chunks = int(total_chunks * val_ratio) \n",
    "    target_test_chunks = total_chunks - target_train_chunks - target_val_chunks\n",
    "    \n",
    "    print(f\"Target allocation: Train={target_train_chunks}, Val={target_val_chunks}, Test={target_test_chunks}\")\n",
    "    \n",
    "    # Strategy: Distribute chunk types across splits to balance attack ratios\n",
    "    train_chunks = []\n",
    "    val_chunks = []\n",
    "    test_chunks = []\n",
    "    \n",
    "    # Start with mixed chunks (easiest to balance)\n",
    "    mixed_per_split = len(mixed_chunks) // 3\n",
    "    train_chunks.extend(mixed_chunks[:mixed_per_split])\n",
    "    val_chunks.extend(mixed_chunks[mixed_per_split:2*mixed_per_split])\n",
    "    test_chunks.extend(mixed_chunks[2*mixed_per_split:])\n",
    "    \n",
    "    # Distribute attack-heavy and benign-heavy chunks\n",
    "    remaining_attack_heavy = attack_heavy_chunks.copy()\n",
    "    remaining_benign_heavy = benign_heavy_chunks.copy()\n",
    "    \n",
    "    # Fill remaining slots, balancing attack ratios\n",
    "    splits = [\n",
    "        ('train', train_chunks, target_train_chunks),\n",
    "        ('val', val_chunks, target_val_chunks), \n",
    "        ('test', test_chunks, target_test_chunks)\n",
    "    ]\n",
    "    \n",
    "    for split_name, split_chunks, target_size in splits:\n",
    "        while len(split_chunks) < target_size:\n",
    "            # Calculate current attack ratio for this split\n",
    "            current_attacks = sum(c['attack_count'] for c in split_chunks)\n",
    "            current_total = sum(c['total_samples'] for c in split_chunks)\n",
    "            current_ratio = current_attacks / current_total if current_total > 0 else 0\n",
    "            \n",
    "            # Decide what type of chunk to add\n",
    "            if current_ratio < target_attack_ratio - 0.05 and remaining_attack_heavy:\n",
    "                # Need more attacks\n",
    "                chunk = remaining_attack_heavy.pop(0)\n",
    "                split_chunks.append(chunk)\n",
    "                print(f\"  {split_name}: Added attack-heavy chunk {chunk['id']} (ratio={chunk['attack_ratio']:.3f})\")\n",
    "            elif current_ratio > target_attack_ratio + 0.05 and remaining_benign_heavy:\n",
    "                # Need more benign\n",
    "                chunk = remaining_benign_heavy.pop(0)\n",
    "                split_chunks.append(chunk)\n",
    "                print(f\"  {split_name}: Added benign-heavy chunk {chunk['id']} (ratio={chunk['attack_ratio']:.3f})\")\n",
    "            elif remaining_attack_heavy:\n",
    "                chunk = remaining_attack_heavy.pop(0)\n",
    "                split_chunks.append(chunk)\n",
    "            elif remaining_benign_heavy:\n",
    "                chunk = remaining_benign_heavy.pop(0)\n",
    "                split_chunks.append(chunk)\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    # Add any remaining chunks to the splits\n",
    "    all_remaining = remaining_attack_heavy + remaining_benign_heavy\n",
    "    for i, chunk in enumerate(all_remaining):\n",
    "        target_split = i % 3\n",
    "        if target_split == 0:\n",
    "            train_chunks.append(chunk)\n",
    "        elif target_split == 1:\n",
    "            val_chunks.append(chunk)\n",
    "        else:\n",
    "            test_chunks.append(chunk)\n",
    "    \n",
    "    return train_chunks, val_chunks, test_chunks\n",
    "\n",
    "# Allocate chunks\n",
    "train_chunk_list, val_chunk_list, test_chunk_list = allocate_chunks_for_balance(chunks_info)\n",
    "\n",
    "# ==============================\n",
    "# COMBINE CHUNKS INTO SPLITS\n",
    "# ==============================\n",
    "print(f\"\\n🔧 COMBINING CHUNKS INTO FINAL SPLITS...\")\n",
    "\n",
    "def combine_chunks(chunk_list, split_name):\n",
    "    \"\"\"Combine chunks while maintaining temporal order within each chunk\"\"\"\n",
    "    if not chunk_list:\n",
    "        print(f\"⚠️  {split_name}: No chunks allocated!\")\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    # Sort chunks by temporal order (chunk ID)\n",
    "    chunk_list.sort(key=lambda x: x['id'])\n",
    "    \n",
    "    X_combined = []\n",
    "    y_combined = []\n",
    "    \n",
    "    total_samples = 0\n",
    "    total_attacks = 0\n",
    "    total_benign = 0\n",
    "    \n",
    "    chunk_ids = []\n",
    "    for chunk in chunk_list:\n",
    "        X_combined.append(chunk['X'])\n",
    "        y_combined.append(chunk['y'])\n",
    "        total_samples += chunk['total_samples']\n",
    "        total_attacks += chunk['attack_count']\n",
    "        total_benign += chunk['benign_count']\n",
    "        chunk_ids.append(chunk['id'])\n",
    "    \n",
    "    X_final = np.vstack(X_combined) if X_combined else np.array([]).reshape(0, X.shape[1])\n",
    "    y_final = np.hstack(y_combined) if y_combined else np.array([])\n",
    "    \n",
    "    attack_ratio = total_attacks / total_samples if total_samples > 0 else 0\n",
    "    \n",
    "    print(f\"{split_name}:\")\n",
    "    print(f\"  Chunks: {chunk_ids}\")\n",
    "    print(f\"  Samples: {total_samples:,} | Benign: {total_benign:,} ({total_benign/total_samples:.1%}) | Attack: {total_attacks:,} ({attack_ratio:.1%})\")\n",
    "    \n",
    "    return X_final, y_final\n",
    "\n",
    "# Create final splits\n",
    "X_train, y_train = combine_chunks(train_chunk_list, \"TRAIN\")\n",
    "X_val, y_val = combine_chunks(val_chunk_list, \"VAL\")\n",
    "X_test, y_test = combine_chunks(test_chunk_list, \"TEST\")\n",
    "\n",
    "# ==============================\n",
    "# FINAL VALIDATION & REPORTING\n",
    "# ==============================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"🎯 FINAL CHUNK-BASED RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "total_retained = len(X_train) + len(X_val) + len(X_test)\n",
    "retention_rate = total_retained / len(X) * 100\n",
    "\n",
    "print(f\"Dataset size: {len(X):,} → {total_retained:,} ({retention_rate:.1f}% retained)\")\n",
    "print(f\"Split sizes: Train={len(X_train):,} | Val={len(X_val):,} | Test={len(X_test):,}\")\n",
    "\n",
    "# Final balance check\n",
    "print(f\"\\nFINAL CLASS DISTRIBUTIONS:\")\n",
    "for name, y_split in [(\"Train\", y_train), (\"Val\", y_val), (\"Test\", y_test)]:\n",
    "    if len(y_split) == 0:\n",
    "        print(f\"{name:5s}: 🚨 EMPTY SPLIT!\")\n",
    "        continue\n",
    "        \n",
    "    unique_vals, counts = np.unique(y_split, return_counts=True)\n",
    "    total = len(y_split)\n",
    "    \n",
    "    if len(unique_vals) >= 2:\n",
    "        benign_count = counts[unique_vals == 0][0] if 0 in unique_vals else 0\n",
    "        attack_count = counts[unique_vals == 1][0] if 1 in unique_vals else 0\n",
    "        \n",
    "        benign_pct = benign_count / total\n",
    "        attack_pct = attack_count / total\n",
    "        minority_pct = min(benign_pct, attack_pct) * 100\n",
    "        \n",
    "        print(f\"{name:5s}: Benign={benign_pct:.1%} | Attack={attack_pct:.1%} | Minority={minority_pct:.1f}%\", end=\"\")\n",
    "        \n",
    "        if minority_pct >= 25:  # More relaxed for test set\n",
    "            print(\" ✅ USABLE\")\n",
    "        else:\n",
    "            print(f\" ⚠️  STILL IMBALANCED\")\n",
    "    else:\n",
    "        print(f\"{name:5s}: 🚨 SINGLE CLASS ({unique_vals[0]}) - {counts[0]:,} samples!\")\n",
    "\n",
    "# Verify data integrity\n",
    "if total_retained > 0:\n",
    "    print(f\"\\n✅ Data integrity: {len(X):,} → {total_retained:,} samples\")\n",
    "    print(f\"🍭 Temporal chunks successfully distributed!\")\n",
    "else:\n",
    "    print(f\"\\n🚨 ERROR: No samples retained!\")\n",
    "\n",
    "print(f\"\\n🚀 Ready for training with chunk-balanced splits!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72b7b78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 3: Chronological Feature Scaling ===\n",
      "📊 Scaling based on training data only (no future information)\n",
      "Training data after scaling - Mean range: [-0.000000, 0.000000]\n",
      "Training data after scaling - Std range: [1.000000, 1.000000]\n",
      "✅ Feature scaling completed with temporal integrity\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# STEP 3: Feature Scaling (Applied Chronologically)\n",
    "# ==============================\n",
    "print(\"\\n=== STEP 3: Chronological Feature Scaling ===\")\n",
    "print(\"📊 Scaling based on training data only (no future information)\")\n",
    "    \n",
    "    # Fit scaler ONLY on training data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)  # Transform only, no fitting\n",
    "X_test_scaled = scaler.transform(X_test)  # Transform only, no fitting\n",
    "    \n",
    "    # DEBUG: Check for scaling issues\n",
    "train_mean = np.mean(X_train_scaled, axis=0)\n",
    "train_std = np.std(X_train_scaled, axis=0)\n",
    "    \n",
    "print(f\"Training data after scaling - Mean range: [{train_mean.min():.6f}, {train_mean.max():.6f}]\")\n",
    "print(f\"Training data after scaling - Std range: [{train_std.min():.6f}, {train_std.max():.6f}]\")\n",
    "    \n",
    "    # Check for constant features (std = 0)\n",
    "constant_features = np.sum(train_std < 1e-8)\n",
    "if constant_features > 0:\n",
    "    print(f\"⚠️  {constant_features} constant features detected - these won't help learning\")\n",
    "    \n",
    "print(\"✅ Feature scaling completed with temporal integrity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c912ea8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 4: Baseline Model Evaluation ===\n",
      "Training baseline models...\n",
      "Adding Majority Class Predictor...\n",
      "Adding Logistic Regression...\n",
      "Adding Random Forest...\n",
      "\n",
      "=== Baseline Model Evaluation ===\n",
      "\n",
      "Evaluating Majority Class...\n",
      "  Accuracy:  0.1967\n",
      "  Precision: 0.1967\n",
      "  Recall:    1.0000\n",
      "  F1 Score:  0.3287\n",
      "  AUC:       0.5000\n",
      "  PR-AUC:    0.1967\n",
      "\n",
      "Evaluating Logistic Regression...\n",
      "  Accuracy:  0.8747\n",
      "  Precision: 0.7716\n",
      "  Recall:    0.5151\n",
      "  F1 Score:  0.6178\n",
      "  AUC:       0.8322\n",
      "  PR-AUC:    0.7056\n",
      "\n",
      "Evaluating Random Forest...\n",
      "  Accuracy:  0.8869\n",
      "  Precision: 0.9617\n",
      "  Recall:    0.4426\n",
      "  F1 Score:  0.6062\n",
      "  AUC:       0.9190\n",
      "  PR-AUC:    0.8415\n",
      "\n",
      "=== Baseline Results Summary ===\n",
      "              Model  Accuracy  Precision  Recall     F1    AUC  PR-AUC\n",
      "     Majority Class    0.1967     0.1967  1.0000 0.3287 0.5000  0.1967\n",
      "Logistic Regression    0.8747     0.7716  0.5151 0.6178 0.8322  0.7056\n",
      "      Random Forest    0.8869     0.9617  0.4426 0.6062 0.9190  0.8415\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== STEP 4: Baseline Model Evaluation ===\")\n",
    "    \n",
    "baseline_eval = BaselineEvaluator()\n",
    "print(\"Training baseline models...\")\n",
    "baseline_eval.add_dummy_classifier(X_train_scaled, y_train)\n",
    "baseline_eval.add_logistic_regression(X_train_scaled, y_train)\n",
    "baseline_eval.add_random_forest(X_train_scaled, y_train)\n",
    "    \n",
    "if len(X_train_scaled) <= 50000:\n",
    "    baseline_eval.add_knn(X_train_scaled, y_train)\n",
    "    \n",
    "baseline_eval.evaluate_all(X_test_scaled, y_test, \"Baseline\")\n",
    "baseline_results_df = baseline_eval.get_results_summary()\n",
    "print(\"\\n=== Baseline Results Summary ===\")\n",
    "print(baseline_results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "522d6e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_length = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2da1828d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 5: IMPROVED Chronological Sequence Creation ===\n",
      "Creating robust sequences for all splits...\n",
      "  Input: 697464 samples, sequence length: 64\n",
      "  Using stride: 51 (overlap: 20.3%)\n",
      "  Created 13675 sequences\n",
      "    Class distribution: {np.int64(0): np.int64(6018), np.int64(1): np.int64(7657)}\n",
      "    Minority class ratio: 0.440\n",
      "  Input: 232488 samples, sequence length: 64\n",
      "  Using stride: 32 (overlap: 50.0%)\n",
      "  Created 7264 sequences\n",
      "    Class distribution: {np.int64(0): np.int64(4614), np.int64(1): np.int64(2650)}\n",
      "    Minority class ratio: 0.365\n",
      "  Input: 232488 samples, sequence length: 64\n",
      "  Using stride: 32 (overlap: 50.0%)\n",
      "  Created 7264 sequences\n",
      "    Class distribution: {np.int64(0): np.int64(5107), np.int64(1): np.int64(2157)}\n",
      "    Minority class ratio: 0.297\n",
      "\n",
      "Final sequence shapes:\n",
      "  Training: (13675, 64, 50)\n",
      "  Validation: (7264, 64, 50)\n",
      "  Test: (7264, 64, 50)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== STEP 5: IMPROVED Chronological Sequence Creation ===\")\n",
    "\n",
    "def create_robust_sequences(X, y, seq_len, overlap_ratio=0.7, min_attack_ratio=0.01):\n",
    "    \"\"\"\n",
    "    Create sequences with improved labeling and class balance validation\n",
    "    \"\"\"\n",
    "    print(f\"  Input: {len(X)} samples, sequence length: {seq_len}\")\n",
    "    \n",
    "    if len(X) < seq_len:\n",
    "        print(f\"⚠️  Not enough samples for sequences (need {seq_len}, have {len(X)})\")\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    sequences = []\n",
    "    labels = []\n",
    "    \n",
    "    # Use controlled overlap to prevent overfitting while maximizing data\n",
    "    stride = max(1, int(seq_len * overlap_ratio))\n",
    "    print(f\"  Using stride: {stride} (overlap: {1-stride/seq_len:.1%})\")\n",
    "    \n",
    "    for i in range(0, len(X) - seq_len + 1, stride):\n",
    "        sequence = X[i:i + seq_len]\n",
    "        sequence_labels = y[i:i + seq_len]\n",
    "        \n",
    "        # IMPROVED LABELING: Multiple strategies with validation\n",
    "        attack_ratio = np.mean(sequence_labels)\n",
    "        \n",
    "        # Strategy: Attack if significant portion of sequence contains attacks\n",
    "        sequence_label = 1 if attack_ratio >= min_attack_ratio else 0\n",
    "        \n",
    "        sequences.append(sequence)\n",
    "        labels.append(sequence_label)\n",
    "    \n",
    "    sequences = np.array(sequences)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    print(f\"  Created {len(sequences)} sequences\")\n",
    "    \n",
    "    # Validate class distribution\n",
    "    unique_seq, counts_seq = np.unique(labels, return_counts=True)\n",
    "    print(f\"    Class distribution: {dict(zip(unique_seq, counts_seq))}\")\n",
    "    \n",
    "    if len(unique_seq) < 2:\n",
    "        print(f\"🚨 SINGLE CLASS DETECTED - Trying alternative labeling...\")\n",
    "        # Fallback: ANY attack in sequence\n",
    "        labels = []\n",
    "        for i in range(0, len(X) - seq_len + 1, stride):\n",
    "            sequence_labels = y[i:i + seq_len]\n",
    "            sequence_label = 1 if np.any(sequence_labels) else 0\n",
    "            labels.append(sequence_label)\n",
    "        \n",
    "        labels = np.array(labels)\n",
    "        unique_seq, counts_seq = np.unique(labels, return_counts=True)\n",
    "        print(f\"    Fallback distribution: {dict(zip(unique_seq, counts_seq))}\")\n",
    "    \n",
    "    # Class balance check\n",
    "    if len(unique_seq) > 1:\n",
    "        minority_ratio = min(counts_seq) / len(labels)\n",
    "        print(f\"    Minority class ratio: {minority_ratio:.3f}\")\n",
    "        \n",
    "        if minority_ratio < 0.05:\n",
    "            print(f\"    ⚠️ Severe class imbalance detected!\")\n",
    "    \n",
    "    return sequences, labels\n",
    "\n",
    "# Apply improved sequence creation\n",
    "print(\"Creating robust sequences for all splits...\")\n",
    "sequence_length = window_length\n",
    "X_train_seq, y_train_seq = create_robust_sequences(\n",
    "    X_train_scaled, y_train, sequence_length, overlap_ratio=0.8\n",
    ")\n",
    "\n",
    "X_val_seq, y_val_seq = create_robust_sequences(\n",
    "    X_val_scaled, y_val, sequence_length, overlap_ratio=0.5\n",
    ")\n",
    "\n",
    "X_test_seq, y_test_seq = create_robust_sequences(\n",
    "    X_test_scaled, y_test, sequence_length, overlap_ratio=0.5\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal sequence shapes:\")\n",
    "print(f\"  Training: {X_train_seq.shape}\")\n",
    "print(f\"  Validation: {X_val_seq.shape}\")\n",
    "print(f\"  Test: {X_test_seq.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ddfff7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 6: Anti-Overfitting Configuration ===\n",
      "Training class distribution: {np.int64(0): 6018, np.int64(1): 7657}\n",
      "Calculated pos_weight: 0.786\n",
      "Conservative parameters: {'hidden_dim': 64, 'num_layers': 5, 'dropout': 0.5, 'use_attention': False, 'bidirectional': False, 'lr': 0.001, 'batch_size': 64, 'optimizer': 'adam', 'l2_reg': 0.001, 'gradient_clip': 1.0, 'early_stopping_patience': 40, 'reduce_lr_patience': 7, 'min_lr': 1e-06, 'min_delta': 0.0001, 'pos_weight': 0.7859474990205042, 'focal_loss': False}\n",
      "Model complexity ratio: 1.1700\n",
      "⚠️ Still potentially complex - monitor for overfitting\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# STEP 6: ANTI-OVERFITTING PARAMETERS\n",
    "# ==============================\n",
    "print(\"\\n=== STEP 6: Anti-Overfitting Configuration ===\")\n",
    "\n",
    "# Calculate class weights for imbalanced data\n",
    "from collections import Counter\n",
    "train_class_counts = Counter(y_train_seq)\n",
    "print(f\"Training class distribution: {dict(train_class_counts)}\")\n",
    "\n",
    "if len(train_class_counts) > 1:\n",
    "    total_samples = sum(train_class_counts.values())\n",
    "    class_weights = {\n",
    "        cls: total_samples / (len(train_class_counts) * count) \n",
    "        for cls, count in train_class_counts.items()\n",
    "    }\n",
    "    pos_weight = class_weights.get(1, 1.0) / class_weights.get(0, 1.0)\n",
    "    print(f\"Calculated pos_weight: {pos_weight:.3f}\")\n",
    "else:\n",
    "    pos_weight = 1.0\n",
    "    print(\"⚠️ Single class detected, using default weight\")\n",
    "\n",
    "# CONSERVATIVE parameters to prevent overfitting\n",
    "n_train = len(X_train_seq)\n",
    "n_features = X_train_seq.shape[2]\n",
    "  \n",
    "# tag \n",
    "\n",
    "best_params = {\n",
    "    # Model architecture - keep simple\n",
    "    'hidden_dim': min(128, max(64, n_features)),  # Scale with features but cap low\n",
    "    'num_layers': 5,                             # Single layer to prevent overfitting\n",
    "    'dropout': 0.5,                              # Strong dropout\n",
    "    'use_attention': False,                       # Disable attention for simplicity\n",
    "    'bidirectional': False,                       # Disable bidirectional\n",
    "    \n",
    "    # Training parameters\n",
    "    'lr': 0.001,                                 # Conservative learning rate\n",
    "    'batch_size': min(64, max(16, n_train // 20)), # Reasonable batch size\n",
    "    'optimizer': 'adam',                         # Standard Adam\n",
    "    'l2_reg': 1e-3,                             # Strong L2 regularization\n",
    "    'gradient_clip': 1.0,                       # Gradient clipping\n",
    "    \n",
    "    # Early stopping\n",
    "    'early_stopping_patience': 40,              # Increased patience\n",
    "    'reduce_lr_patience': 7,                    # LR reduction patience\n",
    "    'min_lr': 1e-6,                             # Minimum learning rate\n",
    "    'min_delta': 1e-4,                          # Minimum improvement\n",
    "    \n",
    "    # Class imbalance handling\n",
    "    'pos_weight': pos_weight,\n",
    "    'focal_loss': False  # Keep simple for now\n",
    "}\n",
    "\n",
    "print(f\"Conservative parameters: {best_params}\")\n",
    "\n",
    "# Model complexity check\n",
    "model_params = best_params['hidden_dim'] * best_params['num_layers'] * n_features\n",
    "complexity_ratio = model_params / n_train\n",
    "print(f\"Model complexity ratio: {complexity_ratio:.4f}\")\n",
    "if complexity_ratio > 0.1:\n",
    "    print(\"⚠️ Still potentially complex - monitor for overfitting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d872fe5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "=== IMMEDIATE FIXES FOR DECLINING PERFORMANCE ===\n",
      "\n",
      "1. Reduce Regularization:\n",
      "   - Set dropout to 0.1-0.2 (not 0.2-0.6)\n",
      "   - Reduce l2_reg to 1e-4 or 1e-5\n",
      "   - Remove L1 regularization entirely\n",
      "   - Remove excessive batch normalization\n",
      "\n",
      "2. Fix Loss Function:\n",
      "   - Use standard BCEWithLogitsLoss instead of focal loss\n",
      "   - Reduce pos_weight to sqrt((1-pos_ratio)/pos_ratio)\n",
      "   - Remove alpha and gamma parameters from focal loss\n",
      "\n",
      "3. Adjust Learning:\n",
      "   - Increase learning rate to 0.001-0.005\n",
      "   - Use factor=0.7 in ReduceLROnPlateau (not 0.5)\n",
      "   - Increase patience to 7-10 epochs\n",
      "   - Monitor combined performance metric\n",
      "\n",
      "4. Architecture Changes:\n",
      "   - Simplify to max 2 LSTM layers\n",
      "   - Remove input layer normalization\n",
      "   - Use wider intermediate layers (less bottlenecking)\n",
      "   - Remove excessive dropout layers\n",
      "\n",
      "5. Data Preprocessing:\n",
      "   - Use StandardScaler instead of RobustScaler\n",
      "   - Increase clipping threshold to 6.0 (from 5.0)\n",
      "   - Don't over-preprocess time series data\n",
      "\n",
      "============================================================\n",
      "KEY INSIGHTS ON YOUR DECLINING PERFORMANCE:\n",
      "============================================================\n",
      "Your F1/AUC decline is likely due to:\n",
      "1. Over-regularization preventing learning\n",
      "2. Focal loss fighting against reasonable predictions\n",
      "3. Aggressive preprocessing removing signal\n",
      "4. Model architecture bottlenecks\n",
      "5. Learning rate dropping too quickly\n",
      "\n",
      "The 'good' loss gap was masking poor actual performance!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# FIXED LSTM TRAINING - ADDRESSING DECLINING PERFORMANCE\n",
    "# ==============================\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from sklearn.metrics import f1_score, roc_auc_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import wandb\n",
    "import pickle\n",
    "import json\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def preprocess_data_fixed(X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "    \"\"\"\n",
    "    FIXED: More conservative preprocessing to preserve signal\n",
    "    \"\"\"\n",
    "    print(\"=== FIXED Data Preprocessing ===\")\n",
    "    \n",
    "    # Convert to numpy if needed\n",
    "    X_train = np.array(X_train, dtype=np.float32)\n",
    "    X_val = np.array(X_val, dtype=np.float32)\n",
    "    X_test = np.array(X_test, dtype=np.float32)\n",
    "    y_train = np.array(y_train, dtype=np.float32)\n",
    "    y_val = np.array(y_val, dtype=np.float32)\n",
    "    y_test = np.array(y_test, dtype=np.float32)\n",
    "    \n",
    "    # Clean data\n",
    "    def clean_sequences(X, y):\n",
    "        valid_mask = ~(np.isnan(X).any(axis=(1,2)) | np.isinf(X).any(axis=(1,2)) | \n",
    "                      np.isnan(y) | np.isinf(y))\n",
    "        return X[valid_mask], y[valid_mask]\n",
    "    \n",
    "    X_train, y_train = clean_sequences(X_train, y_train)\n",
    "    X_val, y_val = clean_sequences(X_val, y_val)\n",
    "    X_test, y_test = clean_sequences(X_test, y_test)\n",
    "    \n",
    "    print(f\"After cleaning - Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "    \n",
    "    # CONSERVATIVE scaling - preserve more signal\n",
    "    original_shape_train = X_train.shape\n",
    "    original_shape_val = X_val.shape\n",
    "    original_shape_test = X_test.shape\n",
    "    \n",
    "    # Reshape to (samples * timesteps, features)\n",
    "    X_train_flat = X_train.reshape(-1, X_train.shape[-1])\n",
    "    X_val_flat = X_val.reshape(-1, X_val.shape[-1])\n",
    "    X_test_flat = X_test.reshape(-1, X_test.shape[-1])\n",
    "    \n",
    "    # Use StandardScaler (less aggressive than RobustScaler)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_flat = scaler.fit_transform(X_train_flat)\n",
    "    X_val_flat = scaler.transform(X_val_flat)\n",
    "    X_test_flat = scaler.transform(X_test_flat)\n",
    "    \n",
    "    # Reshape back\n",
    "    X_train = X_train_flat.reshape(original_shape_train)\n",
    "    X_val = X_val_flat.reshape(original_shape_val)\n",
    "    X_test = X_test_flat.reshape(original_shape_test)\n",
    "    \n",
    "    # REMOVED: Aggressive clipping that can remove signal\n",
    "    # Only clip extreme outliers (>6 sigma)\n",
    "    clip_value = 6.0\n",
    "    X_train = np.clip(X_train, -clip_value, clip_value)\n",
    "    X_val = np.clip(X_val, -clip_value, clip_value)\n",
    "    X_test = np.clip(X_test, -clip_value, clip_value)\n",
    "    \n",
    "    # Ensure labels are properly shaped\n",
    "    y_train = y_train.reshape(-1)\n",
    "    y_val = y_val.reshape(-1)\n",
    "    y_test = y_test.reshape(-1)\n",
    "    \n",
    "    # More conservative pos_weight calculation\n",
    "    train_pos_ratio = np.mean(y_train)\n",
    "    val_pos_ratio = np.mean(y_val)\n",
    "    \n",
    "    print(f\"Class balance - Train: {train_pos_ratio:.3f}, Val: {val_pos_ratio:.3f}\")\n",
    "    \n",
    "    # Less aggressive pos_weight\n",
    "    if train_pos_ratio > 0:\n",
    "        pos_weight = (1 - train_pos_ratio) / train_pos_ratio  # Remove the sqrt()\n",
    "    else:\n",
    "        pos_weight = 1.0\n",
    "    \n",
    "    print(f\"Calculated pos_weight: {pos_weight:.2f}\")\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, scaler, pos_weight\n",
    "\n",
    "class FixedLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    FIXED: Reduced over-regularization and better architecture\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout=0.2):\n",
    "        super(FixedLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # REMOVED: Input normalization that can hurt time series patterns\n",
    "        \n",
    "        # LSTM with conservative dropout\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, hidden_dim, num_layers,\n",
    "            batch_first=True, \n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=False  # Keep simple\n",
    "        )\n",
    "        \n",
    "        # SIMPLIFIED architecture - no excessive bottlenecking\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        # Wider intermediate layer (less bottlenecking)\n",
    "        intermediate_dim = max(hidden_dim // 2, 32)\n",
    "        self.fc1 = nn.Linear(hidden_dim, intermediate_dim)\n",
    "        \n",
    "        # REMOVED: Excessive batch norm and dropout layers\n",
    "        self.dropout2 = nn.Dropout(dropout * 0.5)  # Lighter dropout\n",
    "        \n",
    "        # Final output layer\n",
    "        self.fc_out = nn.Linear(intermediate_dim, 1)\n",
    "        \n",
    "        # Proper initialization\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        \"\"\"Conservative weight initialization\"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                torch.nn.init.xavier_uniform_(param.data)\n",
    "            elif 'weight_hh' in name:\n",
    "                torch.nn.init.orthogonal_(param.data)\n",
    "            elif 'bias' in name:\n",
    "                param.data.fill_(0)\n",
    "            elif 'fc' in name and 'weight' in name:\n",
    "                torch.nn.init.xavier_uniform_(param.data)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # LSTM forward pass\n",
    "        lstm_out, (hidden, cell) = self.lstm(x)\n",
    "        \n",
    "        # Use final hidden state\n",
    "        sequence_output = hidden[-1]\n",
    "        \n",
    "        # Simple forward pass with light regularization\n",
    "        x = self.dropout1(sequence_output)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        output = self.fc_out(x)\n",
    "        \n",
    "        return output.squeeze(-1)\n",
    "\n",
    "def train_lstm_fixed(X_train, y_train, X_val, y_val, params, model, epochs=50, trial=None):\n",
    "    \"\"\"\n",
    "    FIXED training function addressing declining performance\n",
    "    \"\"\"\n",
    "    print(f\"Training with FIXED approach - monitoring for declining performance\")\n",
    "    \n",
    "    # Ensure proper data types\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    X_val = X_val.astype(np.float32)\n",
    "    y_train = y_train.astype(np.float32)\n",
    "    y_val = y_val.astype(np.float32)\n",
    "    \n",
    "    # Create datasets - No shuffling for time series\n",
    "    train_dataset = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "    val_dataset = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=params['batch_size'], shuffle=False)\n",
    "\n",
    "    # FIXED: Standard Adam with conservative settings\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=params['lr'],\n",
    "        weight_decay=params['l2_reg'],\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "\n",
    "    # FIXED: Less aggressive learning rate scheduling\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.7,  # Less aggressive reduction\n",
    "        patience=7,  # More patience\n",
    "        verbose=True,\n",
    "        min_lr=5e-6\n",
    "    )\n",
    "\n",
    "    # FIXED: Use standard BCE loss instead of focal loss\n",
    "    criterion = nn.BCEWithLogitsLoss(\n",
    "        pos_weight=torch.tensor([params['pos_weight']], device=device)\n",
    "    )\n",
    "    \n",
    "    # Tracking variables\n",
    "    train_losses, val_losses = [], []\n",
    "    train_f1s, val_f1s, val_aucs = [], [], []\n",
    "    best_val_f1, patience_counter = 0, 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    # Performance decline detection\n",
    "    performance_history = []\n",
    "    decline_threshold = 3  # Stop if performance declines for 3 epochs\n",
    "\n",
    "    print(f\"Starting training with performance monitoring...\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # =============\n",
    "        # TRAINING PHASE\n",
    "        # =============\n",
    "        model.train()\n",
    "        epoch_loss, train_preds_all, train_targets_all = 0, [], []\n",
    "\n",
    "        for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            \n",
    "            # Ensure proper shapes\n",
    "            if outputs.shape != y_batch.shape:\n",
    "                outputs = outputs.view(-1)\n",
    "                y_batch = y_batch.view(-1)\n",
    "            \n",
    "            # FIXED: Standard BCE loss\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            \n",
    "            # FIXED: Much lighter L1 regularization\n",
    "            if params.get('l1_reg', 0) > 0:\n",
    "                l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "                loss = loss + params['l1_reg'] * l1_norm\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), params['gradient_clip'])\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Collect predictions\n",
    "            with torch.no_grad():\n",
    "                probs = torch.sigmoid(outputs).cpu().numpy().flatten()\n",
    "                preds = (probs >= 0.5).astype(int)\n",
    "                train_preds_all.extend(preds)\n",
    "                train_targets_all.extend(y_batch.cpu().numpy().flatten())\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Training metrics\n",
    "        if len(set(train_targets_all)) > 1:\n",
    "            train_f1 = f1_score(train_targets_all, train_preds_all, zero_division=0)\n",
    "        else:\n",
    "            train_f1 = 0.0\n",
    "        train_f1s.append(train_f1)\n",
    "\n",
    "        # =============\n",
    "        # VALIDATION PHASE\n",
    "        # =============\n",
    "        model.eval()\n",
    "        val_loss, val_probs_all, val_targets_all = 0, [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                \n",
    "                if outputs.shape != y_batch.shape:\n",
    "                    outputs = outputs.view(-1)\n",
    "                    y_batch = y_batch.view(-1)\n",
    "                \n",
    "                batch_loss = criterion(outputs, y_batch)\n",
    "                val_loss += batch_loss.item()\n",
    "                \n",
    "                probs = torch.sigmoid(outputs).cpu().numpy().flatten()\n",
    "                val_probs_all.extend(probs)\n",
    "                val_targets_all.extend(y_batch.cpu().numpy().flatten())\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        # Calculate validation metrics\n",
    "        if len(np.unique(val_targets_all)) > 1:\n",
    "            val_auc = roc_auc_score(val_targets_all, val_probs_all)\n",
    "        else:\n",
    "            val_auc = 0.5\n",
    "        val_aucs.append(val_auc)\n",
    "        \n",
    "        # Find best F1 across thresholds\n",
    "        best_val_f1_epoch = 0\n",
    "        for thresh in np.arange(0.3, 0.7, 0.05):  # Narrower threshold range\n",
    "            preds = (np.array(val_probs_all) >= thresh).astype(int)\n",
    "            f1 = f1_score(val_targets_all, preds, zero_division=0)\n",
    "            best_val_f1_epoch = max(best_val_f1_epoch, f1)\n",
    "        \n",
    "        val_f1s.append(best_val_f1_epoch)\n",
    "        \n",
    "        # FIXED: Performance decline detection\n",
    "        current_performance = (train_f1 + best_val_f1_epoch + val_auc) / 3\n",
    "        performance_history.append(current_performance)\n",
    "        \n",
    "        # Check for consistent decline\n",
    "        if len(performance_history) >= decline_threshold + 1:\n",
    "            recent_trend = performance_history[-decline_threshold:]\n",
    "            if all(recent_trend[i] <= recent_trend[i-1] for i in range(1, len(recent_trend))):\n",
    "                print(f\"PERFORMANCE DECLINE DETECTED - stopping training\")\n",
    "                print(f\"Recent performance: {recent_trend}\")\n",
    "                break\n",
    "\n",
    "        loss_gap = avg_val_loss - avg_train_loss\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:2d} | Loss: T={avg_train_loss:.4f} V={avg_val_loss:.4f} Gap={loss_gap:.4f}\")\n",
    "        print(f\"         | F1: T={train_f1:.4f} V={best_val_f1_epoch:.4f} | AUC: {val_auc:.4f}\")\n",
    "        print(f\"         | Combined Performance: {current_performance:.4f}\")\n",
    "\n",
    "        # Model saving logic\n",
    "        if best_val_f1_epoch > best_val_f1:\n",
    "            best_val_f1 = best_val_f1_epoch\n",
    "            best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            patience_counter = 0\n",
    "            print(f\"    ✅ New best F1: {best_val_f1:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        # FIXED: More conservative early stopping\n",
    "        if patience_counter >= 15:  # Increased patience\n",
    "            print(f\"Early stopping after {patience_counter} epochs without improvement\")\n",
    "            break\n",
    "        \n",
    "        # FIXED: Less aggressive learning rate scheduling\n",
    "        scheduler.step(current_performance)  # Use combined performance metric\n",
    "\n",
    "        # Optuna pruning (if applicable)\n",
    "        if trial is not None:\n",
    "            trial.report(current_performance, epoch)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    # Restore best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict({k: v.to(device) for k, v in best_model_state.items()})\n",
    "        print(f\"Restored best model with F1: {best_val_f1:.4f}\")\n",
    "\n",
    "    return model, {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_f1s': train_f1s,\n",
    "        'val_f1s': val_f1s,\n",
    "        'val_aucs': val_aucs,\n",
    "        'best_val_f1': best_val_f1,\n",
    "        'performance_history': performance_history\n",
    "    }\n",
    "\n",
    "def fixed_objective(trial):\n",
    "    \"\"\"\n",
    "    FIXED objective with conservative parameter ranges\n",
    "    \"\"\"\n",
    "    print(f\"\\n🔬 FIXED Trial #{trial.number}\")\n",
    "    \n",
    "    # CONSERVATIVE parameter ranges to prevent over-regularization\n",
    "    params = {\n",
    "        # Architecture - keep simple\n",
    "        'hidden_dim': trial.suggest_categorical('hidden_dim', [32, 48, 64, 96]),\n",
    "        'num_layers': trial.suggest_int('num_layers', 1, 2),  # Max 2 layers\n",
    "        'sequence_length': trial.suggest_categorical('sequence_length', [15, 20, 25]),\n",
    "        \n",
    "        # LIGHT regularization only\n",
    "        'dropout': trial.suggest_float('dropout', 0.1, 0.3),  # Much lower dropout\n",
    "        'l2_reg': trial.suggest_float('l2_reg', 1e-5, 1e-3, log=True),  # Lighter L2\n",
    "        'l1_reg': 0.0,  # REMOVED L1 regularization\n",
    "        \n",
    "        # Training parameters\n",
    "        'lr': trial.suggest_float('lr', 0.0005, 0.005, log=True),  # Higher learning rates\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [64, 128, 256]),\n",
    "        'gradient_clip': trial.suggest_float('gradient_clip', 1.0, 5.0),  # Less aggressive clipping\n",
    "        \n",
    "        # Fixed parameters\n",
    "        'early_stopping_patience': 15,\n",
    "        'min_delta': 1e-5\n",
    "    }\n",
    "    \n",
    "    print(f\"Trial params: hidden={params['hidden_dim']}, layers={params['num_layers']}, \"\n",
    "          f\"dropout={params['dropout']:.3f}, lr={params['lr']:.4f}\")\n",
    "    \n",
    "    try:\n",
    "        # Sequence length adjustment\n",
    "        seq_len = params['sequence_length']\n",
    "        if 'X_train_seq' not in globals():\n",
    "            raise ValueError(\"Training data not found\")\n",
    "        \n",
    "        # Adjust sequences\n",
    "        if seq_len != X_train_seq.shape[1]:\n",
    "            if seq_len < X_train_seq.shape[1]:\n",
    "                X_train_trial = X_train_seq[:, -seq_len:, :]\n",
    "                X_val_trial = X_val_seq[:, -seq_len:, :]\n",
    "                X_test_trial = X_test_seq[:, -seq_len:, :]\n",
    "            else:\n",
    "                pad_size = seq_len - X_train_seq.shape[1]\n",
    "                X_train_trial = np.pad(X_train_seq, ((0,0), (pad_size,0), (0,0)), mode='constant')\n",
    "                X_val_trial = np.pad(X_val_seq, ((0,0), (pad_size,0), (0,0)), mode='constant')\n",
    "                X_test_trial = np.pad(X_test_seq, ((0,0), (pad_size,0), (0,0)), mode='constant')\n",
    "        else:\n",
    "            X_train_trial = X_train_seq\n",
    "            X_val_trial = X_val_seq\n",
    "            X_test_trial = X_test_seq\n",
    "        \n",
    "        # Apply FIXED preprocessing\n",
    "        X_train_clean, X_val_clean, X_test_clean, y_train_clean, y_val_clean, y_test_clean, scaler, pos_weight = preprocess_data_fixed(\n",
    "            X_train_trial, X_val_trial, X_test_trial, y_train_seq, y_val_seq, y_test_seq\n",
    "        )\n",
    "        \n",
    "        params['pos_weight'] = pos_weight\n",
    "        \n",
    "        # Create model\n",
    "        model = FixedLSTM(\n",
    "            input_dim=X_train_clean.shape[2],\n",
    "            hidden_dim=params['hidden_dim'],\n",
    "            num_layers=params['num_layers'],\n",
    "            dropout=params['dropout']\n",
    "        ).to(device)\n",
    "        \n",
    "        # Train model\n",
    "        trained_model, history = train_lstm_fixed(\n",
    "            X_train_clean, y_train_clean, X_val_clean, y_val_clean,\n",
    "            params, model, epochs=50, trial=trial\n",
    "        )\n",
    "        \n",
    "        # FIXED scoring: Penalize declining performance\n",
    "        best_f1 = history['best_val_f1']\n",
    "        \n",
    "        # Check if performance declined significantly\n",
    "        if len(history['performance_history']) > 5:\n",
    "            early_performance = np.mean(history['performance_history'][:5])\n",
    "            late_performance = np.mean(history['performance_history'][-5:])\n",
    "            decline_penalty = max(0, early_performance - late_performance) * 10  # Heavy penalty for decline\n",
    "        else:\n",
    "            decline_penalty = 0\n",
    "        \n",
    "        final_score = best_f1 - decline_penalty\n",
    "        \n",
    "        print(f\"Trial {trial.number}: F1={best_f1:.4f}, Decline_penalty={decline_penalty:.4f}, \"\n",
    "              f\"Score={final_score:.4f}\")\n",
    "        \n",
    "        return final_score\n",
    "        \n",
    "    except optuna.exceptions.TrialPruned:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Trial {trial.number} failed: {str(e)}\")\n",
    "        return 0.0\n",
    "\n",
    "def quick_diagnostic_check(model, X_train, X_val, y_train, y_val):\n",
    "    \"\"\"\n",
    "    Quick check to identify why performance is declining\n",
    "    \"\"\"\n",
    "    print(\"\\n=== PERFORMANCE DECLINE DIAGNOSTIC ===\")\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get predictions for both sets\n",
    "        train_tensor = torch.from_numpy(X_train[:1000].astype(np.float32)).to(device)\n",
    "        val_tensor = torch.from_numpy(X_val.astype(np.float32)).to(device)\n",
    "        \n",
    "        train_outputs = model(train_tensor)\n",
    "        val_outputs = model(val_tensor)\n",
    "        \n",
    "        train_probs = torch.sigmoid(train_outputs).cpu().numpy()\n",
    "        val_probs = torch.sigmoid(val_outputs).cpu().numpy()\n",
    "        \n",
    "        # Check for degenerate predictions\n",
    "        train_prob_range = np.max(train_probs) - np.min(train_probs)\n",
    "        val_prob_range = np.max(val_probs) - np.min(val_probs)\n",
    "        \n",
    "        print(f\"Prediction analysis:\")\n",
    "        print(f\"   Train prob range: {train_prob_range:.4f}\")\n",
    "        print(f\"   Val prob range: {val_prob_range:.4f}\")\n",
    "        print(f\"   Train prob mean: {np.mean(train_probs):.4f}\")\n",
    "        print(f\"   Val prob mean: {np.mean(val_probs):.4f}\")\n",
    "        \n",
    "        # Check for mode collapse\n",
    "        train_entropy = -np.mean(train_probs * np.log(train_probs + 1e-8) + \n",
    "                                (1-train_probs) * np.log(1-train_probs + 1e-8))\n",
    "        val_entropy = -np.mean(val_probs * np.log(val_probs + 1e-8) + \n",
    "                              (1-val_probs) * np.log(1-val_probs + 1e-8))\n",
    "        \n",
    "        print(f\"   Train entropy: {train_entropy:.4f}\")\n",
    "        print(f\"   Val entropy: {val_entropy:.4f}\")\n",
    "        \n",
    "        if train_prob_range < 0.1:\n",
    "            print(\"   🚨 WARNING: Training predictions too narrow (mode collapse)\")\n",
    "        if val_prob_range < 0.1:\n",
    "            print(\"   🚨 WARNING: Validation predictions too narrow\")\n",
    "        if train_entropy < 0.2:\n",
    "            print(\"   🚨 WARNING: Very low prediction entropy on training data\")\n",
    "            \n",
    "        # Gradient analysis\n",
    "        model.train()\n",
    "        dummy_input = torch.from_numpy(X_train[:32].astype(np.float32)).to(device)\n",
    "        dummy_target = torch.from_numpy(y_train[:32].astype(np.float32)).to(device)\n",
    "        \n",
    "        outputs = model(dummy_input)\n",
    "        loss = criterion(outputs.view(-1), dummy_target.view(-1))\n",
    "        loss.backward()\n",
    "        \n",
    "        grad_norms = []\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                grad_norms.append(param.grad.norm().item())\n",
    "        \n",
    "        avg_grad_norm = np.mean(grad_norms)\n",
    "        print(f\"   Average gradient norm: {avg_grad_norm:.6f}\")\n",
    "        \n",
    "        if avg_grad_norm < 1e-6:\n",
    "            print(\"   🚨 WARNING: Very small gradients (vanishing gradient problem)\")\n",
    "        elif avg_grad_norm > 10:\n",
    "            print(\"   🚨 WARNING: Very large gradients (exploding gradient problem)\")\n",
    "\n",
    "# ==============================\n",
    "# RECOMMENDED QUICK FIXES\n",
    "# ==============================\n",
    "\n",
    "def apply_quick_fixes():\n",
    "    \"\"\"\n",
    "    Immediate changes you can make to fix declining performance\n",
    "    \"\"\"\n",
    "    print(\"\\n=== IMMEDIATE FIXES FOR DECLINING PERFORMANCE ===\")\n",
    "    \n",
    "    fixes = {\n",
    "        \"1. Reduce Regularization\": [\n",
    "            \"Set dropout to 0.1-0.2 (not 0.2-0.6)\",\n",
    "            \"Reduce l2_reg to 1e-4 or 1e-5\",\n",
    "            \"Remove L1 regularization entirely\",\n",
    "            \"Remove excessive batch normalization\"\n",
    "        ],\n",
    "        \n",
    "        \"2. Fix Loss Function\": [\n",
    "            \"Use standard BCEWithLogitsLoss instead of focal loss\",\n",
    "            \"Reduce pos_weight to sqrt((1-pos_ratio)/pos_ratio)\",\n",
    "            \"Remove alpha and gamma parameters from focal loss\"\n",
    "        ],\n",
    "        \n",
    "        \"3. Adjust Learning\": [\n",
    "            \"Increase learning rate to 0.001-0.005\",\n",
    "            \"Use factor=0.7 in ReduceLROnPlateau (not 0.5)\",\n",
    "            \"Increase patience to 7-10 epochs\",\n",
    "            \"Monitor combined performance metric\"\n",
    "        ],\n",
    "        \n",
    "        \"4. Architecture Changes\": [\n",
    "            \"Simplify to max 2 LSTM layers\",\n",
    "            \"Remove input layer normalization\",\n",
    "            \"Use wider intermediate layers (less bottlenecking)\",\n",
    "            \"Remove excessive dropout layers\"\n",
    "        ],\n",
    "        \n",
    "        \"5. Data Preprocessing\": [\n",
    "            \"Use StandardScaler instead of RobustScaler\",\n",
    "            \"Increase clipping threshold to 6.0 (from 5.0)\",\n",
    "            \"Don't over-preprocess time series data\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for category, fix_list in fixes.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        for fix in fix_list:\n",
    "            print(f\"   - {fix}\")\n",
    "    \n",
    "    return fixes\n",
    "\n",
    "# ==============================\n",
    "# USAGE\n",
    "# ==============================\n",
    "\n",
    "# Apply immediate fixes\n",
    "quick_fixes = apply_quick_fixes()\n",
    "\n",
    "# Or run the fixed optimization\n",
    "# study_fixed, model_fixed, params_fixed, summary_fixed = run_fixed_optimization()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY INSIGHTS ON YOUR DECLINING PERFORMANCE:\")\n",
    "print(\"=\"*60)\n",
    "print(\"Your F1/AUC decline is likely due to:\")\n",
    "print(\"1. Over-regularization preventing learning\")\n",
    "print(\"2. Focal loss fighting against reasonable predictions\") \n",
    "print(\"3. Aggressive preprocessing removing signal\")\n",
    "print(\"4. Model architecture bottlenecks\")\n",
    "print(\"5. Learning rate dropping too quickly\")\n",
    "print(\"\\nThe 'good' loss gap was masking poor actual performance!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4d941aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 14:56:02,044] A new study created in memory with name: no-name-96712735-78cb-41bd-8349-5288bab2e5e8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\gokde\\OneDrive\\Masaüstü\\Anomaly-Detection-and-Network-Optimization-in-SDNs\\real_project\\wandb\\run-20250901_145602-wdmmuxas</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/trial-5-/runs/wdmmuxas' target=\"_blank\">trial_0</a></strong> to <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/trial-5-' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/trial-5-' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/trial-5-</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/trial-5-/runs/wdmmuxas' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/trial-5-/runs/wdmmuxas</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing AdvancedLSTM:\n",
      "  Input dim: 50\n",
      "  Hidden dim: 8\n",
      "  Num layers: 3\n",
      "  Dropout: 0.2\n",
      "  Attention: False\n",
      "  Bidirectional: True\n",
      "  Total parameters: 7,361\n",
      "  Trainable parameters: 7,361\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to True."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_0</strong> at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/trial-5-/runs/wdmmuxas' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/trial-5-/runs/wdmmuxas</a><br> View project at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/trial-5-' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/trial-5-</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250901_145602-wdmmuxas\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "creating run (0.2s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\gokde\\OneDrive\\Masaüstü\\Anomaly-Detection-and-Network-Optimization-in-SDNs\\real_project\\wandb\\run-20250901_145603-6gkbj9zz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-optuna/runs/6gkbj9zz' target=\"_blank\">trial_0</a></strong> to <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-optuna' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-optuna' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-optuna</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-optuna/runs/6gkbj9zz' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-optuna/runs/6gkbj9zz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 50 epochs...\n",
      "Epoch  1 | Train Loss: 1.0741 | Val Loss: 0.9406 | Train F1: 0.5502 | Val F1: 0.5346 | Val AUC: 0.7519\n",
      "Epoch  2 | Train Loss: 1.0356 | Val Loss: 0.9431 | Train F1: 0.7084 | Val F1: 0.5346 | Val AUC: 0.8003\n",
      "Epoch  3 | Train Loss: 1.0031 | Val Loss: 0.9467 | Train F1: 0.7175 | Val F1: 0.5346 | Val AUC: 0.8811\n",
      "Epoch  4 | Train Loss: 1.0006 | Val Loss: 0.9547 | Train F1: 0.7179 | Val F1: 0.5346 | Val AUC: 0.8381\n",
      "Epoch  5 | Train Loss: 0.9850 | Val Loss: 0.9650 | Train F1: 0.7179 | Val F1: 0.5346 | Val AUC: 0.4986\n",
      "Epoch  6 | Train Loss: 0.9768 | Val Loss: 0.9811 | Train F1: 0.7179 | Val F1: 0.5346 | Val AUC: 0.5000\n",
      "Epoch  7 | Train Loss: 0.9570 | Val Loss: 0.9904 | Train F1: 0.7179 | Val F1: 0.5346 | Val AUC: 0.5000\n",
      "Epoch  8 | Train Loss: 0.9552 | Val Loss: 0.9999 | Train F1: 0.7179 | Val F1: 0.5346 | Val AUC: 0.5000\n",
      "Epoch  9 | Train Loss: 0.9525 | Val Loss: 1.0085 | Train F1: 0.7179 | Val F1: 0.5346 | Val AUC: 0.5000\n",
      "⏹ Early stopping at epoch 9\n",
      "✅ Restored best model with validation F1: 0.5346\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch_end</td><td>▁▂▃▄▅▅▆▇█</td></tr><tr><td>final_val_auc</td><td>▁</td></tr><tr><td>final_val_f1</td><td>▁</td></tr><tr><td>learning_rate</td><td>██████▁▁▁</td></tr><tr><td>loss_gap</td><td>▁▃▄▄▅▆▇██</td></tr><tr><td>patience_counter</td><td>▁▁▂▃▄▅▆▇█</td></tr><tr><td>total_epochs</td><td>▁</td></tr><tr><td>train_f1</td><td>▁████████</td></tr><tr><td>train_loss</td><td>█▆▄▄▃▂▁▁▁</td></tr><tr><td>val_auc</td><td>▆▇█▇▁▁▁▁▁</td></tr><tr><td>+2</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>converged</td><td>False</td></tr><tr><td>epoch_end</td><td>8</td></tr><tr><td>final_val_auc</td><td>0.5</td></tr><tr><td>final_val_f1</td><td>0.5346</td></tr><tr><td>learning_rate</td><td>0.0006</td></tr><tr><td>loss_gap</td><td>0.05594</td></tr><tr><td>patience_counter</td><td>7</td></tr><tr><td>total_epochs</td><td>9</td></tr><tr><td>train_f1</td><td>0.71789</td></tr><tr><td>train_loss</td><td>0.95253</td></tr><tr><td>+3</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_0</strong> at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-optuna/runs/6gkbj9zz' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-optuna/runs/6gkbj9zz</a><br> View project at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-optuna' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-optuna</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250901_145603-6gkbj9zz\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 14:56:22,259] Trial 0 finished with value: 0.5345975388339722 and parameters: {'hidden_dim': 8, 'num_layers': 3, 'dropout': 0.2, 'use_attention': False, 'bidirectional': True, 'sequence_length': 2}. Best is trial 0 with value: 0.5345975388339722.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "creating run (0.2s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\gokde\\OneDrive\\Masaüstü\\Anomaly-Detection-and-Network-Optimization-in-SDNs\\real_project\\wandb\\run-20250901_145622-h8m4dw7r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/trial-5-/runs/h8m4dw7r' target=\"_blank\">trial_1</a></strong> to <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/trial-5-' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/trial-5-' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/trial-5-</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/trial-5-/runs/h8m4dw7r' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/trial-5-/runs/h8m4dw7r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing AdvancedLSTM:\n",
      "  Input dim: 50\n",
      "  Hidden dim: 32\n",
      "  Num layers: 1\n",
      "  Dropout: 0.8\n",
      "  Attention: True\n",
      "  Bidirectional: True\n",
      "  Total parameters: 40,833\n",
      "  Trainable parameters: 40,833\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to True."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_1</strong> at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/trial-5-/runs/h8m4dw7r' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/trial-5-/runs/h8m4dw7r</a><br> View project at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/trial-5-' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/trial-5-</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250901_145622-h8m4dw7r\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "creating run (0.1s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\gokde\\OneDrive\\Masaüstü\\Anomaly-Detection-and-Network-Optimization-in-SDNs\\real_project\\wandb\\run-20250901_145623-5iiv9g1b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-optuna/runs/5iiv9g1b' target=\"_blank\">trial_1</a></strong> to <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-optuna' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-optuna' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-optuna</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-optuna/runs/5iiv9g1b' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-optuna/runs/5iiv9g1b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 50 epochs...\n",
      "Epoch  1 | Train Loss: 1.0893 | Val Loss: 0.9656 | Train F1: 0.5323 | Val F1: 0.5346 | Val AUC: 0.3816\n",
      "Epoch  2 | Train Loss: 1.0073 | Val Loss: 0.9646 | Train F1: 0.7041 | Val F1: 0.5346 | Val AUC: 0.4664\n",
      "Epoch  3 | Train Loss: 0.9439 | Val Loss: 1.0225 | Train F1: 0.7179 | Val F1: 0.5346 | Val AUC: 0.4458\n",
      "Epoch  4 | Train Loss: 0.8813 | Val Loss: 1.0626 | Train F1: 0.7179 | Val F1: 0.5346 | Val AUC: 0.4531\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch_end</td><td>▁▃▆█</td></tr><tr><td>learning_rate</td><td>▁▁▁▁</td></tr><tr><td>loss_gap</td><td>▁▃▆█</td></tr><tr><td>patience_counter</td><td>▁▁▅█</td></tr><tr><td>train_f1</td><td>▁▇██</td></tr><tr><td>train_loss</td><td>█▅▃▁</td></tr><tr><td>val_auc</td><td>▁█▆▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁</td></tr><tr><td>val_loss</td><td>▁▁▅█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch_end</td><td>3</td></tr><tr><td>learning_rate</td><td>0.001</td></tr><tr><td>loss_gap</td><td>0.18131</td></tr><tr><td>patience_counter</td><td>2</td></tr><tr><td>train_f1</td><td>0.71789</td></tr><tr><td>train_loss</td><td>0.88129</td></tr><tr><td>val_auc</td><td>0.45314</td></tr><tr><td>val_f1</td><td>0.5346</td></tr><tr><td>val_loss</td><td>1.0626</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_1</strong> at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-optuna/runs/5iiv9g1b' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-optuna/runs/5iiv9g1b</a><br> View project at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-optuna' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-optuna</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250901_145623-5iiv9g1b\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-09-01 14:56:33,127] Trial 1 failed with parameters: {'hidden_dim': 32, 'num_layers': 1, 'dropout': 0.8, 'use_attention': True, 'bidirectional': True, 'sequence_length': 48} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\gokde\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\gokde\\AppData\\Local\\Temp\\ipykernel_25360\\3076918293.py\", line 233, in objective\n",
      "    trained_model, history = train_lstm_robust_antioverfitting(\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\gokde\\AppData\\Local\\Temp\\ipykernel_25360\\3076918293.py\", line 65, in train_lstm_robust_antioverfitting\n",
      "    loss.backward()\n",
      "  File \"c:\\Users\\gokde\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\torch\\_tensor.py\", line 581, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"c:\\Users\\gokde\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\torch\\autograd\\__init__.py\", line 347, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"c:\\Users\\gokde\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\torch\\autograd\\graph.py\", line 825, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2025-09-01 14:56:33,147] Trial 1 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 264\u001b[39m\n\u001b[32m    262\u001b[39m sampler = optuna.samplers.GridSampler(search_space=param_grid)\n\u001b[32m    263\u001b[39m study = optuna.create_study(direction=\u001b[33m'\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m'\u001b[39m, sampler=sampler)\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Explore all combinations\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gokde\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\optuna\\study\\study.py:489\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    388\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    389\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    397\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    398\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    399\u001b[39m \n\u001b[32m    400\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    487\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    488\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gokde\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\optuna\\study\\_optimize.py:64\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     77\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gokde\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\optuna\\study\\_optimize.py:161\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     frozen_trial = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gokde\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\optuna\\study\\_optimize.py:253\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    246\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    249\u001b[39m     frozen_trial.state == TrialState.FAIL\n\u001b[32m    250\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    251\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    252\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gokde\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\optuna\\study\\_optimize.py:201\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    203\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    204\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 233\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m    227\u001b[39m total_params = \u001b[38;5;28msum\u001b[39m(p.numel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model.parameters())\n\u001b[32m    228\u001b[39m wandb.config.update({\n\u001b[32m    229\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtotal_parameters\u001b[39m\u001b[33m\"\u001b[39m: total_params,\n\u001b[32m    230\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mparams_per_sample\u001b[39m\u001b[33m\"\u001b[39m: total_params / \u001b[38;5;28mlen\u001b[39m(X_train_trial)\n\u001b[32m    231\u001b[39m })\n\u001b[32m--> \u001b[39m\u001b[32m233\u001b[39m trained_model, history = \u001b[43mtrain_lstm_robust_antioverfitting\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train_trial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val_trial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[38;5;66;03m# Report to Optuna\u001b[39;00m\n\u001b[32m    238\u001b[39m trial.report(history[\u001b[33m'\u001b[39m\u001b[33mbest_val_f1\u001b[39m\u001b[33m'\u001b[39m], step=\u001b[32m50\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36mtrain_lstm_robust_antioverfitting\u001b[39m\u001b[34m(X_train, y_train, X_val, y_val, params, model, epochs, trial)\u001b[39m\n\u001b[32m     62\u001b[39m outputs, y_batch = outputs.view(-\u001b[32m1\u001b[39m), y_batch.view(-\u001b[32m1\u001b[39m)\n\u001b[32m     64\u001b[39m loss = criterion(outputs, y_batch)\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), params[\u001b[33m'\u001b[39m\u001b[33mgradient_clip\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     67\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gokde\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    573\u001b[39m         Tensor.backward,\n\u001b[32m    574\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         inputs=inputs,\n\u001b[32m    580\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gokde\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gokde\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# STEP 7: ROBUST TRAINING FUNCTION WITH OPTUNA & W&B LOGGING\n",
    "# ==============================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from sklearn.metrics import f1_score, roc_auc_score, classification_report\n",
    "import wandb\n",
    "\n",
    "def train_lstm_robust_antioverfitting(X_train, y_train, X_val, y_val, params, model, epochs=30, trial=None):\n",
    "    \"\"\"\n",
    "    Robust LSTM training with proper W&B epoch-by-epoch logging\n",
    "    \"\"\"\n",
    "    # Initialize W&B run with unique name per trial\n",
    "    wandb.init(\n",
    "        project=\"lstm-cybersecurity-optuna\",\n",
    "        config=params,\n",
    "        reinit=True,\n",
    "        name=f\"trial_{trial.number if trial else 'manual'}\"\n",
    "    )\n",
    "\n",
    "    # Prepare datasets\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    X_val = X_val.astype(np.float32)\n",
    "    y_train = y_train.astype(np.float32)\n",
    "    y_val = y_val.astype(np.float32)\n",
    "\n",
    "    train_dataset = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "    val_dataset = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=params['batch_size'], shuffle=False)\n",
    "\n",
    "    # Optimizer & scheduler\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'], weight_decay=params['l2_reg'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.6, patience=params['reduce_lr_patience'], verbose=True\n",
    "    )\n",
    "\n",
    "    # Loss with class weights\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([params['pos_weight']], device=device))\n",
    "\n",
    "    # Tracking\n",
    "    best_val_f1, patience_counter = 0, 0\n",
    "    best_model_state = None\n",
    "    train_losses, val_losses = [], []\n",
    "    train_f1s, val_f1s, val_aucs = [], [], []\n",
    "\n",
    "    print(f\"Starting training for {epochs} epochs...\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # ===== TRAIN =====\n",
    "        model.train()\n",
    "        epoch_loss, train_preds_all, train_targets_all = 0, [], []\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            if outputs.dim() > 1: outputs = outputs.squeeze(-1)\n",
    "            if y_batch.dim() > 1: y_batch = y_batch.squeeze(-1)\n",
    "            outputs, y_batch = outputs.view(-1), y_batch.view(-1)\n",
    "\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), params['gradient_clip'])\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "                preds = (probs >= 0.5).astype(int)\n",
    "                train_preds_all.extend(preds)\n",
    "                train_targets_all.extend(y_batch.cpu().numpy())\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_f1 = f1_score(train_targets_all, train_preds_all, zero_division=0)\n",
    "        train_f1s.append(train_f1)\n",
    "\n",
    "        # ===== VALIDATION =====\n",
    "        model.eval()\n",
    "        val_loss, val_probs_all, val_targets_all = 0, [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                if outputs.dim() > 1: outputs = outputs.squeeze(-1)\n",
    "                if y_batch.dim() > 1: y_batch = y_batch.squeeze(-1)\n",
    "                outputs, y_batch = outputs.view(-1), y_batch.view(-1)\n",
    "\n",
    "                val_loss += criterion(outputs, y_batch).item()\n",
    "                probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "                val_probs_all.extend(probs)\n",
    "                val_targets_all.extend(y_batch.cpu().numpy())\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        val_probs_all = np.array(val_probs_all)\n",
    "        val_targets_all = np.array(val_targets_all)\n",
    "\n",
    "        # AUC\n",
    "        val_auc = roc_auc_score(val_targets_all, val_probs_all) if len(np.unique(val_targets_all)) > 1 else 0.5\n",
    "        val_aucs.append(val_auc)\n",
    "\n",
    "        # Best F1 across thresholds\n",
    "        best_val_f1_epoch = 0\n",
    "        for thresh in np.arange(0.1, 0.9, 0.05):\n",
    "            preds = (val_probs_all >= thresh).astype(int)\n",
    "            f1 = f1_score(val_targets_all, preds, zero_division=0)\n",
    "            best_val_f1_epoch = max(best_val_f1_epoch, f1)\n",
    "        val_f1s.append(best_val_f1_epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:2d} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} \"\n",
    "              f\"| Train F1: {train_f1:.4f} | Val F1: {best_val_f1_epoch:.4f} | Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        # ===== W&B LOGGING =====\n",
    "        wandb.log({\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"val_loss\": avg_val_loss,\n",
    "            \"train_f1\": train_f1,\n",
    "            \"val_f1\": best_val_f1_epoch,\n",
    "            \"val_auc\": val_auc,\n",
    "            \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "            \"loss_gap\": avg_val_loss - avg_train_loss,\n",
    "            \"patience_counter\": patience_counter\n",
    "        }, step=epoch)\n",
    "        wandb.run.log({\"epoch_end\": epoch})\n",
    "\n",
    "        # ===== EARLY STOPPING =====\n",
    "        if best_val_f1_epoch > best_val_f1 + params.get('min_delta', 1e-4):\n",
    "            best_val_f1 = best_val_f1_epoch\n",
    "            best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= params['early_stopping_patience']:\n",
    "                print(f\"⏹ Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        scheduler.step(best_val_f1_epoch)\n",
    "\n",
    "        # ===== OPTUNA REPORT =====\n",
    "        if trial is not None:\n",
    "            trial.report(best_val_f1_epoch, epoch)\n",
    "            if trial.should_prune():\n",
    "                print(f\"🔥 Trial pruned at epoch {epoch+1}\")\n",
    "                wandb.finish()\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    # Restore best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict({k: v.to(device) for k, v in best_model_state.items()})\n",
    "        print(f\"✅ Restored best model with validation F1: {best_val_f1:.4f}\")\n",
    "    else:\n",
    "        print(\"⚠️ No improvement found, using final model\")\n",
    "\n",
    "    # Log final metrics\n",
    "    wandb.log({\n",
    "        \"final_val_f1\": best_val_f1,\n",
    "        \"final_val_auc\": val_aucs[-1] if val_aucs else 0,\n",
    "        \"total_epochs\": len(train_losses),\n",
    "        \"converged\": patience_counter < params['early_stopping_patience']\n",
    "    })\n",
    "    wandb.finish()\n",
    "\n",
    "    return model, {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_f1s': train_f1s,\n",
    "        'val_f1s': val_f1s,\n",
    "        'val_aucs': val_aucs,\n",
    "        'best_val_f1': best_val_f1\n",
    "    }\n",
    "\n",
    "# ==============================\n",
    "# OPTUNA OBJECTIVE FUNCTION\n",
    "# ==============================\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function compatible with GridSampler\n",
    "    \"\"\"\n",
    "    # Map trial parameters (from GridSampler)\n",
    "    params = {\n",
    "        'hidden_dim': trial.suggest_categorical('hidden_dim', [4,8,16,32,48,64,96,128]),\n",
    "        'num_layers': trial.suggest_categorical('num_layers', [1,2,3,4]),\n",
    "        'dropout': trial.suggest_categorical('dropout', [0.2,0.3,0.4,0.5,0.6,0.7,0.8]),\n",
    "        'use_attention': trial.suggest_categorical('use_attention', [True, False]),\n",
    "        'bidirectional': trial.suggest_categorical('bidirectional', [True, False]),\n",
    "        'sequence_length': trial.suggest_categorical('sequence_length', [2,4,8,16,24,32,48,64,100,128]),\n",
    "        'lr': 0.001,\n",
    "        'batch_size': 64,\n",
    "        'l2_reg': 0.01,\n",
    "        'gradient_clip': 1.0,\n",
    "        'pos_weight': 2.0,\n",
    "        'early_stopping_patience': 8,\n",
    "        'reduce_lr_patience': 4,\n",
    "        'min_delta': 1e-4\n",
    "    }\n",
    "\n",
    "    # Initialize W&B run for this trial\n",
    "    wandb.init(\n",
    "        project=\"trial-5-\", #title\n",
    "        config=params,\n",
    "        reinit=True,\n",
    "        name=f\"trial_{trial.number}\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        seq_len = params['sequence_length']\n",
    "        X_train_trial = X_train_seq[:, :seq_len, :] if seq_len != 20 else X_train_seq\n",
    "        X_val_trial = X_val_seq[:, :seq_len, :] if seq_len != 20 else X_val_seq\n",
    "\n",
    "        model = AdvancedLSTM(\n",
    "            input_dim=X_train_trial.shape[2],\n",
    "            hidden_dim=params['hidden_dim'],\n",
    "            num_layers=params['num_layers'],\n",
    "            dropout=params['dropout'],\n",
    "            use_attention=params['use_attention'],\n",
    "            bidirectional=params['bidirectional']\n",
    "        ).to(device)\n",
    "\n",
    "        # Derived hyperparameters\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        wandb.config.update({\n",
    "            \"total_parameters\": total_params,\n",
    "            \"params_per_sample\": total_params / len(X_train_trial)\n",
    "        })\n",
    "\n",
    "        trained_model, history = train_lstm_robust_antioverfitting(\n",
    "            X_train_trial, y_train_seq, X_val_trial, y_val_seq, params, model, epochs=50, trial=trial\n",
    "        )\n",
    "\n",
    "        # Report to Optuna\n",
    "        trial.report(history['best_val_f1'], step=50)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        return history['best_val_f1']\n",
    "\n",
    "    except optuna.exceptions.TrialPruned:\n",
    "        raise\n",
    "    finally:\n",
    "        wandb.finish()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# RUN GRID SEARCH\n",
    "# -----------------------------\n",
    "param_grid = {\n",
    "    'hidden_dim': [4,8,16,32,48,64,96,128],\n",
    "    'num_layers': [1,2,3,4],\n",
    "    'dropout': [0.2,0.3,0.4,0.5,0.6,0.7,0.8],\n",
    "    'use_attention': [True, False],\n",
    "    'bidirectional': [True, False],\n",
    "    'sequence_length': [2,4,8,16,24,32,48,64,100,128]\n",
    "}\n",
    "\n",
    "sampler = optuna.samplers.GridSampler(search_space=param_grid)\n",
    "study = optuna.create_study(direction='maximize', sampler=sampler)\n",
    "study.optimize(objective, n_trials=None)  # Explore all combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faab2f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "=== FIXED LSTM OPTIMIZATION ===\n",
      "Target: Fix declining F1/AUC performance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 14:56:38,638] A new study created in memory with name: fixed_lstm_1756727798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing parameters...\n",
      "\n",
      "FIXED Trial #0\n",
      "=== FIXED Data Preprocessing ===\n",
      "After cleaning - Train: 13675, Val: 7264, Test: 7264\n",
      "Class balance - Train: 0.560, Val: 0.365\n",
      "Calculated pos_weight: 0.89\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "creating run (0.2s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\gokde\\OneDrive\\Masaüstü\\Anomaly-Detection-and-Network-Optimization-in-SDNs\\real_project\\wandb\\run-20250901_145642-8i5bisrp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/8i5bisrp' target=\"_blank\">fixed_trial_0_h4_l1_bi1_att0</a></strong> to <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/8i5bisrp' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/8i5bisrp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with FIXED approach - monitoring for declining performance\n",
      "All hyperparameters logged to W&B project: lstm-cybersecurity-fixed\n",
      "Epoch  1 | Loss: T=0.6549 V=0.6329 Gap=-0.0220\n",
      "         | F1: T=0.5802 V=0.5761 | AUC: 0.7175\n",
      "         | Combined Performance: 0.6246\n",
      "    New best F1: 0.5761\n",
      "Epoch  2 | Loss: T=0.6512 V=0.6300 Gap=-0.0212\n",
      "         | F1: T=0.6006 V=0.5784 | AUC: 0.7310\n",
      "         | Combined Performance: 0.6367\n",
      "    New best F1: 0.5784\n",
      "Epoch  3 | Loss: T=0.6473 V=0.6271 Gap=-0.0202\n",
      "         | F1: T=0.6188 V=0.5804 | AUC: 0.7405\n",
      "         | Combined Performance: 0.6465\n",
      "    New best F1: 0.5804\n",
      "Epoch  4 | Loss: T=0.6430 V=0.6240 Gap=-0.0190\n",
      "         | F1: T=0.6362 V=0.5822 | AUC: 0.7488\n",
      "         | Combined Performance: 0.6558\n",
      "    New best F1: 0.5822\n",
      "Epoch  5 | Loss: T=0.6381 V=0.6208 Gap=-0.0173\n",
      "         | F1: T=0.6525 V=0.5839 | AUC: 0.7579\n",
      "         | Combined Performance: 0.6648\n",
      "    New best F1: 0.5839\n",
      "Epoch  6 | Loss: T=0.6333 V=0.6173 Gap=-0.0160\n",
      "         | F1: T=0.6690 V=0.5870 | AUC: 0.7673\n",
      "         | Combined Performance: 0.6744\n",
      "    New best F1: 0.5870\n",
      "Epoch  7 | Loss: T=0.6295 V=0.6136 Gap=-0.0159\n",
      "         | F1: T=0.6834 V=0.6110 | AUC: 0.7762\n",
      "         | Combined Performance: 0.6902\n",
      "    New best F1: 0.6110\n",
      "Epoch  8 | Loss: T=0.6244 V=0.6096 Gap=-0.0148\n",
      "         | F1: T=0.6973 V=0.6291 | AUC: 0.7845\n",
      "         | Combined Performance: 0.7036\n",
      "    New best F1: 0.6291\n",
      "Epoch  9 | Loss: T=0.6188 V=0.6054 Gap=-0.0133\n",
      "         | F1: T=0.7160 V=0.6333 | AUC: 0.7928\n",
      "         | Combined Performance: 0.7141\n",
      "    New best F1: 0.6333\n",
      "Epoch 10 | Loss: T=0.6133 V=0.6011 Gap=-0.0122\n",
      "         | F1: T=0.7257 V=0.6355 | AUC: 0.8013\n",
      "         | Combined Performance: 0.7208\n",
      "    New best F1: 0.6355\n",
      "Epoch 11 | Loss: T=0.6098 V=0.5969 Gap=-0.0129\n",
      "         | F1: T=0.7327 V=0.6397 | AUC: 0.8099\n",
      "         | Combined Performance: 0.7274\n",
      "    New best F1: 0.6397\n",
      "Epoch 12 | Loss: T=0.6037 V=0.5928 Gap=-0.0109\n",
      "         | F1: T=0.7463 V=0.6425 | AUC: 0.8179\n",
      "         | Combined Performance: 0.7356\n",
      "    New best F1: 0.6425\n",
      "Epoch 13 | Loss: T=0.5973 V=0.5886 Gap=-0.0087\n",
      "         | F1: T=0.7548 V=0.6451 | AUC: 0.8257\n",
      "         | Combined Performance: 0.7419\n",
      "    New best F1: 0.6451\n",
      "Epoch 14 | Loss: T=0.5911 V=0.5843 Gap=-0.0068\n",
      "         | F1: T=0.7679 V=0.6463 | AUC: 0.8332\n",
      "         | Combined Performance: 0.7491\n",
      "    New best F1: 0.6463\n",
      "Epoch 15 | Loss: T=0.5852 V=0.5801 Gap=-0.0052\n",
      "         | F1: T=0.7802 V=0.6543 | AUC: 0.8413\n",
      "         | Combined Performance: 0.7586\n",
      "    New best F1: 0.6543\n",
      "Epoch 16 | Loss: T=0.5799 V=0.5757 Gap=-0.0043\n",
      "         | F1: T=0.7913 V=0.6605 | AUC: 0.8501\n",
      "         | Combined Performance: 0.7673\n",
      "    New best F1: 0.6605\n",
      "Epoch 17 | Loss: T=0.5734 V=0.5712 Gap=-0.0021\n",
      "         | F1: T=0.7944 V=0.6681 | AUC: 0.8590\n",
      "         | Combined Performance: 0.7738\n",
      "    New best F1: 0.6681\n",
      "Epoch 18 | Loss: T=0.5661 V=0.5667 Gap=0.0006\n",
      "         | F1: T=0.8064 V=0.6757 | AUC: 0.8674\n",
      "         | Combined Performance: 0.7832\n",
      "    New best F1: 0.6757\n",
      "Epoch 19 | Loss: T=0.5586 V=0.5620 Gap=0.0034\n",
      "         | F1: T=0.8133 V=0.7009 | AUC: 0.8752\n",
      "         | Combined Performance: 0.7965\n",
      "    New best F1: 0.7009\n",
      "Epoch 20 | Loss: T=0.5498 V=0.5571 Gap=0.0073\n",
      "         | F1: T=0.8289 V=0.7101 | AUC: 0.8822\n",
      "         | Combined Performance: 0.8071\n",
      "    New best F1: 0.7101\n",
      "Epoch 21 | Loss: T=0.5444 V=0.5522 Gap=0.0078\n",
      "         | F1: T=0.8376 V=0.7163 | AUC: 0.8887\n",
      "         | Combined Performance: 0.8142\n",
      "    New best F1: 0.7163\n",
      "Epoch 22 | Loss: T=0.5353 V=0.5471 Gap=0.0119\n",
      "         | F1: T=0.8413 V=0.7223 | AUC: 0.8947\n",
      "         | Combined Performance: 0.8194\n",
      "    New best F1: 0.7223\n",
      "Epoch 23 | Loss: T=0.5272 V=0.5418 Gap=0.0146\n",
      "         | F1: T=0.8498 V=0.7320 | AUC: 0.9001\n",
      "         | Combined Performance: 0.8273\n",
      "    New best F1: 0.7320\n",
      "Epoch 24 | Loss: T=0.5198 V=0.5364 Gap=0.0166\n",
      "         | F1: T=0.8575 V=0.7441 | AUC: 0.9048\n",
      "         | Combined Performance: 0.8355\n",
      "    New best F1: 0.7441\n",
      "Epoch 25 | Loss: T=0.5103 V=0.5306 Gap=0.0203\n",
      "         | F1: T=0.8612 V=0.7574 | AUC: 0.9090\n",
      "         | Combined Performance: 0.8425\n",
      "    New best F1: 0.7574\n",
      "Epoch 26 | Loss: T=0.5021 V=0.5247 Gap=0.0226\n",
      "         | F1: T=0.8682 V=0.7704 | AUC: 0.9126\n",
      "         | Combined Performance: 0.8504\n",
      "    New best F1: 0.7704\n",
      "Epoch 27 | Loss: T=0.4921 V=0.5186 Gap=0.0265\n",
      "         | F1: T=0.8725 V=0.7943 | AUC: 0.9157\n",
      "         | Combined Performance: 0.8608\n",
      "    New best F1: 0.7943\n",
      "Epoch 28 | Loss: T=0.4814 V=0.5123 Gap=0.0309\n",
      "         | F1: T=0.8844 V=0.7992 | AUC: 0.9185\n",
      "         | Combined Performance: 0.8674\n",
      "    New best F1: 0.7992\n",
      "Epoch 29 | Loss: T=0.4732 V=0.5059 Gap=0.0328\n",
      "         | F1: T=0.8873 V=0.8018 | AUC: 0.9210\n",
      "         | Combined Performance: 0.8701\n",
      "    New best F1: 0.8018\n",
      "Epoch 30 | Loss: T=0.4622 V=0.4995 Gap=0.0373\n",
      "         | F1: T=0.8963 V=0.8035 | AUC: 0.9232\n",
      "         | Combined Performance: 0.8743\n",
      "    New best F1: 0.8035\n",
      "Epoch 31 | Loss: T=0.4528 V=0.4930 Gap=0.0402\n",
      "         | F1: T=0.9026 V=0.8047 | AUC: 0.9252\n",
      "         | Combined Performance: 0.8775\n",
      "    New best F1: 0.8047\n",
      "Epoch 32 | Loss: T=0.4409 V=0.4867 Gap=0.0458\n",
      "         | F1: T=0.9041 V=0.8135 | AUC: 0.9272\n",
      "         | Combined Performance: 0.8816\n",
      "    New best F1: 0.8135\n",
      "Epoch 33 | Loss: T=0.4325 V=0.4802 Gap=0.0477\n",
      "         | F1: T=0.9104 V=0.8173 | AUC: 0.9291\n",
      "         | Combined Performance: 0.8856\n",
      "    New best F1: 0.8173\n",
      "Epoch 34 | Loss: T=0.4194 V=0.4737 Gap=0.0543\n",
      "         | F1: T=0.9169 V=0.8190 | AUC: 0.9309\n",
      "         | Combined Performance: 0.8889\n",
      "    New best F1: 0.8190\n",
      "Epoch 35 | Loss: T=0.4095 V=0.4671 Gap=0.0576\n",
      "         | F1: T=0.9209 V=0.8194 | AUC: 0.9328\n",
      "         | Combined Performance: 0.8910\n",
      "    New best F1: 0.8194\n",
      "Epoch 36 | Loss: T=0.4012 V=0.4607 Gap=0.0595\n",
      "         | F1: T=0.9200 V=0.8213 | AUC: 0.9346\n",
      "         | Combined Performance: 0.8920\n",
      "    New best F1: 0.8213\n",
      "Epoch 37 | Loss: T=0.3892 V=0.4543 Gap=0.0650\n",
      "         | F1: T=0.9236 V=0.8218 | AUC: 0.9366\n",
      "         | Combined Performance: 0.8940\n",
      "    New best F1: 0.8218\n",
      "Epoch 38 | Loss: T=0.3778 V=0.4481 Gap=0.0703\n",
      "         | F1: T=0.9269 V=0.8259 | AUC: 0.9387\n",
      "         | Combined Performance: 0.8971\n",
      "    New best F1: 0.8259\n",
      "Epoch 39 | Loss: T=0.3683 V=0.4419 Gap=0.0736\n",
      "         | F1: T=0.9318 V=0.8277 | AUC: 0.9404\n",
      "         | Combined Performance: 0.9000\n",
      "    New best F1: 0.8277\n",
      "Epoch 40 | Loss: T=0.3565 V=0.4360 Gap=0.0795\n",
      "         | F1: T=0.9348 V=0.8279 | AUC: 0.9418\n",
      "         | Combined Performance: 0.9015\n",
      "    New best F1: 0.8279\n",
      "Epoch 41 | Loss: T=0.3451 V=0.4302 Gap=0.0851\n",
      "         | F1: T=0.9393 V=0.8285 | AUC: 0.9429\n",
      "         | Combined Performance: 0.9035\n",
      "    New best F1: 0.8285\n",
      "Epoch 42 | Loss: T=0.3348 V=0.4247 Gap=0.0898\n",
      "         | F1: T=0.9394 V=0.8309 | AUC: 0.9438\n",
      "         | Combined Performance: 0.9047\n",
      "    New best F1: 0.8309\n",
      "Epoch 43 | Loss: T=0.3259 V=0.4195 Gap=0.0936\n",
      "         | F1: T=0.9411 V=0.8324 | AUC: 0.9445\n",
      "         | Combined Performance: 0.9060\n",
      "    New best F1: 0.8324\n",
      "Epoch 44 | Loss: T=0.3153 V=0.4145 Gap=0.0993\n",
      "         | F1: T=0.9457 V=0.8342 | AUC: 0.9449\n",
      "         | Combined Performance: 0.9083\n",
      "    New best F1: 0.8342\n",
      "Epoch 45 | Loss: T=0.3037 V=0.4100 Gap=0.1063\n",
      "         | F1: T=0.9456 V=0.8357 | AUC: 0.9451\n",
      "         | Combined Performance: 0.9088\n",
      "    New best F1: 0.8357\n",
      "Epoch 46 | Loss: T=0.2971 V=0.4059 Gap=0.1087\n",
      "         | F1: T=0.9473 V=0.8366 | AUC: 0.9448\n",
      "         | Combined Performance: 0.9095\n",
      "    New best F1: 0.8366\n",
      "Epoch 47 | Loss: T=0.2865 V=0.4019 Gap=0.1154\n",
      "         | F1: T=0.9488 V=0.8374 | AUC: 0.9441\n",
      "         | Combined Performance: 0.9101\n",
      "    New best F1: 0.8374\n",
      "Epoch 48 | Loss: T=0.2768 V=0.3983 Gap=0.1215\n",
      "         | F1: T=0.9511 V=0.8375 | AUC: 0.9431\n",
      "         | Combined Performance: 0.9105\n",
      "    New best F1: 0.8375\n",
      "Epoch 49 | Loss: T=0.2688 V=0.3952 Gap=0.1264\n",
      "         | F1: T=0.9513 V=0.8382 | AUC: 0.9420\n",
      "         | Combined Performance: 0.9105\n",
      "    New best F1: 0.8382\n",
      "PERFORMANCE DECLINE DETECTED - stopping training\n",
      "Recent performance: [0.910540243558635, 0.9105098749136489, 0.9104904157328453]\n",
      "Restored best model with F1: 0.8382\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>best_model_saved</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>best_val_f1</td><td>▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▄▅▅▅▅▆▆▇▇▇▇▇▇▇███████████</td></tr><tr><td>combined_performance</td><td>▁▁▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇███████████</td></tr><tr><td>data_summary/feature_dim</td><td>▁</td></tr><tr><td>data_summary/sequence_length</td><td>▁</td></tr><tr><td>decline_detected_epoch</td><td>▁</td></tr><tr><td>decline_stop</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>gradients/avg_norm</td><td>▆▇▇▇▇▇███▆▇▆▆▆▆▆▅▅▆▆▅▆▅▅▆▆▆▅▅▅▅▄▅▄▄▄▃▂▂▁</td></tr><tr><td>+50</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>49</td></tr><tr><td>best_model_saved</td><td>1</td></tr><tr><td>best_val_f1</td><td>0.83818</td></tr><tr><td>combined_performance</td><td>0.91051</td></tr><tr><td>data_summary/feature_dim</td><td>50</td></tr><tr><td>data_summary/sequence_length</td><td>128</td></tr><tr><td>decline_detected_epoch</td><td>50</td></tr><tr><td>decline_stop</td><td>1</td></tr><tr><td>epoch</td><td>49</td></tr><tr><td>gradients/avg_norm</td><td>0.12288</td></tr><tr><td>+53</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fixed_trial_0_h4_l1_bi1_att0</strong> at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/8i5bisrp' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/8i5bisrp</a><br> View project at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250901_145642-8i5bisrp\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0: F1=0.8382, Decrease_penalty=1.8293, Gap_penalty=0.2633, Score=-1.2545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 14:57:07,310] Trial 0 finished with value: -1.2544879968650635 and parameters: {'hidden_dim': 4, 'num_layers': 1, 'dropout': 0.29761191306325957, 'use_attention': False, 'bidirectional': True, 'sequence_length': 128, 'lr': 0.00011505137493659554, 'batch_size': 512, 'l2_reg': 3.4745331159811874e-05, 'gradient_clip': 4.217524229198063, 'l1_reg': 7.216654076139157e-05, 'early_stopping_patience': 35, 'min_delta': 1.4443628656009442e-06}. Best is trial 0 with value: -1.2544879968650635.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FIXED Trial #1\n",
      "=== FIXED Data Preprocessing ===\n",
      "After cleaning - Train: 13675, Val: 7264, Test: 7264\n",
      "Class balance - Train: 0.560, Val: 0.365\n",
      "Calculated pos_weight: 0.89\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "creating run (0.2s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\gokde\\OneDrive\\Masaüstü\\Anomaly-Detection-and-Network-Optimization-in-SDNs\\real_project\\wandb\\run-20250901_145712-ual3f4vm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/ual3f4vm' target=\"_blank\">fixed_trial_1_h48_l4_bi1_att1</a></strong> to <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/ual3f4vm' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/ual3f4vm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with FIXED approach - monitoring for declining performance\n",
      "All hyperparameters logged to W&B project: lstm-cybersecurity-fixed\n",
      "Epoch  1 | Loss: T=1.3608 V=1.1804 Gap=-0.1805\n",
      "         | F1: T=0.7328 V=0.5346 | AUC: 0.3570\n",
      "         | Combined Performance: 0.5414\n",
      "    New best F1: 0.5346\n",
      "Epoch  2 | Loss: T=0.8610 V=1.4433 Gap=0.5823\n",
      "         | F1: T=0.8233 V=0.5821 | AUC: 0.8032\n",
      "         | Combined Performance: 0.7362\n",
      "    New best F1: 0.5821\n",
      "Epoch  3 | Loss: T=0.5118 V=4.1394 Gap=3.6276\n",
      "         | F1: T=0.8132 V=0.5346 | AUC: 0.7400\n",
      "         | Combined Performance: 0.6959\n",
      "PERFORMANCE DECLINE DETECTED - stopping training\n",
      "Recent performance: [0.7361888794363244, 0.6959399738070999, 0.6026112035806764]\n",
      "Restored best model with F1: 0.5821\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁█</td></tr><tr><td>best_model_saved</td><td>▁▁</td></tr><tr><td>best_val_f1</td><td>▁█</td></tr><tr><td>combined_performance</td><td>▁█▇</td></tr><tr><td>data_summary/feature_dim</td><td>▁</td></tr><tr><td>data_summary/sequence_length</td><td>▁</td></tr><tr><td>decline_detected_epoch</td><td>▁</td></tr><tr><td>decline_stop</td><td>▁</td></tr><tr><td>epoch</td><td>▁▅█</td></tr><tr><td>gradients/avg_norm</td><td>▅█▁</td></tr><tr><td>+50</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>2</td></tr><tr><td>best_model_saved</td><td>1</td></tr><tr><td>best_val_f1</td><td>0.58208</td></tr><tr><td>combined_performance</td><td>0.69594</td></tr><tr><td>data_summary/feature_dim</td><td>50</td></tr><tr><td>data_summary/sequence_length</td><td>160</td></tr><tr><td>decline_detected_epoch</td><td>4</td></tr><tr><td>decline_stop</td><td>1</td></tr><tr><td>epoch</td><td>3</td></tr><tr><td>gradients/avg_norm</td><td>0.01075</td></tr><tr><td>+53</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fixed_trial_1_h48_l4_bi1_att1</strong> at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/ual3f4vm' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/ual3f4vm</a><br> View project at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250901_145712-ual3f4vm\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1: F1=0.5821, Decrease_penalty=0.6652, Gap_penalty=8.3129, Score=-8.3960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 14:57:22,656] Trial 1 finished with value: -8.395951515257552 and parameters: {'hidden_dim': 48, 'num_layers': 4, 'dropout': 0.3310570218136604, 'use_attention': True, 'bidirectional': True, 'sequence_length': 160, 'lr': 0.008132060789295221, 'batch_size': 256, 'l2_reg': 0.0024676658263468477, 'gradient_clip': 0.6871024772960626, 'l1_reg': 1.356064234420807e-07, 'early_stopping_patience': 20, 'min_delta': 2.7356405890355384e-06}. Best is trial 0 with value: -1.2544879968650635.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FIXED Trial #2\n",
      "=== FIXED Data Preprocessing ===\n",
      "After cleaning - Train: 13675, Val: 7264, Test: 7264\n",
      "Class balance - Train: 0.560, Val: 0.365\n",
      "Calculated pos_weight: 0.89\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "creating run (0.2s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\gokde\\OneDrive\\Masaüstü\\Anomaly-Detection-and-Network-Optimization-in-SDNs\\real_project\\wandb\\run-20250901_145723-f0mfyvfg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/f0mfyvfg' target=\"_blank\">fixed_trial_2_h128_l8_bi0_att1</a></strong> to <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/f0mfyvfg' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/f0mfyvfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with FIXED approach - monitoring for declining performance\n",
      "All hyperparameters logged to W&B project: lstm-cybersecurity-fixed\n",
      "Epoch  1 | Loss: T=0.7502 V=0.6630 Gap=-0.0872\n",
      "         | F1: T=0.6473 V=0.5346 | AUC: 0.5000\n",
      "         | Combined Performance: 0.5606\n",
      "    New best F1: 0.5346\n",
      "Epoch  2 | Loss: T=0.6542 V=0.6636 Gap=0.0094\n",
      "         | F1: T=0.7174 V=0.5346 | AUC: 0.5000\n",
      "         | Combined Performance: 0.5840\n",
      "Epoch  3 | Loss: T=0.6502 V=0.6641 Gap=0.0139\n",
      "         | F1: T=0.7179 V=0.5346 | AUC: 0.5000\n",
      "         | Combined Performance: 0.5842\n",
      "Epoch  4 | Loss: T=0.6499 V=0.6649 Gap=0.0150\n",
      "         | F1: T=0.7179 V=0.5346 | AUC: 0.5000\n",
      "         | Combined Performance: 0.5842\n",
      "PERFORMANCE DECLINE DETECTED - stopping training\n",
      "Recent performance: [0.5841620522908665, 0.5841620522908665, 0.5841620522908665]\n",
      "Restored best model with F1: 0.5346\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁</td></tr><tr><td>best_model_saved</td><td>▁</td></tr><tr><td>best_val_f1</td><td>▁</td></tr><tr><td>combined_performance</td><td>▁███</td></tr><tr><td>data_summary/feature_dim</td><td>▁</td></tr><tr><td>data_summary/sequence_length</td><td>▁</td></tr><tr><td>decline_detected_epoch</td><td>▁</td></tr><tr><td>decline_stop</td><td>▁</td></tr><tr><td>epoch</td><td>▁▃▆█</td></tr><tr><td>gradients/avg_norm</td><td>█▃▁▁</td></tr><tr><td>+50</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>1</td></tr><tr><td>best_model_saved</td><td>1</td></tr><tr><td>best_val_f1</td><td>0.5346</td></tr><tr><td>combined_performance</td><td>0.58416</td></tr><tr><td>data_summary/feature_dim</td><td>50</td></tr><tr><td>data_summary/sequence_length</td><td>64</td></tr><tr><td>decline_detected_epoch</td><td>5</td></tr><tr><td>decline_stop</td><td>1</td></tr><tr><td>epoch</td><td>4</td></tr><tr><td>gradients/avg_norm</td><td>0.01314</td></tr><tr><td>+53</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fixed_trial_2_h128_l8_bi0_att1</strong> at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/f0mfyvfg' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/f0mfyvfg</a><br> View project at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250901_145723-f0mfyvfg\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2: F1=0.5346, Decrease_penalty=4.4973, Gap_penalty=0.0317, Score=-3.9944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 14:57:37,099] Trial 2 finished with value: -3.9943867628964664 and parameters: {'hidden_dim': 128, 'num_layers': 8, 'dropout': 0.42847319822079144, 'use_attention': True, 'bidirectional': False, 'sequence_length': 64, 'lr': 0.003309948812108701, 'batch_size': 512, 'l2_reg': 0.0031393455895492397, 'gradient_clip': 5.1276816402364895, 'l1_reg': 1.040065363363919e-08, 'early_stopping_patience': 10, 'min_delta': 0.0002824294606833324}. Best is trial 0 with value: -1.2544879968650635.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FIXED Trial #3\n",
      "=== FIXED Data Preprocessing ===\n",
      "After cleaning - Train: 13675, Val: 7264, Test: 7264\n",
      "Class balance - Train: 0.560, Val: 0.365\n",
      "Calculated pos_weight: 0.89\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "creating run (0.2s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\gokde\\OneDrive\\Masaüstü\\Anomaly-Detection-and-Network-Optimization-in-SDNs\\real_project\\wandb\\run-20250901_145739-t1fxp8r6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/t1fxp8r6' target=\"_blank\">fixed_trial_3_h8_l6_bi0_att0</a></strong> to <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/t1fxp8r6' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/t1fxp8r6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with FIXED approach - monitoring for declining performance\n",
      "All hyperparameters logged to W&B project: lstm-cybersecurity-fixed\n",
      "Epoch  1 | Loss: T=0.7048 V=0.6771 Gap=-0.0276\n",
      "         | F1: T=0.6998 V=0.5346 | AUC: 0.8033\n",
      "         | Combined Performance: 0.6792\n",
      "    New best F1: 0.5346\n",
      "Epoch  2 | Loss: T=0.6649 V=0.6730 Gap=0.0081\n",
      "         | F1: T=0.7179 V=0.5346 | AUC: 0.5851\n",
      "         | Combined Performance: 0.6125\n",
      "Epoch  3 | Loss: T=0.6569 V=0.6721 Gap=0.0152\n",
      "         | F1: T=0.7179 V=0.5346 | AUC: 0.5000\n",
      "         | Combined Performance: 0.5842\n",
      "PERFORMANCE DECLINE DETECTED - stopping training\n",
      "Recent performance: [0.6125357195273058, 0.5841620522908665, 0.5841620522908665]\n",
      "Restored best model with F1: 0.5346\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁</td></tr><tr><td>best_model_saved</td><td>▁</td></tr><tr><td>best_val_f1</td><td>▁</td></tr><tr><td>combined_performance</td><td>█▃▁</td></tr><tr><td>data_summary/feature_dim</td><td>▁</td></tr><tr><td>data_summary/sequence_length</td><td>▁</td></tr><tr><td>decline_detected_epoch</td><td>▁</td></tr><tr><td>decline_stop</td><td>▁</td></tr><tr><td>epoch</td><td>▁▅█</td></tr><tr><td>gradients/avg_norm</td><td>█▃▁</td></tr><tr><td>+50</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>1</td></tr><tr><td>best_model_saved</td><td>1</td></tr><tr><td>best_val_f1</td><td>0.5346</td></tr><tr><td>combined_performance</td><td>0.58416</td></tr><tr><td>data_summary/feature_dim</td><td>50</td></tr><tr><td>data_summary/sequence_length</td><td>128</td></tr><tr><td>decline_detected_epoch</td><td>4</td></tr><tr><td>decline_stop</td><td>1</td></tr><tr><td>epoch</td><td>3</td></tr><tr><td>gradients/avg_norm</td><td>0.02656</td></tr><tr><td>+53</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fixed_trial_3_h8_l6_bi0_att0</strong> at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/t1fxp8r6' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/t1fxp8r6</a><br> View project at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250901_145739-t1fxp8r6\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3: F1=0.5346, Decrease_penalty=4.7252, Gap_penalty=0.0342, Score=-4.2248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 14:57:44,725] Trial 3 finished with value: -4.224845181755843 and parameters: {'hidden_dim': 8, 'num_layers': 6, 'dropout': 0.5432975278555351, 'use_attention': False, 'bidirectional': False, 'sequence_length': 128, 'lr': 0.006349782888468667, 'batch_size': 512, 'l2_reg': 0.0005141807933107058, 'gradient_clip': 5.092724337141967, 'l1_reg': 5.0585470146109934e-05, 'early_stopping_patience': 55, 'min_delta': 0.0008301415500743176}. Best is trial 0 with value: -1.2544879968650635.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FIXED Trial #4\n",
      "=== FIXED Data Preprocessing ===\n",
      "After cleaning - Train: 13675, Val: 7264, Test: 7264\n",
      "Class balance - Train: 0.560, Val: 0.365\n",
      "Calculated pos_weight: 0.89\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "creating run (0.2s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\gokde\\OneDrive\\Masaüstü\\Anomaly-Detection-and-Network-Optimization-in-SDNs\\real_project\\wandb\\run-20250901_145747-3ccm2wys</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/3ccm2wys' target=\"_blank\">fixed_trial_4_h48_l2_bi1_att0</a></strong> to <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/3ccm2wys' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/3ccm2wys</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with FIXED approach - monitoring for declining performance\n",
      "All hyperparameters logged to W&B project: lstm-cybersecurity-fixed\n",
      "Epoch  1 | Loss: T=0.4636 V=0.5514 Gap=0.0878\n",
      "         | F1: T=0.9172 V=0.8343 | AUC: 0.9664\n",
      "         | Combined Performance: 0.9060\n",
      "    New best F1: 0.8343\n",
      "Epoch  2 | Loss: T=0.2001 V=0.7440 Gap=0.5439\n",
      "         | F1: T=0.9805 V=0.8462 | AUC: 0.9495\n",
      "         | Combined Performance: 0.9254\n",
      "    New best F1: 0.8462\n",
      "Epoch  3 | Loss: T=0.1679 V=1.1813 Gap=1.0134\n",
      "         | F1: T=0.9832 V=0.7902 | AUC: 0.9160\n",
      "         | Combined Performance: 0.8965\n",
      "Epoch  4 | Loss: T=0.1476 V=0.8192 Gap=0.6717\n",
      "         | F1: T=0.9867 V=0.8451 | AUC: 0.9374\n",
      "         | Combined Performance: 0.9231\n",
      "Epoch  5 | Loss: T=0.1287 V=0.8912 Gap=0.7624\n",
      "         | F1: T=0.9888 V=0.8443 | AUC: 0.8525\n",
      "         | Combined Performance: 0.8952\n",
      "PERFORMANCE DECLINE DETECTED - stopping training\n",
      "Recent performance: [0.9230508252619242, 0.8952436332885023, 0.8923429413206908]\n",
      "Restored best model with F1: 0.8462\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁█</td></tr><tr><td>best_model_saved</td><td>▁▁</td></tr><tr><td>best_val_f1</td><td>▁█</td></tr><tr><td>combined_performance</td><td>▃█▁▇▁</td></tr><tr><td>data_summary/feature_dim</td><td>▁</td></tr><tr><td>data_summary/sequence_length</td><td>▁</td></tr><tr><td>decline_detected_epoch</td><td>▁</td></tr><tr><td>decline_stop</td><td>▁</td></tr><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>gradients/avg_norm</td><td>█▂▁▂▂</td></tr><tr><td>+50</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>2</td></tr><tr><td>best_model_saved</td><td>1</td></tr><tr><td>best_val_f1</td><td>0.84615</td></tr><tr><td>combined_performance</td><td>0.89524</td></tr><tr><td>data_summary/feature_dim</td><td>50</td></tr><tr><td>data_summary/sequence_length</td><td>160</td></tr><tr><td>decline_detected_epoch</td><td>6</td></tr><tr><td>decline_stop</td><td>1</td></tr><tr><td>epoch</td><td>5</td></tr><tr><td>gradients/avg_norm</td><td>0.00296</td></tr><tr><td>+53</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fixed_trial_4_h48_l2_bi1_att0</strong> at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/3ccm2wys' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/3ccm2wys</a><br> View project at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250901_145747-3ccm2wys\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4: F1=0.8462, Decrease_penalty=3.2706, Gap_penalty=1.5848, Score=-4.0093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 14:58:11,645] Trial 4 finished with value: -4.0092998374762425 and parameters: {'hidden_dim': 48, 'num_layers': 2, 'dropout': 0.1707877478354961, 'use_attention': False, 'bidirectional': True, 'sequence_length': 160, 'lr': 0.00013197129348085557, 'batch_size': 32, 'l2_reg': 0.0006487461892987057, 'gradient_clip': 8.293720252567955, 'l1_reg': 2.7394162005065242e-05, 'early_stopping_patience': 50, 'min_delta': 3.527066371394002e-05}. Best is trial 0 with value: -1.2544879968650635.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FIXED Trial #5\n",
      "=== FIXED Data Preprocessing ===\n",
      "After cleaning - Train: 13675, Val: 7264, Test: 7264\n",
      "Class balance - Train: 0.560, Val: 0.365\n",
      "Calculated pos_weight: 0.89\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\gokde\\OneDrive\\Masaüstü\\Anomaly-Detection-and-Network-Optimization-in-SDNs\\real_project\\wandb\\run-20250901_145812-bfhjq7tg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/bfhjq7tg' target=\"_blank\">fixed_trial_5_h48_l4_bi0_att1</a></strong> to <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/bfhjq7tg' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/bfhjq7tg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with FIXED approach - monitoring for declining performance\n",
      "All hyperparameters logged to W&B project: lstm-cybersecurity-fixed\n",
      "Epoch  1 | Loss: T=0.6456 V=0.6348 Gap=-0.0108\n",
      "         | F1: T=0.7069 V=0.8352 | AUC: 0.9508\n",
      "         | Combined Performance: 0.8310\n",
      "    New best F1: 0.8352\n",
      "Epoch  2 | Loss: T=0.3497 V=0.4633 Gap=0.1136\n",
      "         | F1: T=0.9507 V=0.8444 | AUC: 0.9493\n",
      "         | Combined Performance: 0.9148\n",
      "    New best F1: 0.8444\n",
      "Epoch  3 | Loss: T=0.0965 V=0.5260 Gap=0.4295\n",
      "         | F1: T=0.9815 V=0.8449 | AUC: 0.9646\n",
      "         | Combined Performance: 0.9303\n",
      "    New best F1: 0.8449\n",
      "Epoch  4 | Loss: T=0.0740 V=0.6488 Gap=0.5748\n",
      "         | F1: T=0.9838 V=0.8432 | AUC: 0.9628\n",
      "         | Combined Performance: 0.9299\n",
      "PERFORMANCE DECLINE DETECTED - stopping training\n",
      "Recent performance: [0.9303163399742737, 0.9299388253721014, 0.9285206710508512]\n",
      "Restored best model with F1: 0.8449\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁▅█</td></tr><tr><td>best_model_saved</td><td>▁▁▁</td></tr><tr><td>best_val_f1</td><td>▁██</td></tr><tr><td>combined_performance</td><td>▁▇██</td></tr><tr><td>data_summary/feature_dim</td><td>▁</td></tr><tr><td>data_summary/sequence_length</td><td>▁</td></tr><tr><td>decline_detected_epoch</td><td>▁</td></tr><tr><td>decline_stop</td><td>▁</td></tr><tr><td>epoch</td><td>▁▃▆█</td></tr><tr><td>gradients/avg_norm</td><td>█▂▂▁</td></tr><tr><td>+50</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>3</td></tr><tr><td>best_model_saved</td><td>1</td></tr><tr><td>best_val_f1</td><td>0.84486</td></tr><tr><td>combined_performance</td><td>0.92994</td></tr><tr><td>data_summary/feature_dim</td><td>50</td></tr><tr><td>data_summary/sequence_length</td><td>48</td></tr><tr><td>decline_detected_epoch</td><td>5</td></tr><tr><td>decline_stop</td><td>1</td></tr><tr><td>epoch</td><td>4</td></tr><tr><td>gradients/avg_norm</td><td>0.0253</td></tr><tr><td>+53</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fixed_trial_5_h48_l4_bi0_att1</strong> at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/bfhjq7tg' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/bfhjq7tg</a><br> View project at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250901_145812-bfhjq7tg\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5: F1=0.8449, Decrease_penalty=2.1152, Gap_penalty=1.2446, Score=-2.5149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 14:58:20,799] Trial 5 finished with value: -2.5149008175958025 and parameters: {'hidden_dim': 48, 'num_layers': 4, 'dropout': 0.24490068991527264, 'use_attention': True, 'bidirectional': False, 'sequence_length': 48, 'lr': 0.00023947086384778764, 'batch_size': 128, 'l2_reg': 0.001708860414851174, 'gradient_clip': 5.451197458662668, 'l1_reg': 2.2225294359594367e-07, 'early_stopping_patience': 25, 'min_delta': 4.179136183643003e-05}. Best is trial 0 with value: -1.2544879968650635.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FIXED Trial #6\n",
      "=== FIXED Data Preprocessing ===\n",
      "After cleaning - Train: 13675, Val: 7264, Test: 7264\n",
      "Class balance - Train: 0.560, Val: 0.365\n",
      "Calculated pos_weight: 0.89\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "creating run (0.2s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\gokde\\OneDrive\\Masaüstü\\Anomaly-Detection-and-Network-Optimization-in-SDNs\\real_project\\wandb\\run-20250901_145821-0hp9a5uq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/0hp9a5uq' target=\"_blank\">fixed_trial_6_h512_l8_bi0_att0</a></strong> to <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/0hp9a5uq' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/0hp9a5uq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with FIXED approach - monitoring for declining performance\n",
      "All hyperparameters logged to W&B project: lstm-cybersecurity-fixed\n",
      "Epoch  1 | Loss: T=5.2955 V=3.1719 Gap=-2.1236\n",
      "         | F1: T=0.8427 V=0.5346 | AUC: 0.5000\n",
      "         | Combined Performance: 0.6258\n",
      "    New best F1: 0.5346\n",
      "Epoch  2 | Loss: T=1.7365 V=3.0898 Gap=1.3532\n",
      "         | F1: T=0.8864 V=0.5346 | AUC: 0.5000\n",
      "         | Combined Performance: 0.6403\n",
      "Epoch  3 | Loss: T=1.7187 V=3.0860 Gap=1.3673\n",
      "         | F1: T=0.8799 V=0.5346 | AUC: 0.5000\n",
      "         | Combined Performance: 0.6382\n",
      "PERFORMANCE DECLINE DETECTED - stopping training\n",
      "Recent performance: [0.6403259992510971, 0.6381795981647448, 0.6378075528856676]\n",
      "Restored best model with F1: 0.5346\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁</td></tr><tr><td>best_model_saved</td><td>▁</td></tr><tr><td>best_val_f1</td><td>▁</td></tr><tr><td>combined_performance</td><td>▁█▇</td></tr><tr><td>data_summary/feature_dim</td><td>▁</td></tr><tr><td>data_summary/sequence_length</td><td>▁</td></tr><tr><td>decline_detected_epoch</td><td>▁</td></tr><tr><td>decline_stop</td><td>▁</td></tr><tr><td>epoch</td><td>▁▅█</td></tr><tr><td>gradients/avg_norm</td><td>▁▄█</td></tr><tr><td>+50</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>1</td></tr><tr><td>best_model_saved</td><td>1</td></tr><tr><td>best_val_f1</td><td>0.5346</td></tr><tr><td>combined_performance</td><td>0.63818</td></tr><tr><td>data_summary/feature_dim</td><td>50</td></tr><tr><td>data_summary/sequence_length</td><td>32</td></tr><tr><td>decline_detected_epoch</td><td>4</td></tr><tr><td>decline_stop</td><td>1</td></tr><tr><td>epoch</td><td>3</td></tr><tr><td>gradients/avg_norm</td><td>0.07758</td></tr><tr><td>+53</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fixed_trial_6_h512_l8_bi0_att0</strong> at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/0hp9a5uq' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/0hp9a5uq</a><br> View project at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250901_145821-0hp9a5uq\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6: F1=0.5346, Decrease_penalty=0.0000, Gap_penalty=3.2257, Score=-2.6911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 14:58:47,320] Trial 6 finished with value: -2.691122169563647 and parameters: {'hidden_dim': 512, 'num_layers': 8, 'dropout': 0.5172641992750493, 'use_attention': False, 'bidirectional': False, 'sequence_length': 32, 'lr': 0.002965272677005076, 'batch_size': 128, 'l2_reg': 1.7015402629331373e-06, 'gradient_clip': 1.105049132250456, 'l1_reg': 0.0001480902426015118, 'early_stopping_patience': 55, 'min_delta': 0.00015109484994337507}. Best is trial 0 with value: -1.2544879968650635.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FIXED Trial #7\n",
      "=== FIXED Data Preprocessing ===\n",
      "After cleaning - Train: 13675, Val: 7264, Test: 7264\n",
      "Class balance - Train: 0.560, Val: 0.365\n",
      "Calculated pos_weight: 0.89\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "creating run (0.2s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\gokde\\OneDrive\\Masaüstü\\Anomaly-Detection-and-Network-Optimization-in-SDNs\\real_project\\wandb\\run-20250901_145847-mcxt1568</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/mcxt1568' target=\"_blank\">fixed_trial_7_h48_l2_bi1_att0</a></strong> to <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/mcxt1568' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/mcxt1568</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with FIXED approach - monitoring for declining performance\n",
      "All hyperparameters logged to W&B project: lstm-cybersecurity-fixed\n",
      "Epoch  1 | Loss: T=0.2028 V=1.1730 Gap=0.9703\n",
      "         | F1: T=0.9257 V=0.7825 | AUC: 0.9571\n",
      "         | Combined Performance: 0.8884\n",
      "    New best F1: 0.7825\n",
      "Epoch  2 | Loss: T=0.0739 V=1.3175 Gap=1.2436\n",
      "         | F1: T=0.9790 V=0.8429 | AUC: 0.9538\n",
      "         | Combined Performance: 0.9252\n",
      "    New best F1: 0.8429\n",
      "Epoch  3 | Loss: T=0.0674 V=1.3997 Gap=1.3323\n",
      "         | F1: T=0.9812 V=0.8430 | AUC: 0.9454\n",
      "         | Combined Performance: 0.9232\n",
      "    New best F1: 0.8430\n",
      "PERFORMANCE DECLINE DETECTED - stopping training\n",
      "Recent performance: [0.925232040568925, 0.9232103086966913, 0.9225265587489289]\n",
      "Restored best model with F1: 0.8430\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁▅█</td></tr><tr><td>best_model_saved</td><td>▁▁▁</td></tr><tr><td>best_val_f1</td><td>▁██</td></tr><tr><td>combined_performance</td><td>▁██</td></tr><tr><td>data_summary/feature_dim</td><td>▁</td></tr><tr><td>data_summary/sequence_length</td><td>▁</td></tr><tr><td>decline_detected_epoch</td><td>▁</td></tr><tr><td>decline_stop</td><td>▁</td></tr><tr><td>epoch</td><td>▁▅█</td></tr><tr><td>gradients/avg_norm</td><td>█▂▁</td></tr><tr><td>+50</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>3</td></tr><tr><td>best_model_saved</td><td>1</td></tr><tr><td>best_val_f1</td><td>0.84304</td></tr><tr><td>combined_performance</td><td>0.92321</td></tr><tr><td>data_summary/feature_dim</td><td>50</td></tr><tr><td>data_summary/sequence_length</td><td>16</td></tr><tr><td>decline_detected_epoch</td><td>4</td></tr><tr><td>decline_stop</td><td>1</td></tr><tr><td>epoch</td><td>3</td></tr><tr><td>gradients/avg_norm</td><td>2e-05</td></tr><tr><td>+53</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fixed_trial_7_h48_l2_bi1_att0</strong> at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/mcxt1568' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/mcxt1568</a><br> View project at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250901_145847-mcxt1568\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7: F1=0.8430, Decrease_penalty=4.3142, Gap_penalty=2.6488, Score=-6.1200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 14:58:57,945] Trial 7 finished with value: -6.119980260657092 and parameters: {'hidden_dim': 48, 'num_layers': 2, 'dropout': 0.1274013249081754, 'use_attention': False, 'bidirectional': True, 'sequence_length': 16, 'lr': 0.0008428658960383928, 'batch_size': 64, 'l2_reg': 0.0003938634067979974, 'gradient_clip': 0.5658273992550926, 'l1_reg': 1.3282162001999418e-08, 'early_stopping_patience': 35, 'min_delta': 1.2706117715434265e-05}. Best is trial 0 with value: -1.2544879968650635.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FIXED Trial #8\n",
      "=== FIXED Data Preprocessing ===\n",
      "After cleaning - Train: 13675, Val: 7264, Test: 7264\n",
      "Class balance - Train: 0.560, Val: 0.365\n",
      "Calculated pos_weight: 0.89\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "creating run (0.1s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\gokde\\OneDrive\\Masaüstü\\Anomaly-Detection-and-Network-Optimization-in-SDNs\\real_project\\wandb\\run-20250901_145858-88vskvbn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/88vskvbn' target=\"_blank\">fixed_trial_8_h96_l2_bi0_att1</a></strong> to <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/88vskvbn' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/88vskvbn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with FIXED approach - monitoring for declining performance\n",
      "All hyperparameters logged to W&B project: lstm-cybersecurity-fixed\n",
      "Epoch  1 | Loss: T=0.4679 V=0.6696 Gap=0.2017\n",
      "         | F1: T=0.9532 V=0.8314 | AUC: 0.9646\n",
      "         | Combined Performance: 0.9164\n",
      "    New best F1: 0.8314\n",
      "Epoch  2 | Loss: T=0.3118 V=1.0423 Gap=0.7305\n",
      "         | F1: T=0.9722 V=0.7970 | AUC: 0.9663\n",
      "         | Combined Performance: 0.9118\n",
      "Epoch  3 | Loss: T=0.2905 V=1.1222 Gap=0.8317\n",
      "         | F1: T=0.9666 V=0.8021 | AUC: 0.9571\n",
      "         | Combined Performance: 0.9086\n",
      "PERFORMANCE DECLINE DETECTED - stopping training\n",
      "Recent performance: [0.9118482395565312, 0.9085864471290995, 0.8907577453866589]\n",
      "Restored best model with F1: 0.8314\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁</td></tr><tr><td>best_model_saved</td><td>▁</td></tr><tr><td>best_val_f1</td><td>▁</td></tr><tr><td>combined_performance</td><td>█▄▁</td></tr><tr><td>data_summary/feature_dim</td><td>▁</td></tr><tr><td>data_summary/sequence_length</td><td>▁</td></tr><tr><td>decline_detected_epoch</td><td>▁</td></tr><tr><td>decline_stop</td><td>▁</td></tr><tr><td>epoch</td><td>▁▅█</td></tr><tr><td>gradients/avg_norm</td><td>█▁▁</td></tr><tr><td>+50</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>1</td></tr><tr><td>best_model_saved</td><td>1</td></tr><tr><td>best_val_f1</td><td>0.83138</td></tr><tr><td>combined_performance</td><td>0.90859</td></tr><tr><td>data_summary/feature_dim</td><td>50</td></tr><tr><td>data_summary/sequence_length</td><td>16</td></tr><tr><td>decline_detected_epoch</td><td>4</td></tr><tr><td>decline_stop</td><td>1</td></tr><tr><td>epoch</td><td>3</td></tr><tr><td>gradients/avg_norm</td><td>0.00572</td></tr><tr><td>+53</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fixed_trial_8_h96_l2_bi0_att1</strong> at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/88vskvbn' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/88vskvbn</a><br> View project at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250901_145858-88vskvbn\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8: F1=0.8314, Decrease_penalty=3.6683, Gap_penalty=2.6141, Score=-5.4509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 14:59:07,518] Trial 8 finished with value: -5.450948256240226 and parameters: {'hidden_dim': 96, 'num_layers': 2, 'dropout': 0.5267292163607336, 'use_attention': True, 'bidirectional': False, 'sequence_length': 16, 'lr': 0.00765040091301092, 'batch_size': 64, 'l2_reg': 0.0003639461130058051, 'gradient_clip': 5.983751166502884, 'l1_reg': 8.641386614214664e-05, 'early_stopping_patience': 20, 'min_delta': 0.00015841582731518974}. Best is trial 0 with value: -1.2544879968650635.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FIXED Trial #9\n",
      "=== FIXED Data Preprocessing ===\n",
      "After cleaning - Train: 13675, Val: 7264, Test: 7264\n",
      "Class balance - Train: 0.560, Val: 0.365\n",
      "Calculated pos_weight: 0.89\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "creating run (0.2s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\gokde\\OneDrive\\Masaüstü\\Anomaly-Detection-and-Network-Optimization-in-SDNs\\real_project\\wandb\\run-20250901_145909-tk28s0ml</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/tk28s0ml' target=\"_blank\">fixed_trial_9_h8_l9_bi0_att0</a></strong> to <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/tk28s0ml' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/tk28s0ml</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with FIXED approach - monitoring for declining performance\n",
      "All hyperparameters logged to W&B project: lstm-cybersecurity-fixed\n",
      "Epoch  1 | Loss: T=0.6506 V=0.6641 Gap=0.0134\n",
      "         | F1: T=0.5320 V=0.5346 | AUC: 0.1773\n",
      "         | Combined Performance: 0.4146\n",
      "    New best F1: 0.5346\n",
      "Epoch  2 | Loss: T=0.6496 V=0.6646 Gap=0.0150\n",
      "         | F1: T=0.6588 V=0.5346 | AUC: 0.3740\n",
      "         | Combined Performance: 0.5225\n",
      "Epoch  3 | Loss: T=0.6494 V=0.6647 Gap=0.0153\n",
      "         | F1: T=0.6858 V=0.5346 | AUC: 0.3778\n",
      "         | Combined Performance: 0.5327\n",
      "Epoch  4 | Loss: T=0.6493 V=0.6649 Gap=0.0156\n",
      "         | F1: T=0.7179 V=0.5346 | AUC: 0.5151\n",
      "         | Combined Performance: 0.5892\n",
      "Epoch  5 | Loss: T=0.6492 V=0.6650 Gap=0.0158\n",
      "         | F1: T=0.7179 V=0.5346 | AUC: 0.5000\n",
      "         | Combined Performance: 0.5842\n",
      "PERFORMANCE DECLINE DETECTED - stopping training\n",
      "Recent performance: [0.5891945347819451, 0.5841620522908665, 0.5841620522908665]\n",
      "Restored best model with F1: 0.5346\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁</td></tr><tr><td>best_model_saved</td><td>▁</td></tr><tr><td>best_val_f1</td><td>▁</td></tr><tr><td>combined_performance</td><td>▁▅▆██</td></tr><tr><td>data_summary/feature_dim</td><td>▁</td></tr><tr><td>data_summary/sequence_length</td><td>▁</td></tr><tr><td>decline_detected_epoch</td><td>▁</td></tr><tr><td>decline_stop</td><td>▁</td></tr><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>gradients/avg_norm</td><td>█▅▂▁▁</td></tr><tr><td>+50</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>1</td></tr><tr><td>best_model_saved</td><td>1</td></tr><tr><td>best_val_f1</td><td>0.5346</td></tr><tr><td>combined_performance</td><td>0.58416</td></tr><tr><td>data_summary/feature_dim</td><td>50</td></tr><tr><td>data_summary/sequence_length</td><td>96</td></tr><tr><td>decline_detected_epoch</td><td>6</td></tr><tr><td>decline_stop</td><td>1</td></tr><tr><td>epoch</td><td>5</td></tr><tr><td>gradients/avg_norm</td><td>0.01305</td></tr><tr><td>+53</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fixed_trial_9_h8_l9_bi0_att0</strong> at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/tk28s0ml' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/tk28s0ml</a><br> View project at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250901_145909-tk28s0ml\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9: F1=0.5346, Decrease_penalty=4.9928, Gap_penalty=0.0320, Score=-4.4902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 14:59:24,087] Trial 9 finished with value: -4.490217144416753 and parameters: {'hidden_dim': 8, 'num_layers': 9, 'dropout': 0.47865595127671495, 'use_attention': False, 'bidirectional': False, 'sequence_length': 96, 'lr': 0.00011729726918091112, 'batch_size': 128, 'l2_reg': 0.0006004768187452731, 'gradient_clip': 4.284662042701847, 'l1_reg': 4.684012712629877e-08, 'early_stopping_patience': 50, 'min_delta': 0.00039747908046585843}. Best is trial 0 with value: -1.2544879968650635.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FIXED Trial #10\n",
      "=== FIXED Data Preprocessing ===\n",
      "After cleaning - Train: 13675, Val: 7264, Test: 7264\n",
      "Class balance - Train: 0.560, Val: 0.365\n",
      "Calculated pos_weight: 0.89\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "creating run (0.1s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\gokde\\OneDrive\\Masaüstü\\Anomaly-Detection-and-Network-Optimization-in-SDNs\\real_project\\wandb\\run-20250901_145926-gj7z5vjk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/gj7z5vjk' target=\"_blank\">fixed_trial_10_h4_l1_bi1_att0</a></strong> to <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/gj7z5vjk' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/gj7z5vjk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with FIXED approach - monitoring for declining performance\n",
      "All hyperparameters logged to W&B project: lstm-cybersecurity-fixed\n",
      "Epoch  1 | Loss: T=0.3336 V=0.5326 Gap=0.1990\n",
      "         | F1: T=0.8834 V=0.8101 | AUC: 0.8821\n",
      "         | Combined Performance: 0.8585\n",
      "    New best F1: 0.8101\n",
      "Epoch  2 | Loss: T=0.1113 V=0.7012 Gap=0.5899\n",
      "         | F1: T=0.9682 V=0.8352 | AUC: 0.8583\n",
      "         | Combined Performance: 0.8872\n",
      "    New best F1: 0.8352\n",
      "Epoch  3 | Loss: T=0.0891 V=0.7965 Gap=0.7074\n",
      "         | F1: T=0.9768 V=0.8405 | AUC: 0.8556\n",
      "         | Combined Performance: 0.8910\n",
      "    New best F1: 0.8405\n",
      "Epoch  4 | Loss: T=0.0807 V=0.8659 Gap=0.7852\n",
      "         | F1: T=0.9792 V=0.8409 | AUC: 0.8560\n",
      "         | Combined Performance: 0.8920\n",
      "    New best F1: 0.8409\n",
      "Epoch  5 | Loss: T=0.0762 V=0.9171 Gap=0.8409\n",
      "         | F1: T=0.9811 V=0.8392 | AUC: 0.8576\n",
      "         | Combined Performance: 0.8927\n",
      "Epoch  6 | Loss: T=0.0723 V=1.0145 Gap=0.9421\n",
      "         | F1: T=0.9823 V=0.8383 | AUC: 0.8583\n",
      "         | Combined Performance: 0.8930\n",
      "Epoch  7 | Loss: T=0.0712 V=1.0109 Gap=0.9397\n",
      "         | F1: T=0.9823 V=0.8383 | AUC: 0.8655\n",
      "         | Combined Performance: 0.8954\n",
      "Epoch  8 | Loss: T=0.0667 V=1.1064 Gap=1.0397\n",
      "         | F1: T=0.9838 V=0.8407 | AUC: 0.8726\n",
      "         | Combined Performance: 0.8990\n",
      "Epoch  9 | Loss: T=0.0650 V=1.2067 Gap=1.1417\n",
      "         | F1: T=0.9843 V=0.8366 | AUC: 0.8702\n",
      "         | Combined Performance: 0.8970\n",
      "Epoch 10 | Loss: T=0.0617 V=1.2580 Gap=1.1963\n",
      "         | F1: T=0.9851 V=0.8393 | AUC: 0.8688\n",
      "         | Combined Performance: 0.8977\n",
      "Epoch 11 | Loss: T=0.0606 V=1.3472 Gap=1.2867\n",
      "         | F1: T=0.9850 V=0.8348 | AUC: 0.8681\n",
      "         | Combined Performance: 0.8960\n",
      "Epoch 12 | Loss: T=0.0601 V=1.3276 Gap=1.2675\n",
      "         | F1: T=0.9859 V=0.8389 | AUC: 0.8710\n",
      "         | Combined Performance: 0.8986\n",
      "Epoch 13 | Loss: T=0.0594 V=1.3347 Gap=1.2753\n",
      "         | F1: T=0.9864 V=0.8389 | AUC: 0.8702\n",
      "         | Combined Performance: 0.8985\n",
      "PERFORMANCE DECLINE DETECTED - stopping training\n",
      "Recent performance: [0.8985811038552914, 0.8985030614264296, 0.8979494263105751]\n",
      "Restored best model with F1: 0.8409\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁▃▆█</td></tr><tr><td>best_model_saved</td><td>▁▁▁▁</td></tr><tr><td>best_val_f1</td><td>▁▇██</td></tr><tr><td>combined_performance</td><td>▁▆▇▇▇▇▇███▇██</td></tr><tr><td>data_summary/feature_dim</td><td>▁</td></tr><tr><td>data_summary/sequence_length</td><td>▁</td></tr><tr><td>decline_detected_epoch</td><td>▁</td></tr><tr><td>decline_stop</td><td>▁</td></tr><tr><td>epoch</td><td>▁▂▂▃▃▄▅▅▆▆▇▇█</td></tr><tr><td>gradients/avg_norm</td><td>█▄▅▁▄▁▁▄▁▃▁▁▁</td></tr><tr><td>+50</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>4</td></tr><tr><td>best_model_saved</td><td>1</td></tr><tr><td>best_val_f1</td><td>0.84091</td></tr><tr><td>combined_performance</td><td>0.8985</td></tr><tr><td>data_summary/feature_dim</td><td>50</td></tr><tr><td>data_summary/sequence_length</td><td>128</td></tr><tr><td>decline_detected_epoch</td><td>14</td></tr><tr><td>decline_stop</td><td>1</td></tr><tr><td>epoch</td><td>13</td></tr><tr><td>gradients/avg_norm</td><td>0.00149</td></tr><tr><td>+53</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fixed_trial_10_h4_l1_bi1_att0</strong> at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/gj7z5vjk' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/gj7z5vjk</a><br> View project at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250901_145926-gj7z5vjk\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10: F1=0.8409, Decrease_penalty=3.6185, Gap_penalty=2.4864, Score=-5.2640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 15:00:59,428] Trial 10 finished with value: -5.264049695931092 and parameters: {'hidden_dim': 4, 'num_layers': 1, 'dropout': 0.060381431114513995, 'use_attention': False, 'bidirectional': True, 'sequence_length': 128, 'lr': 0.00046823168701604317, 'batch_size': 16, 'l2_reg': 1.4043535902876375e-05, 'gradient_clip': 2.9442400071494195, 'l1_reg': 2.6752506397619974e-06, 'early_stopping_patience': 40, 'min_delta': 1.2275661554031075e-06}. Best is trial 0 with value: -1.2544879968650635.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FIXED Trial #11\n",
      "=== FIXED Data Preprocessing ===\n",
      "After cleaning - Train: 13675, Val: 7264, Test: 7264\n",
      "Class balance - Train: 0.560, Val: 0.365\n",
      "Calculated pos_weight: 0.89\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "creating run (0.2s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\gokde\\OneDrive\\Masaüstü\\Anomaly-Detection-and-Network-Optimization-in-SDNs\\real_project\\wandb\\run-20250901_150100-dt5w3m5d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/dt5w3m5d' target=\"_blank\">fixed_trial_11_h4_l5_bi1_att1</a></strong> to <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/dt5w3m5d' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/dt5w3m5d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with FIXED approach - monitoring for declining performance\n",
      "All hyperparameters logged to W&B project: lstm-cybersecurity-fixed\n",
      "Epoch  1 | Loss: T=0.6486 V=0.6585 Gap=0.0100\n",
      "         | F1: T=0.6347 V=0.5346 | AUC: 0.8304\n",
      "         | Combined Performance: 0.6666\n",
      "    New best F1: 0.5346\n",
      "Epoch  2 | Loss: T=0.6471 V=0.6580 Gap=0.0108\n",
      "         | F1: T=0.7263 V=0.5462 | AUC: 0.8537\n",
      "         | Combined Performance: 0.7088\n",
      "    New best F1: 0.5462\n",
      "Epoch  3 | Loss: T=0.6459 V=0.6566 Gap=0.0107\n",
      "         | F1: T=0.7629 V=0.6852 | AUC: 0.8662\n",
      "         | Combined Performance: 0.7715\n",
      "    New best F1: 0.6852\n",
      "Epoch  4 | Loss: T=0.6440 V=0.6542 Gap=0.0102\n",
      "         | F1: T=0.8417 V=0.8088 | AUC: 0.8851\n",
      "         | Combined Performance: 0.8452\n",
      "    New best F1: 0.8088\n",
      "Epoch  5 | Loss: T=0.6414 V=0.6513 Gap=0.0099\n",
      "         | F1: T=0.8765 V=0.8278 | AUC: 0.8932\n",
      "         | Combined Performance: 0.8658\n",
      "    New best F1: 0.8278\n",
      "Epoch  6 | Loss: T=0.6378 V=0.6472 Gap=0.0094\n",
      "         | F1: T=0.8805 V=0.8350 | AUC: 0.8962\n",
      "         | Combined Performance: 0.8706\n",
      "    New best F1: 0.8350\n",
      "Epoch  7 | Loss: T=0.6325 V=0.6413 Gap=0.0088\n",
      "         | F1: T=0.8871 V=0.8393 | AUC: 0.8994\n",
      "         | Combined Performance: 0.8753\n",
      "    New best F1: 0.8393\n",
      "Epoch  8 | Loss: T=0.6243 V=0.6327 Gap=0.0084\n",
      "         | F1: T=0.9052 V=0.8413 | AUC: 0.9067\n",
      "         | Combined Performance: 0.8844\n",
      "    New best F1: 0.8413\n",
      "Epoch  9 | Loss: T=0.6124 V=0.6205 Gap=0.0081\n",
      "         | F1: T=0.9313 V=0.8431 | AUC: 0.9151\n",
      "         | Combined Performance: 0.8965\n",
      "    New best F1: 0.8431\n",
      "Epoch 10 | Loss: T=0.5954 V=0.6037 Gap=0.0083\n",
      "         | F1: T=0.9468 V=0.8443 | AUC: 0.9200\n",
      "         | Combined Performance: 0.9037\n",
      "    New best F1: 0.8443\n",
      "Epoch 11 | Loss: T=0.5711 V=0.5827 Gap=0.0117\n",
      "         | F1: T=0.9599 V=0.8449 | AUC: 0.9227\n",
      "         | Combined Performance: 0.9092\n",
      "    New best F1: 0.8449\n",
      "Epoch 12 | Loss: T=0.5391 V=0.5584 Gap=0.0194\n",
      "         | F1: T=0.9683 V=0.8455 | AUC: 0.9243\n",
      "         | Combined Performance: 0.9127\n",
      "    New best F1: 0.8455\n",
      "Epoch 13 | Loss: T=0.5015 V=0.5318 Gap=0.0304\n",
      "         | F1: T=0.9739 V=0.8464 | AUC: 0.9247\n",
      "         | Combined Performance: 0.9150\n",
      "    New best F1: 0.8464\n",
      "Epoch 14 | Loss: T=0.4576 V=0.5057 Gap=0.0481\n",
      "         | F1: T=0.9776 V=0.8470 | AUC: 0.9246\n",
      "         | Combined Performance: 0.9164\n",
      "    New best F1: 0.8470\n",
      "Epoch 15 | Loss: T=0.4127 V=0.4828 Gap=0.0701\n",
      "         | F1: T=0.9794 V=0.8474 | AUC: 0.9241\n",
      "         | Combined Performance: 0.9170\n",
      "    New best F1: 0.8474\n",
      "Epoch 16 | Loss: T=0.3679 V=0.4650 Gap=0.0971\n",
      "         | F1: T=0.9812 V=0.8474 | AUC: 0.9270\n",
      "         | Combined Performance: 0.9186\n",
      "Epoch 17 | Loss: T=0.3267 V=0.4534 Gap=0.1267\n",
      "         | F1: T=0.9816 V=0.8479 | AUC: 0.9304\n",
      "         | Combined Performance: 0.9200\n",
      "    New best F1: 0.8479\n",
      "Epoch 18 | Loss: T=0.2904 V=0.4480 Gap=0.1576\n",
      "         | F1: T=0.9830 V=0.8477 | AUC: 0.9323\n",
      "         | Combined Performance: 0.9210\n",
      "Epoch 19 | Loss: T=0.2581 V=0.4463 Gap=0.1881\n",
      "         | F1: T=0.9836 V=0.8482 | AUC: 0.9349\n",
      "         | Combined Performance: 0.9222\n",
      "    New best F1: 0.8482\n",
      "Epoch 20 | Loss: T=0.2322 V=0.4481 Gap=0.2159\n",
      "         | F1: T=0.9842 V=0.8480 | AUC: 0.9355\n",
      "         | Combined Performance: 0.9225\n",
      "Epoch 21 | Loss: T=0.2096 V=0.4514 Gap=0.2418\n",
      "         | F1: T=0.9844 V=0.8477 | AUC: 0.9353\n",
      "         | Combined Performance: 0.9225\n",
      "PERFORMANCE DECLINE DETECTED - stopping training\n",
      "Recent performance: [0.9225431722594376, 0.9224959062633022, 0.9218370029040841]\n",
      "Restored best model with F1: 0.8482\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇█</td></tr><tr><td>best_model_saved</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>best_val_f1</td><td>▁▁▄▇█████████████</td></tr><tr><td>combined_performance</td><td>▁▂▄▆▆▇▇▇▇▇███████████</td></tr><tr><td>data_summary/feature_dim</td><td>▁</td></tr><tr><td>data_summary/sequence_length</td><td>▁</td></tr><tr><td>decline_detected_epoch</td><td>▁</td></tr><tr><td>decline_stop</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>gradients/avg_norm</td><td>▁▂▂▃▃▃▃▅▆▇████▇▅▄▃▃▂▁</td></tr><tr><td>+50</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>19</td></tr><tr><td>best_model_saved</td><td>1</td></tr><tr><td>best_val_f1</td><td>0.84823</td></tr><tr><td>combined_performance</td><td>0.9225</td></tr><tr><td>data_summary/feature_dim</td><td>50</td></tr><tr><td>data_summary/sequence_length</td><td>48</td></tr><tr><td>decline_detected_epoch</td><td>22</td></tr><tr><td>decline_stop</td><td>1</td></tr><tr><td>epoch</td><td>21</td></tr><tr><td>gradients/avg_norm</td><td>0.02094</td></tr><tr><td>+53</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fixed_trial_11_h4_l5_bi1_att1</strong> at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/dt5w3m5d' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/dt5w3m5d</a><br> View project at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250901_150100-dt5w3m5d\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 11: F1=0.8482, Decrease_penalty=1.6929, Gap_penalty=0.5356, Score=-1.3803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 15:01:13,234] Trial 11 finished with value: -1.380266143793531 and parameters: {'hidden_dim': 4, 'num_layers': 5, 'dropout': 0.27132338010152324, 'use_attention': True, 'bidirectional': True, 'sequence_length': 48, 'lr': 0.000331680665289538, 'batch_size': 1024, 'l2_reg': 3.433400105649509e-05, 'gradient_clip': 7.320665243284988, 'l1_reg': 1.4758582133388554e-06, 'early_stopping_patience': 30, 'min_delta': 2.07439856912824e-05}. Best is trial 0 with value: -1.2544879968650635.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FIXED Trial #12\n",
      "=== FIXED Data Preprocessing ===\n",
      "After cleaning - Train: 13675, Val: 7264, Test: 7264\n",
      "Class balance - Train: 0.560, Val: 0.365\n",
      "Calculated pos_weight: 0.89\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "creating run (0.2s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\gokde\\OneDrive\\Masaüstü\\Anomaly-Detection-and-Network-Optimization-in-SDNs\\real_project\\wandb\\run-20250901_150117-0mvrbssn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/0mvrbssn' target=\"_blank\">fixed_trial_12_h4_l5_bi1_att1</a></strong> to <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/0mvrbssn' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/0mvrbssn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with FIXED approach - monitoring for declining performance\n",
      "All hyperparameters logged to W&B project: lstm-cybersecurity-fixed\n",
      "Epoch  1 | Loss: T=0.6494 V=0.6592 Gap=0.0097\n",
      "         | F1: T=0.7135 V=0.5346 | AUC: 0.5807\n",
      "         | Combined Performance: 0.6096\n",
      "    New best F1: 0.5346\n",
      "Epoch  2 | Loss: T=0.6483 V=0.6591 Gap=0.0108\n",
      "         | F1: T=0.7179 V=0.5346 | AUC: 0.6051\n",
      "         | Combined Performance: 0.6192\n",
      "Epoch  3 | Loss: T=0.6478 V=0.6590 Gap=0.0112\n",
      "         | F1: T=0.7179 V=0.5346 | AUC: 0.6481\n",
      "         | Combined Performance: 0.6335\n",
      "Epoch  4 | Loss: T=0.6471 V=0.6588 Gap=0.0116\n",
      "         | F1: T=0.7179 V=0.5346 | AUC: 0.6920\n",
      "         | Combined Performance: 0.6482\n",
      "Epoch  5 | Loss: T=0.6464 V=0.6584 Gap=0.0120\n",
      "         | F1: T=0.7179 V=0.5346 | AUC: 0.7429\n",
      "         | Combined Performance: 0.6651\n",
      "Epoch  6 | Loss: T=0.6453 V=0.6577 Gap=0.0124\n",
      "         | F1: T=0.7179 V=0.5346 | AUC: 0.7879\n",
      "         | Combined Performance: 0.6801\n",
      "Epoch  7 | Loss: T=0.6439 V=0.6566 Gap=0.0127\n",
      "         | F1: T=0.7179 V=0.5346 | AUC: 0.8179\n",
      "         | Combined Performance: 0.6901\n",
      "Epoch  8 | Loss: T=0.6420 V=0.6550 Gap=0.0130\n",
      "         | F1: T=0.7179 V=0.5346 | AUC: 0.8375\n",
      "         | Combined Performance: 0.6967\n",
      "Epoch  9 | Loss: T=0.6393 V=0.6527 Gap=0.0134\n",
      "         | F1: T=0.7179 V=0.5346 | AUC: 0.8576\n",
      "         | Combined Performance: 0.7034\n",
      "Epoch 10 | Loss: T=0.6356 V=0.6495 Gap=0.0139\n",
      "         | F1: T=0.7179 V=0.5346 | AUC: 0.8713\n",
      "         | Combined Performance: 0.7079\n",
      "Epoch 11 | Loss: T=0.6300 V=0.6449 Gap=0.0149\n",
      "         | F1: T=0.7195 V=0.5346 | AUC: 0.8844\n",
      "         | Combined Performance: 0.7128\n",
      "Error in training: \n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁</td></tr><tr><td>best_model_saved</td><td>▁</td></tr><tr><td>best_val_f1</td><td>▁</td></tr><tr><td>combined_performance</td><td>▁▂▃▄▅▆▆▇▇██</td></tr><tr><td>data_summary/feature_dim</td><td>▁</td></tr><tr><td>data_summary/sequence_length</td><td>▁</td></tr><tr><td>epoch</td><td>▁▂▂▃▄▅▅▆▇▇█</td></tr><tr><td>gradients/avg_norm</td><td>▁▁▂▂▃▃▄▅▆▇█</td></tr><tr><td>gradients/clipping_threshold</td><td>▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>gradients/max_norm</td><td>██▇▇▆▆▅▅▄▃▁</td></tr><tr><td>+44</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>1</td></tr><tr><td>best_model_saved</td><td>1</td></tr><tr><td>best_val_f1</td><td>0.5346</td></tr><tr><td>combined_performance</td><td>0.71281</td></tr><tr><td>data_summary/feature_dim</td><td>50</td></tr><tr><td>data_summary/sequence_length</td><td>192</td></tr><tr><td>epoch</td><td>11</td></tr><tr><td>gradients/avg_norm</td><td>0.06168</td></tr><tr><td>gradients/clipping_threshold</td><td>8.10552</td></tr><tr><td>gradients/max_norm</td><td>0.41114</td></tr><tr><td>+48</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fixed_trial_12_h4_l5_bi1_att1</strong> at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/0mvrbssn' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/0mvrbssn</a><br> View project at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250901_150117-0mvrbssn\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 15:01:28,239] Trial 12 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FIXED Trial #13\n",
      "=== FIXED Data Preprocessing ===\n",
      "After cleaning - Train: 13675, Val: 7264, Test: 7264\n",
      "Class balance - Train: 0.560, Val: 0.365\n",
      "Calculated pos_weight: 0.89\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for wandb.init()..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\gokde\\OneDrive\\Masaüstü\\Anomaly-Detection-and-Network-Optimization-in-SDNs\\real_project\\wandb\\run-20250901_150129-ql4iimvx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/ql4iimvx' target=\"_blank\">fixed_trial_13_h4_l6_bi1_att1</a></strong> to <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/ql4iimvx' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/ql4iimvx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with FIXED approach - monitoring for declining performance\n",
      "All hyperparameters logged to W&B project: lstm-cybersecurity-fixed\n",
      "Epoch  1 | Loss: T=1.3732 V=0.6607 Gap=-0.7126\n",
      "         | F1: T=0.6999 V=0.5346 | AUC: 0.6774\n",
      "         | Combined Performance: 0.6373\n",
      "    New best F1: 0.5346\n",
      "Epoch  2 | Loss: T=1.3376 V=0.6611 Gap=-0.6764\n",
      "         | F1: T=0.7179 V=0.5346 | AUC: 0.7033\n",
      "         | Combined Performance: 0.6519\n",
      "Epoch  3 | Loss: T=1.3043 V=0.6616 Gap=-0.6427\n",
      "         | F1: T=0.7179 V=0.5346 | AUC: 0.6837\n",
      "         | Combined Performance: 0.6454\n",
      "PERFORMANCE DECLINE DETECTED - stopping training\n",
      "Recent performance: [0.6519208830847588, 0.6453899667868086, 0.6354444904814432]\n",
      "Restored best model with F1: 0.5346\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁</td></tr><tr><td>best_model_saved</td><td>▁</td></tr><tr><td>best_val_f1</td><td>▁</td></tr><tr><td>combined_performance</td><td>▁█▅</td></tr><tr><td>data_summary/feature_dim</td><td>▁</td></tr><tr><td>data_summary/sequence_length</td><td>▁</td></tr><tr><td>decline_detected_epoch</td><td>▁</td></tr><tr><td>decline_stop</td><td>▁</td></tr><tr><td>epoch</td><td>▁▅█</td></tr><tr><td>gradients/avg_norm</td><td>█▃▁</td></tr><tr><td>+50</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>1</td></tr><tr><td>best_model_saved</td><td>1</td></tr><tr><td>best_val_f1</td><td>0.5346</td></tr><tr><td>combined_performance</td><td>0.64539</td></tr><tr><td>data_summary/feature_dim</td><td>50</td></tr><tr><td>data_summary/sequence_length</td><td>48</td></tr><tr><td>decline_detected_epoch</td><td>4</td></tr><tr><td>decline_stop</td><td>1</td></tr><tr><td>epoch</td><td>3</td></tr><tr><td>gradients/avg_norm</td><td>0.03386</td></tr><tr><td>+53</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fixed_trial_13_h4_l6_bi1_att1</strong> at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/ql4iimvx' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/ql4iimvx</a><br> View project at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250901_150129-ql4iimvx\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 13: F1=0.5346, Decrease_penalty=4.4944, Gap_penalty=1.2201, Score=-5.1799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 15:01:33,305] Trial 13 finished with value: -5.179899399411664 and parameters: {'hidden_dim': 4, 'num_layers': 6, 'dropout': 0.35574624798113696, 'use_attention': True, 'bidirectional': True, 'sequence_length': 48, 'lr': 0.0006699222440926764, 'batch_size': 1024, 'l2_reg': 1.0997248989411298e-05, 'gradient_clip': 9.937800123068653, 'l1_reg': 0.000940731220094604, 'early_stopping_patience': 30, 'min_delta': 6.372677111711983e-06}. Best is trial 0 with value: -1.2544879968650635.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FIXED Trial #14\n",
      "=== FIXED Data Preprocessing ===\n",
      "After cleaning - Train: 13675, Val: 7264, Test: 7264\n",
      "Class balance - Train: 0.560, Val: 0.365\n",
      "Calculated pos_weight: 0.89\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "creating run (0.2s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\gokde\\OneDrive\\Masaüstü\\Anomaly-Detection-and-Network-Optimization-in-SDNs\\real_project\\wandb\\run-20250901_150133-ybwbzh9x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/ybwbzh9x' target=\"_blank\">fixed_trial_14_h64_l10_bi1_att1</a></strong> to <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/ybwbzh9x' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/ybwbzh9x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with FIXED approach - monitoring for declining performance\n",
      "All hyperparameters logged to W&B project: lstm-cybersecurity-fixed\n",
      "Epoch  1 | Loss: T=1.4349 V=0.6604 Gap=-0.7745\n",
      "         | F1: T=0.6936 V=0.5346 | AUC: 0.4574\n",
      "         | Combined Performance: 0.5619\n",
      "    New best F1: 0.5346\n",
      "Epoch  2 | Loss: T=1.3927 V=0.6610 Gap=-0.7317\n",
      "         | F1: T=0.7179 V=0.5346 | AUC: 0.7449\n",
      "         | Combined Performance: 0.6658\n",
      "Epoch  3 | Loss: T=1.3528 V=0.6615 Gap=-0.6913\n",
      "         | F1: T=0.7179 V=0.5346 | AUC: 0.7826\n",
      "         | Combined Performance: 0.6783\n",
      "Epoch  4 | Loss: T=1.3145 V=0.6620 Gap=-0.6524\n",
      "         | F1: T=0.7179 V=0.5346 | AUC: 0.7958\n",
      "         | Combined Performance: 0.6828\n",
      "Epoch  5 | Loss: T=1.2777 V=0.6625 Gap=-0.6152\n",
      "         | F1: T=0.7179 V=0.5346 | AUC: 0.7869\n",
      "         | Combined Performance: 0.6798\n",
      "PERFORMANCE DECLINE DETECTED - stopping training\n",
      "Recent performance: [0.6827603844110476, 0.6797905469189195, 0.6725530580621997]\n",
      "Restored best model with F1: 0.5346\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁</td></tr><tr><td>best_model_saved</td><td>▁</td></tr><tr><td>best_val_f1</td><td>▁</td></tr><tr><td>combined_performance</td><td>▁▇███</td></tr><tr><td>data_summary/feature_dim</td><td>▁</td></tr><tr><td>data_summary/sequence_length</td><td>▁</td></tr><tr><td>decline_detected_epoch</td><td>▁</td></tr><tr><td>decline_stop</td><td>▁</td></tr><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>gradients/avg_norm</td><td>█▃▂▁▂</td></tr><tr><td>+50</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>1</td></tr><tr><td>best_model_saved</td><td>1</td></tr><tr><td>best_val_f1</td><td>0.5346</td></tr><tr><td>combined_performance</td><td>0.67979</td></tr><tr><td>data_summary/feature_dim</td><td>50</td></tr><tr><td>data_summary/sequence_length</td><td>8</td></tr><tr><td>decline_detected_epoch</td><td>6</td></tr><tr><td>decline_stop</td><td>1</td></tr><tr><td>epoch</td><td>5</td></tr><tr><td>gradients/avg_norm</td><td>0.01967</td></tr><tr><td>+53</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fixed_trial_14_h64_l10_bi1_att1</strong> at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/ybwbzh9x' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/ybwbzh9x</a><br> View project at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250901_150133-ybwbzh9x\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 14: F1=0.5346, Decrease_penalty=4.0369, Gap_penalty=1.1586, Score=-4.6609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 15:01:39,801] Trial 14 finished with value: -4.660884406527437 and parameters: {'hidden_dim': 64, 'num_layers': 10, 'dropout': 0.2358000423343667, 'use_attention': True, 'bidirectional': True, 'sequence_length': 8, 'lr': 0.00022127074051089392, 'batch_size': 1024, 'l2_reg': 8.378248131201999e-05, 'gradient_clip': 7.320651666956616, 'l1_reg': 1.4254560789459858e-05, 'early_stopping_patience': 40, 'min_delta': 1.1248324994349208e-06}. Best is trial 0 with value: -1.2544879968650635.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FIXED Trial #15\n",
      "=== FIXED Data Preprocessing ===\n",
      "After cleaning - Train: 13675, Val: 7264, Test: 7264\n",
      "Class balance - Train: 0.560, Val: 0.365\n",
      "Calculated pos_weight: 0.89\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "creating run (0.1s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\gokde\\OneDrive\\Masaüstü\\Anomaly-Detection-and-Network-Optimization-in-SDNs\\real_project\\wandb\\run-20250901_150139-t6mqeecu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/t6mqeecu' target=\"_blank\">fixed_trial_15_h192_l4_bi1_att0</a></strong> to <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/t6mqeecu' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/t6mqeecu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with FIXED approach - monitoring for declining performance\n",
      "All hyperparameters logged to W&B project: lstm-cybersecurity-fixed\n",
      "Epoch  1 | Loss: T=0.7140 V=0.6321 Gap=-0.0819\n",
      "         | F1: T=0.7779 V=0.8012 | AUC: 0.9222\n",
      "         | Combined Performance: 0.8338\n",
      "    New best F1: 0.8012\n",
      "Epoch  2 | Loss: T=0.6235 V=0.4487 Gap=-0.1748\n",
      "         | F1: T=0.8156 V=0.8251 | AUC: 0.9378\n",
      "         | Combined Performance: 0.8595\n",
      "    New best F1: 0.8251\n",
      "Epoch  3 | Loss: T=0.3648 V=0.3780 Gap=0.0132\n",
      "         | F1: T=0.9170 V=0.8225 | AUC: 0.9415\n",
      "         | Combined Performance: 0.8937\n",
      "Epoch  4 | Loss: T=0.2164 V=0.4352 Gap=0.2188\n",
      "         | F1: T=0.9518 V=0.8260 | AUC: 0.9539\n",
      "         | Combined Performance: 0.9106\n",
      "    New best F1: 0.8260\n",
      "Epoch  5 | Loss: T=0.1854 V=0.4808 Gap=0.2955\n",
      "         | F1: T=0.9677 V=0.8291 | AUC: 0.9570\n",
      "         | Combined Performance: 0.9179\n",
      "    New best F1: 0.8291\n",
      "Epoch  6 | Loss: T=0.1722 V=0.5004 Gap=0.3282\n",
      "         | F1: T=0.9698 V=0.8317 | AUC: 0.9586\n",
      "         | Combined Performance: 0.9200\n",
      "    New best F1: 0.8317\n",
      "Epoch  7 | Loss: T=0.1646 V=0.5264 Gap=0.3618\n",
      "         | F1: T=0.9723 V=0.8339 | AUC: 0.9595\n",
      "         | Combined Performance: 0.9219\n",
      "    New best F1: 0.8339\n",
      "Epoch  8 | Loss: T=0.1582 V=0.5475 Gap=0.3893\n",
      "         | F1: T=0.9733 V=0.8355 | AUC: 0.9596\n",
      "         | Combined Performance: 0.9228\n",
      "    New best F1: 0.8355\n",
      "Epoch  9 | Loss: T=0.1534 V=0.5615 Gap=0.4081\n",
      "         | F1: T=0.9744 V=0.8380 | AUC: 0.9589\n",
      "         | Combined Performance: 0.9238\n",
      "    New best F1: 0.8380\n",
      "Epoch 10 | Loss: T=0.1491 V=0.5948 Gap=0.4456\n",
      "         | F1: T=0.9761 V=0.8395 | AUC: 0.9575\n",
      "         | Combined Performance: 0.9243\n",
      "    New best F1: 0.8395\n",
      "Epoch 11 | Loss: T=0.1456 V=0.6280 Gap=0.4824\n",
      "         | F1: T=0.9767 V=0.8397 | AUC: 0.9555\n",
      "         | Combined Performance: 0.9240\n",
      "    New best F1: 0.8397\n",
      "PERFORMANCE DECLINE DETECTED - stopping training\n",
      "Recent performance: [0.924345844901918, 0.923987977045898, 0.9237294823425097]\n",
      "Restored best model with F1: 0.8397\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁▂▃▄▅▅▆▇▇█</td></tr><tr><td>best_model_saved</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>best_val_f1</td><td>▁▅▆▆▇▇▇███</td></tr><tr><td>combined_performance</td><td>▁▃▆▇███████</td></tr><tr><td>data_summary/feature_dim</td><td>▁</td></tr><tr><td>data_summary/sequence_length</td><td>▁</td></tr><tr><td>decline_detected_epoch</td><td>▁</td></tr><tr><td>decline_stop</td><td>▁</td></tr><tr><td>epoch</td><td>▁▂▂▃▄▅▅▆▇▇█</td></tr><tr><td>gradients/avg_norm</td><td>▅█▅▂▂▂▁▁▁▁▁</td></tr><tr><td>+50</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>11</td></tr><tr><td>best_model_saved</td><td>1</td></tr><tr><td>best_val_f1</td><td>0.83971</td></tr><tr><td>combined_performance</td><td>0.92399</td></tr><tr><td>data_summary/feature_dim</td><td>50</td></tr><tr><td>data_summary/sequence_length</td><td>4</td></tr><tr><td>decline_detected_epoch</td><td>12</td></tr><tr><td>decline_stop</td><td>1</td></tr><tr><td>epoch</td><td>11</td></tr><tr><td>gradients/avg_norm</td><td>0.02532</td></tr><tr><td>+53</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fixed_trial_15_h192_l4_bi1_att0</strong> at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/t6mqeecu' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/t6mqeecu</a><br> View project at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250901_150139-t6mqeecu\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 15: F1=0.8397, Decrease_penalty=2.1426, Gap_penalty=1.0023, Score=-2.3052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 15:01:58,364] Trial 15 finished with value: -2.3051841717018067 and parameters: {'hidden_dim': 192, 'num_layers': 4, 'dropout': 0.24725992181526096, 'use_attention': False, 'bidirectional': True, 'sequence_length': 4, 'lr': 0.000105273021212, 'batch_size': 512, 'l2_reg': 7.168622979400978e-05, 'gradient_clip': 3.117450595359421, 'l1_reg': 7.28851255928117e-07, 'early_stopping_patience': 10, 'min_delta': 1.5174060422994171e-05}. Best is trial 0 with value: -1.2544879968650635.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FIXED Trial #16\n",
      "=== FIXED Data Preprocessing ===\n",
      "After cleaning - Train: 13675, Val: 7264, Test: 7264\n",
      "Class balance - Train: 0.560, Val: 0.365\n",
      "Calculated pos_weight: 0.89\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "creating run (0.1s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\gokde\\OneDrive\\Masaüstü\\Anomaly-Detection-and-Network-Optimization-in-SDNs\\real_project\\wandb\\run-20250901_150158-gspjovrv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/gspjovrv' target=\"_blank\">fixed_trial_16_h256_l1_bi1_att0</a></strong> to <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/gspjovrv' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/gspjovrv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with FIXED approach - monitoring for declining performance\n",
      "All hyperparameters logged to W&B project: lstm-cybersecurity-fixed\n",
      "Epoch  1 | Loss: T=0.2830 V=1.1430 Gap=0.8600\n",
      "         | F1: T=0.9716 V=0.6682 | AUC: 0.9403\n",
      "         | Combined Performance: 0.8600\n",
      "    New best F1: 0.6682\n",
      "Epoch  2 | Loss: T=0.2272 V=1.6268 Gap=1.3996\n",
      "         | F1: T=0.9736 V=0.7457 | AUC: 0.9419\n",
      "         | Combined Performance: 0.8871\n",
      "    New best F1: 0.7457\n",
      "Epoch  3 | Loss: T=0.1766 V=1.2717 Gap=1.0951\n",
      "         | F1: T=0.9809 V=0.8050 | AUC: 0.9548\n",
      "         | Combined Performance: 0.9136\n",
      "    New best F1: 0.8050\n",
      "Epoch  4 | Loss: T=0.1311 V=4.3652 Gap=4.2341\n",
      "         | F1: T=0.9827 V=0.6651 | AUC: 0.9121\n",
      "         | Combined Performance: 0.8533\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁▅█</td></tr><tr><td>best_model_saved</td><td>▁▁▁</td></tr><tr><td>best_val_f1</td><td>▁▅█</td></tr><tr><td>combined_performance</td><td>▂▅█▁</td></tr><tr><td>data_summary/feature_dim</td><td>▁</td></tr><tr><td>data_summary/sequence_length</td><td>▁</td></tr><tr><td>epoch</td><td>▁▃▆█</td></tr><tr><td>gradients/avg_norm</td><td>█▁▁▁</td></tr><tr><td>gradients/clipping_threshold</td><td>▁▁▁▁</td></tr><tr><td>gradients/max_norm</td><td>█▁▁▁</td></tr><tr><td>+42</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>3</td></tr><tr><td>best_model_saved</td><td>1</td></tr><tr><td>best_val_f1</td><td>0.80498</td></tr><tr><td>combined_performance</td><td>0.85331</td></tr><tr><td>data_summary/feature_dim</td><td>50</td></tr><tr><td>data_summary/sequence_length</td><td>24</td></tr><tr><td>epoch</td><td>4</td></tr><tr><td>gradients/avg_norm</td><td>0.00142</td></tr><tr><td>gradients/clipping_threshold</td><td>6.68008</td></tr><tr><td>gradients/max_norm</td><td>0.00435</td></tr><tr><td>+45</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fixed_trial_16_h256_l1_bi1_att0</strong> at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/gspjovrv' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/gspjovrv</a><br> View project at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250901_150158-gspjovrv\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-09-01 15:02:35,142] Trial 16 failed with parameters: {'hidden_dim': 256, 'num_layers': 1, 'dropout': 0.40672929359239657, 'use_attention': False, 'bidirectional': True, 'sequence_length': 24, 'lr': 0.001720285786525446, 'batch_size': 16, 'l2_reg': 3.88316549877468e-06, 'gradient_clip': 6.680078748311293, 'l1_reg': 8.492029333065207e-06, 'early_stopping_patience': 45, 'min_delta': 3.0819327342426026e-06} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\gokde\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\gokde\\AppData\\Local\\Temp\\ipykernel_25360\\1439352675.py\", line 523, in fixed_objective\n",
      "    trained_model, history = train_lstm_fixed(\n",
      "                             ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\gokde\\AppData\\Local\\Temp\\ipykernel_25360\\1439352675.py\", line 298, in train_lstm_fixed\n",
      "    loss.backward()\n",
      "  File \"c:\\Users\\gokde\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\torch\\_tensor.py\", line 581, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"c:\\Users\\gokde\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\torch\\autograd\\__init__.py\", line 347, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"c:\\Users\\gokde\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\torch\\autograd\\graph.py\", line 825, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2025-09-01 15:02:35,146] Trial 16 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimization interrupted\n",
      "\n",
      "Best score: -1.2545\n",
      "Best parameters:\n",
      "   hidden_dim: 4\n",
      "   num_layers: 1\n",
      "   dropout: 0.29761191306325957\n",
      "   use_attention: False\n",
      "   bidirectional: True\n",
      "   sequence_length: 128\n",
      "   lr: 0.00011505137493659554\n",
      "   batch_size: 512\n",
      "   l2_reg: 3.4745331159811874e-05\n",
      "   gradient_clip: 4.217524229198063\n",
      "   l1_reg: 7.216654076139157e-05\n",
      "   early_stopping_patience: 35\n",
      "   min_delta: 1.4443628656009442e-06\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "creating run (0.1s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\gokde\\OneDrive\\Masaüstü\\Anomaly-Detection-and-Network-Optimization-in-SDNs\\real_project\\wandb\\run-20250901_150235-slviaczp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/slviaczp' target=\"_blank\">study_summary</a></strong> to <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/slviaczp' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/slviaczp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_hyperparams/batch_size</td><td>▁</td></tr><tr><td>best_hyperparams/dropout</td><td>▁</td></tr><tr><td>best_hyperparams/early_stopping_patience</td><td>▁</td></tr><tr><td>best_hyperparams/gradient_clip</td><td>▁</td></tr><tr><td>best_hyperparams/hidden_dim</td><td>▁</td></tr><tr><td>best_hyperparams/l1_reg</td><td>▁</td></tr><tr><td>best_hyperparams/l2_reg</td><td>▁</td></tr><tr><td>best_hyperparams/lr</td><td>▁</td></tr><tr><td>best_hyperparams/min_delta</td><td>▁</td></tr><tr><td>best_hyperparams/num_layers</td><td>▁</td></tr><tr><td>+4</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_hyperparams/batch_size</td><td>512</td></tr><tr><td>best_hyperparams/bidirectional</td><td>True</td></tr><tr><td>best_hyperparams/dropout</td><td>0.29761</td></tr><tr><td>best_hyperparams/early_stopping_patience</td><td>35</td></tr><tr><td>best_hyperparams/gradient_clip</td><td>4.21752</td></tr><tr><td>best_hyperparams/hidden_dim</td><td>4</td></tr><tr><td>best_hyperparams/l1_reg</td><td>7e-05</td></tr><tr><td>best_hyperparams/l2_reg</td><td>3e-05</td></tr><tr><td>best_hyperparams/lr</td><td>0.00012</td></tr><tr><td>best_hyperparams/min_delta</td><td>0.0</td></tr><tr><td>+7</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">study_summary</strong> at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/slviaczp' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/slviaczp</a><br> View project at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250901_150235-slviaczp\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TRAINING FINAL FIXED MODEL ===\n",
      "=== FIXED Data Preprocessing ===\n",
      "After cleaning - Train: 13675, Val: 7264, Test: 7264\n",
      "Class balance - Train: 0.560, Val: 0.365\n",
      "Calculated pos_weight: 0.89\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "creating run (0.2s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\gokde\\OneDrive\\Masaüstü\\Anomaly-Detection-and-Network-Optimization-in-SDNs\\real_project\\wandb\\run-20250901_150239-oeobekq3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/oeobekq3' target=\"_blank\">final_fixed_model</a></strong> to <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/oeobekq3' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/oeobekq3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with FIXED approach - monitoring for declining performance\n",
      "All hyperparameters logged to W&B project: lstm-cybersecurity-fixed\n",
      "Epoch  1 | Loss: T=0.6763 V=0.6575 Gap=-0.0188\n",
      "         | F1: T=0.5525 V=0.5683 | AUC: 0.6885\n",
      "         | Combined Performance: 0.6031\n",
      "    New best F1: 0.5683\n",
      "Epoch  2 | Loss: T=0.6727 V=0.6540 Gap=-0.0187\n",
      "         | F1: T=0.5777 V=0.5958 | AUC: 0.7457\n",
      "         | Combined Performance: 0.6397\n",
      "    New best F1: 0.5958\n",
      "Epoch  3 | Loss: T=0.6697 V=0.6508 Gap=-0.0189\n",
      "         | F1: T=0.5925 V=0.6238 | AUC: 0.7830\n",
      "         | Combined Performance: 0.6664\n",
      "    New best F1: 0.6238\n",
      "Epoch  4 | Loss: T=0.6666 V=0.6477 Gap=-0.0189\n",
      "         | F1: T=0.6017 V=0.6476 | AUC: 0.8075\n",
      "         | Combined Performance: 0.6856\n",
      "    New best F1: 0.6476\n",
      "Epoch  5 | Loss: T=0.6636 V=0.6446 Gap=-0.0190\n",
      "         | F1: T=0.6157 V=0.6585 | AUC: 0.8236\n",
      "         | Combined Performance: 0.6993\n",
      "    New best F1: 0.6585\n",
      "Epoch  6 | Loss: T=0.6606 V=0.6416 Gap=-0.0190\n",
      "         | F1: T=0.6232 V=0.6613 | AUC: 0.8347\n",
      "         | Combined Performance: 0.7064\n",
      "    New best F1: 0.6613\n",
      "Epoch  7 | Loss: T=0.6565 V=0.6384 Gap=-0.0181\n",
      "         | F1: T=0.6294 V=0.6562 | AUC: 0.8425\n",
      "         | Combined Performance: 0.7094\n",
      "Epoch  8 | Loss: T=0.6534 V=0.6353 Gap=-0.0181\n",
      "         | F1: T=0.6406 V=0.6434 | AUC: 0.8479\n",
      "         | Combined Performance: 0.7106\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# FIXED LSTM TRAINING - ADDRESSING DECLINING PERFORMANCE (WORKING + W&B)\n",
    "# Enhanced with comprehensive hyperparameter logging\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import wandb\n",
    "import pickle\n",
    "import json\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ---- W&B robust defaults (online if API key present, else offline) ----\n",
    "if not os.environ.get(\"WANDB_API_KEY\", \"\"):\n",
    "    os.environ[\"WANDB_MODE\"] = \"offline\"  # still logs locally\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def preprocess_data_fixed(X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "    print(\"=== FIXED Data Preprocessing ===\")\n",
    "\n",
    "    X_train = np.array(X_train, dtype=np.float32)\n",
    "    X_val   = np.array(X_val,   dtype=np.float32)\n",
    "    X_test  = np.array(X_test,  dtype=np.float32)\n",
    "    y_train = np.array(y_train, dtype=np.float32).reshape(-1)\n",
    "    y_val   = np.array(y_val,   dtype=np.float32).reshape(-1)\n",
    "    y_test  = np.array(y_test,  dtype=np.float32).reshape(-1)\n",
    "\n",
    "    def clean_sequences(X, y):\n",
    "        valid_mask = ~(np.isnan(X).any(axis=(1,2)) | np.isinf(X).any(axis=(1,2)) |\n",
    "                       np.isnan(y) | np.isinf(y))\n",
    "        return X[valid_mask], y[valid_mask]\n",
    "\n",
    "    X_train, y_train = clean_sequences(X_train, y_train)\n",
    "    X_val,   y_val   = clean_sequences(X_val,   y_val)\n",
    "    X_test,  y_test  = clean_sequences(X_test,  y_test)\n",
    "\n",
    "    print(f\"After cleaning - Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "\n",
    "    s_train, s_val, s_test = X_train.shape, X_val.shape, X_test.shape\n",
    "    X_train_f = X_train.reshape(-1, s_train[-1])\n",
    "    X_val_f   = X_val.reshape(-1, s_val[-1])\n",
    "    X_test_f  = X_test.reshape(-1, s_test[-1])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_f = scaler.fit_transform(X_train_f)\n",
    "    X_val_f   = scaler.transform(X_val_f)\n",
    "    X_test_f  = scaler.transform(X_test_f)\n",
    "\n",
    "    X_train = np.clip(X_train_f.reshape(s_train), -6.0, 6.0)\n",
    "    X_val   = np.clip(X_val_f.reshape(s_val),   -6.0, 6.0)\n",
    "    X_test  = np.clip(X_test_f.reshape(s_test), -6.0, 6.0)\n",
    "\n",
    "    train_pos_ratio = float(np.mean(y_train))\n",
    "    val_pos_ratio   = float(np.mean(y_val))\n",
    "    print(f\"Class balance - Train: {train_pos_ratio:.3f}, Val: {val_pos_ratio:.3f}\")\n",
    "\n",
    "    if train_pos_ratio > 0:\n",
    "        pos_weight = np.sqrt((1 - train_pos_ratio) / max(train_pos_ratio, 1e-8))\n",
    "        pos_weight = float(np.clip(pos_weight, 0.8, 3.0))\n",
    "    else:\n",
    "        pos_weight = 1.0\n",
    "    print(f\"Calculated pos_weight: {pos_weight:.2f}\")\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, scaler, pos_weight\n",
    "\n",
    "\n",
    "class FixedLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM with optional bidirectionality and simple additive attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout=0.2, bidirectional=False, use_attention=False):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.use_attention = use_attention\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "\n",
    "        lstm_out_dim = hidden_dim * (2 if bidirectional else 1)\n",
    "\n",
    "        # Simple additive attention over time if enabled\n",
    "        if use_attention:\n",
    "            self.attn = nn.Sequential(\n",
    "                nn.Linear(lstm_out_dim, lstm_out_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(lstm_out_dim, 1, bias=False)\n",
    "            )\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        inter = max(lstm_out_dim // 2, 32)\n",
    "        self.fc1 = nn.Linear(lstm_out_dim, inter)\n",
    "        self.dropout2 = nn.Dropout(dropout * 0.5)\n",
    "        self.fc_out = nn.Linear(inter, 1)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, p in self.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                nn.init.xavier_uniform_(p.data)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(p.data)\n",
    "            elif 'bias' in name:\n",
    "                p.data.fill_(0)\n",
    "            elif 'fc' in name and 'weight' in name:\n",
    "                nn.init.xavier_uniform_(p.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, F)\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)  # lstm_out: (B, T, H*)\n",
    "        if self.use_attention:\n",
    "            # scores: (B, T, 1)\n",
    "            scores = self.attn(lstm_out)\n",
    "            weights = torch.softmax(scores, dim=1)  # (B, T, 1)\n",
    "            context = torch.sum(weights * lstm_out, dim=1)  # (B, H*)\n",
    "            feats = context\n",
    "        else:\n",
    "            # last hidden (handle bidirectional)\n",
    "            if self.bidirectional:\n",
    "                # concat last layer's forward & backward\n",
    "                last_layer_h = h_n[-2:,:,:]  # (2, B, H)\n",
    "                feats = torch.cat([last_layer_h[0], last_layer_h[1]], dim=1)  # (B, 2H)\n",
    "            else:\n",
    "                feats = h_n[-1]  # (B, H)\n",
    "\n",
    "        x = self.dropout1(feats)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        out = self.fc_out(x)\n",
    "        return out.squeeze(-1)\n",
    "\n",
    "\n",
    "def train_lstm_fixed(X_train, y_train, X_val, y_val, params, model, epochs=50, trial=None, run_name=None):\n",
    "    if run_name is None:\n",
    "        run_name = f\"fixed_trial_{trial.number if trial else 'manual'}\"\n",
    "\n",
    "    # Create comprehensive config with ALL hyperparameters\n",
    "    wandb_config = dict(params)\n",
    "    wandb_config.update({\n",
    "        'epochs': epochs,\n",
    "        'model_params': int(sum(p.numel() for p in model.parameters())),\n",
    "        'device': str(device),\n",
    "        'train_samples': len(X_train),\n",
    "        'val_samples': len(X_val),\n",
    "        'input_features': X_train.shape[2],\n",
    "        'sequence_length_actual': X_train.shape[1],\n",
    "        'train_pos_ratio': float(np.mean(y_train)),\n",
    "        'val_pos_ratio': float(np.mean(y_val)),\n",
    "        # Model architecture details\n",
    "        'model_type': 'FixedLSTM',\n",
    "        'lstm_bidirectional': params.get('bidirectional', False),\n",
    "        'use_attention': params.get('use_attention', False),\n",
    "        'hidden_dim': params.get('hidden_dim'),\n",
    "        'num_layers': params.get('num_layers'),\n",
    "        'dropout_rate': params.get('dropout'),\n",
    "        # Training hyperparameters\n",
    "        'learning_rate': params.get('lr'),\n",
    "        'batch_size': params.get('batch_size'),\n",
    "        'l2_regularization': params.get('l2_reg'),\n",
    "        'l1_regularization': params.get('l1_reg', 0.0),\n",
    "        'gradient_clipping': params.get('gradient_clip'),\n",
    "        'pos_weight': params.get('pos_weight'),\n",
    "        'early_stopping_patience': params.get('early_stopping_patience', 15),\n",
    "        'min_delta': params.get('min_delta', 1e-5),\n",
    "        # Optimizer details\n",
    "        'optimizer_type': 'Adam',\n",
    "        'adam_beta1': 0.9,\n",
    "        'adam_beta2': 0.999,\n",
    "        'scheduler_type': 'ReduceLROnPlateau',\n",
    "        'scheduler_factor': 0.7,\n",
    "        'scheduler_patience': 7,\n",
    "        'scheduler_min_lr': 5e-6,\n",
    "        # Loss function\n",
    "        'loss_function': 'BCEWithLogitsLoss',\n",
    "        'criterion_pos_weight': params.get('pos_weight'),\n",
    "        # Data preprocessing\n",
    "        'data_normalization': 'StandardScaler',\n",
    "        'data_clipping_range': '(-6.0, 6.0)',\n",
    "        'decline_detection_threshold': 3,\n",
    "        # Trial info\n",
    "        'trial_number': trial.number if trial else None,\n",
    "        'optuna_study': trial.study.study_name if trial else None\n",
    "    })\n",
    "\n",
    "    run = wandb.init(\n",
    "        project=params.get(\"wandb_project\", \"lstm-cybersecurity-fixed\"),\n",
    "        config=wandb_config,\n",
    "        name=run_name,\n",
    "        reinit=True,\n",
    "        tags=[\"fixed\", \"lstm\", \"declining_performance_fix\", \"comprehensive_logging\"]\n",
    "    )\n",
    "    \n",
    "    # Log model architecture summary\n",
    "    wandb.log({\n",
    "        \"model_summary/total_params\": wandb_config['model_params'],\n",
    "        \"model_summary/trainable_params\": sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
    "        \"model_summary/lstm_output_dim\": wandb_config['hidden_dim'] * (2 if wandb_config['lstm_bidirectional'] else 1),\n",
    "        \"data_summary/feature_dim\": wandb_config['input_features'],\n",
    "        \"data_summary/sequence_length\": wandb_config['sequence_length_actual']\n",
    "    })\n",
    "    \n",
    "    wandb.watch(model, log=\"all\", log_freq=100)\n",
    "\n",
    "    print(f\"Training with FIXED approach - monitoring for declining performance\")\n",
    "    print(f\"All hyperparameters logged to W&B project: {wandb_config.get('wandb_project')}\")\n",
    "\n",
    "    try:\n",
    "        X_train = X_train.astype(np.float32)\n",
    "        X_val   = X_val.astype(np.float32)\n",
    "        y_train = y_train.astype(np.float32)\n",
    "        y_val   = y_val.astype(np.float32)\n",
    "\n",
    "        train_loader = DataLoader(TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train)),\n",
    "                                  batch_size=params['batch_size'], shuffle=False)\n",
    "        val_loader   = DataLoader(TensorDataset(torch.from_numpy(X_val),   torch.from_numpy(y_val)),\n",
    "                                  batch_size=params['batch_size'], shuffle=False)\n",
    "\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=params['lr'],\n",
    "            weight_decay=params['l2_reg'],\n",
    "            betas=(0.9, 0.999),\n",
    "        )\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='max', factor=0.7, patience=7, verbose=True, min_lr=5e-6\n",
    "        )\n",
    "\n",
    "        criterion = nn.BCEWithLogitsLoss(\n",
    "            pos_weight=torch.tensor([params['pos_weight']], device=device)\n",
    "        )\n",
    "\n",
    "        # Log optimizer and scheduler hyperparameters\n",
    "        wandb.log({\n",
    "            \"optimizer/initial_lr\": params['lr'],\n",
    "            \"optimizer/weight_decay\": params['l2_reg'],\n",
    "            \"optimizer/beta1\": 0.9,\n",
    "            \"optimizer/beta2\": 0.999,\n",
    "            \"scheduler/factor\": 0.7,\n",
    "            \"scheduler/patience\": 7,\n",
    "            \"scheduler/min_lr\": 5e-6,\n",
    "            \"loss/pos_weight\": params['pos_weight']\n",
    "        })\n",
    "\n",
    "        train_losses, val_losses = [], []\n",
    "        train_f1s, val_f1s, val_aucs = [], [], []\n",
    "        best_val_f1, patience_counter = 0.0, 0\n",
    "        best_model_state = None\n",
    "\n",
    "        performance_history = []\n",
    "        decline_threshold = 3\n",
    "\n",
    "        model.to(device)\n",
    "\n",
    "        # Log initial model state\n",
    "        wandb.log({\n",
    "            \"model_state/initial_weight_norm\": sum(p.norm().item() for p in model.parameters()),\n",
    "            \"training/decline_threshold\": decline_threshold,\n",
    "            \"training/max_epochs\": epochs\n",
    "        })\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # --- TRAIN ---\n",
    "            model.train()\n",
    "            epoch_loss = 0.0\n",
    "            train_preds_all, train_targets_all = [], []\n",
    "\n",
    "            for Xb, yb in train_loader:\n",
    "                Xb = Xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(Xb).view(-1)\n",
    "                loss = criterion(logits, yb.view(-1))\n",
    "                if params.get('l1_reg', 0) > 0:\n",
    "                    l1 = sum(p.abs().sum() for p in model.parameters())\n",
    "                    loss = loss + params['l1_reg'] * l1\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), params['gradient_clip'])\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += float(loss.item())\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    probs = torch.sigmoid(logits).detach().cpu().numpy().flatten()\n",
    "                    preds = (probs >= 0.5).astype(int)\n",
    "                    train_preds_all.extend(preds.tolist())\n",
    "                    train_targets_all.extend(yb.detach().cpu().numpy().flatten().tolist())\n",
    "\n",
    "            avg_train_loss = epoch_loss / max(1, len(train_loader))\n",
    "            train_losses.append(avg_train_loss)\n",
    "            train_f1 = f1_score(train_targets_all, train_preds_all, zero_division=0) if len(set(train_targets_all)) > 1 else 0.0\n",
    "            train_f1s.append(train_f1)\n",
    "\n",
    "            # --- VAL ---\n",
    "            model.eval()\n",
    "            val_loss_sum = 0.0\n",
    "            val_probs_all, val_targets_all = [], []\n",
    "            with torch.no_grad():\n",
    "                for Xb, yb in val_loader:\n",
    "                    Xb = Xb.to(device)\n",
    "                    yb = yb.to(device)\n",
    "                    logits = model(Xb).view(-1)\n",
    "                    val_loss_sum += float(criterion(logits, yb.view(-1)).item())\n",
    "                    probs = torch.sigmoid(logits).detach().cpu().numpy().flatten()\n",
    "                    val_probs_all.extend(probs.tolist())\n",
    "                    val_targets_all.extend(yb.detach().cpu().numpy().flatten().tolist())\n",
    "\n",
    "            avg_val_loss = val_loss_sum / max(1, len(val_loader))\n",
    "            val_losses.append(avg_val_loss)\n",
    "\n",
    "            if len(np.unique(val_targets_all)) > 1:\n",
    "                val_auc = roc_auc_score(val_targets_all, val_probs_all)\n",
    "            else:\n",
    "                val_auc = 0.5\n",
    "            val_aucs.append(val_auc)\n",
    "\n",
    "            best_val_f1_epoch = 0.0\n",
    "            vp = np.array(val_probs_all)\n",
    "            vt = np.array(val_targets_all)\n",
    "            for thresh in np.arange(0.3, 0.7, 0.05):\n",
    "                preds = (vp >= thresh).astype(int)\n",
    "                f1 = f1_score(vt, preds, zero_division=0)\n",
    "                if f1 > best_val_f1_epoch:\n",
    "                    best_val_f1_epoch = f1\n",
    "            val_f1s.append(best_val_f1_epoch)\n",
    "\n",
    "            current_perf = (train_f1 + best_val_f1_epoch + val_auc) / 3.0\n",
    "            performance_history.append(current_perf)\n",
    "\n",
    "            # Calculate gradient statistics for monitoring\n",
    "            grad_norms = [p.grad.norm().item() for p in model.parameters() if p.grad is not None]\n",
    "            avg_grad_norm = np.mean(grad_norms) if grad_norms else 0.0\n",
    "            max_grad_norm = np.max(grad_norms) if grad_norms else 0.0\n",
    "\n",
    "            # Check for performance decline\n",
    "            decline_detected = False\n",
    "            if len(performance_history) >= decline_threshold + 1:\n",
    "                recent = performance_history[-decline_threshold:]\n",
    "                if all(recent[i] <= recent[i-1] for i in range(1, len(recent))):\n",
    "                    decline_detected = True\n",
    "                    print(\"PERFORMANCE DECLINE DETECTED - stopping training\")\n",
    "                    print(f\"Recent performance: {recent}\")\n",
    "                    wandb.log({\"decline_stop\": 1, \"decline_detected_epoch\": epoch + 1})\n",
    "                    break\n",
    "\n",
    "            loss_gap = avg_val_loss - avg_train_loss\n",
    "            print(f\"Epoch {epoch+1:2d} | Loss: T={avg_train_loss:.4f} V={avg_val_loss:.4f} Gap={loss_gap:.4f}\")\n",
    "            print(f\"         | F1: T={train_f1:.4f} V={best_val_f1_epoch:.4f} | AUC: {val_auc:.4f}\")\n",
    "            print(f\"         | Combined Performance: {current_perf:.4f}\")\n",
    "\n",
    "            # Comprehensive logging to W&B\n",
    "            log_dict = {\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"train_loss\": avg_train_loss,\n",
    "                \"val_loss\": avg_val_loss,\n",
    "                \"loss_gap\": loss_gap,\n",
    "                \"train_f1\": train_f1,\n",
    "                \"val_f1\": best_val_f1_epoch,\n",
    "                \"val_auc\": val_auc,\n",
    "                \"combined_performance\": current_perf,\n",
    "                \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "                \"patience_counter\": patience_counter,\n",
    "                \"performance_trend\": np.mean(performance_history[-3:]) if len(performance_history) >= 3 else current_perf,\n",
    "                # Gradient monitoring\n",
    "                \"gradients/avg_norm\": avg_grad_norm,\n",
    "                \"gradients/max_norm\": max_grad_norm,\n",
    "                \"gradients/clipping_threshold\": params['gradient_clip'],\n",
    "                # Training dynamics\n",
    "                \"training/overfitting_gap\": loss_gap,\n",
    "                \"training/performance_stability\": np.std(performance_history[-5:]) if len(performance_history) >= 5 else 0.0,\n",
    "                \"training/decline_detected\": int(decline_detected),\n",
    "                # Model state monitoring\n",
    "                \"model_state/weight_norm\": sum(p.norm().item() for p in model.parameters()),\n",
    "                \"predictions/train_prob_mean\": np.mean([torch.sigmoid(model(torch.from_numpy(X_train[:100].astype(np.float32)).to(device))).detach().cpu().numpy().mean()]),\n",
    "                \"predictions/val_prob_mean\": np.mean(val_probs_all),\n",
    "                \"predictions/val_prob_std\": np.std(val_probs_all),\n",
    "            }\n",
    "            \n",
    "            # Add hyperparameter tracking each epoch (for easy filtering/analysis)\n",
    "            hyperparams_log = {f\"hyperparams/{k}\": v for k, v in params.items() if isinstance(v, (int, float, bool, str))}\n",
    "            log_dict.update(hyperparams_log)\n",
    "            \n",
    "            wandb.log(log_dict)\n",
    "\n",
    "            if best_val_f1_epoch > best_val_f1:\n",
    "                best_val_f1 = best_val_f1_epoch\n",
    "                best_model_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "                patience_counter = 0\n",
    "                print(f\"    New best F1: {best_val_f1:.4f}\")\n",
    "                wandb.log({\n",
    "                    \"best_val_f1\": best_val_f1,\n",
    "                    \"best_epoch\": epoch + 1,\n",
    "                    \"best_model_saved\": 1\n",
    "                })\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= 15:\n",
    "                print(f\"Early stopping after {patience_counter} epochs without improvement\")\n",
    "                wandb.log({\"early_stop\": 1, \"early_stop_epoch\": epoch + 1})\n",
    "                break\n",
    "\n",
    "            scheduler.step(current_perf)\n",
    "\n",
    "            if trial is not None:\n",
    "                trial.report(current_perf, epoch)\n",
    "                if trial.should_prune():\n",
    "                    wandb.log({\"pruned\": 1, \"pruned_epoch\": epoch + 1})\n",
    "                    raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        if best_model_state is not None:\n",
    "            model.load_state_dict({k: v.to(device) for k, v in best_model_state.items()})\n",
    "            print(f\"Restored best model with F1: {best_val_f1:.4f}\")\n",
    "\n",
    "        # Log final training summary\n",
    "        wandb.log({\n",
    "            \"summary/final_best_val_f1\": best_val_f1,\n",
    "            \"summary/total_epochs_trained\": epoch + 1,\n",
    "            \"summary/early_stopped\": int(patience_counter >= 15),\n",
    "            \"summary/decline_stopped\": int(decline_detected),\n",
    "            \"summary/final_lr\": optimizer.param_groups[0]['lr'],\n",
    "            \"summary/performance_decline_episodes\": sum(1 for i in range(1, len(performance_history)) \n",
    "                                                      if performance_history[i] < performance_history[i-1])\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in training: {e}\")\n",
    "        wandb.log({\"training_error\": str(e)})\n",
    "        raise\n",
    "    finally:\n",
    "        wandb.finish()\n",
    "\n",
    "    return model, {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_f1s': train_f1s,\n",
    "        'val_f1s': val_f1s,\n",
    "        'val_aucs': val_aucs,\n",
    "        'best_val_f1': best_val_f1,\n",
    "        'performance_history': performance_history\n",
    "    }\n",
    "\n",
    "\n",
    "def fixed_objective(trial):\n",
    "    print(f\"\\nFIXED Trial #{trial.number}\")\n",
    "\n",
    "    # --- Hyperparameter search space ---\n",
    "    params = {\n",
    "    'hidden_dim': trial.suggest_categorical(\n",
    "        'hidden_dim', [4, 8, 16, 32, 48, 64, 96, 128, 192, 256, 384, 512]\n",
    "    ),\n",
    "    'num_layers': trial.suggest_int('num_layers', 1, 10, step=1),\n",
    "    'dropout': trial.suggest_float('dropout', 0.05, 0.6),  # allow slightly lower and higher\n",
    "    'use_attention': trial.suggest_categorical('use_attention', [True, False]),\n",
    "    'bidirectional': trial.suggest_categorical('bidirectional', [True, False]),\n",
    "    'sequence_length': trial.suggest_categorical(\n",
    "        'sequence_length', [4, 8, 16, 24, 32, 48, 64, 96, 128, 160, 192]\n",
    "    ),\n",
    "    'lr': trial.suggest_float('lr', 1e-4, 1e-2, log=True),  # wider range\n",
    "    'batch_size': trial.suggest_categorical('batch_size', [16, 32, 64, 128, 256, 512, 1024]),\n",
    "    'l2_reg': trial.suggest_float('l2_reg', 1e-6, 5e-3, log=True),  # slightly wider\n",
    "    'gradient_clip': trial.suggest_float('gradient_clip', 0.5, 10.0),  # explore more\n",
    "    'l1_reg': trial.suggest_float('l1_reg', 1e-8, 1e-3, log=True),\n",
    "    'early_stopping_patience': trial.suggest_int('early_stopping_patience', 10, 60, step=5),\n",
    "    'min_delta': trial.suggest_float('min_delta', 1e-6, 1e-3, log=True),\n",
    "    'wandb_project': \"lstm-cybersecurity-fixed\"\n",
    "}\n",
    "\n",
    "\n",
    "    # --- Prepare data (sequence length adjustment, preprocessing) ---\n",
    "    seq_len = params['sequence_length']\n",
    "    def adjust_seq(x, target_len):\n",
    "        cur = x.shape[1]\n",
    "        if target_len == cur:\n",
    "            return x\n",
    "        if target_len < cur:\n",
    "            return x[:, -target_len:, :]\n",
    "        pad = target_len - cur\n",
    "        return np.pad(x, ((0,0), (pad,0), (0,0)), mode='constant')\n",
    "\n",
    "    X_train_trial = adjust_seq(globals()['X_train_seq'], seq_len)\n",
    "    X_val_trial   = adjust_seq(globals()['X_val_seq'],   seq_len)\n",
    "    X_test_trial  = adjust_seq(globals()['X_test_seq'],  seq_len)\n",
    "\n",
    "    X_train_c, X_val_c, X_test_c, y_train_c, y_val_c, y_test_c, scaler, pos_weight = preprocess_data_fixed(\n",
    "        X_train_trial, X_val_trial, X_test_trial,\n",
    "        globals()['y_train_seq'], globals()['y_val_seq'], globals()['y_test_seq']\n",
    "    )\n",
    "    params['pos_weight'] = pos_weight\n",
    "\n",
    "    model = FixedLSTM(\n",
    "        input_dim=X_train_c.shape[2],\n",
    "        hidden_dim=params['hidden_dim'],\n",
    "        num_layers=params['num_layers'],\n",
    "        dropout=params['dropout'],\n",
    "        bidirectional=params['bidirectional'],\n",
    "        use_attention=params['use_attention']\n",
    "    ).to(device)\n",
    "\n",
    "    run_name = f\"fixed_trial_{trial.number}_h{params['hidden_dim']}_l{params['num_layers']}_bi{int(params['bidirectional'])}_att{int(params['use_attention'])}\"\n",
    "\n",
    "    trained_model, history = train_lstm_fixed(\n",
    "        X_train_c, y_train_c, X_val_c, y_val_c,\n",
    "        params, model, epochs=100, trial=trial, run_name=run_name\n",
    "    )\n",
    "\n",
    "    best_f1 = history['best_val_f1']\n",
    "\n",
    "    # --- Loss decrease penalties ---\n",
    "    train_loss_decrease = max(0.0, history['train_losses'][0] - history['train_losses'][-1])\n",
    "    val_loss_decrease   = max(0.0, history['val_losses'][0] - history['val_losses'][-1])\n",
    "    decrease_penalty = max(0.0, 1.0 - train_loss_decrease - val_loss_decrease) * 5.0  # penalize if losses didn't decrease enough\n",
    "\n",
    "    # --- Loss gap penalty (want gap close to 0) ---\n",
    "    final_train_loss = history['train_losses'][-1]\n",
    "    final_val_loss   = history['val_losses'][-1]\n",
    "    loss_gap = abs(final_val_loss - final_train_loss)\n",
    "    gap_penalty = loss_gap * 2.0  # heavily penalize large gap\n",
    "\n",
    "    # --- Final score (Optuna maximizes this) ---\n",
    "    final_score = best_f1 - decrease_penalty - gap_penalty\n",
    "    print(f\"Trial {trial.number}: \"\n",
    "          f\"F1={best_f1:.4f}, Decrease_penalty={decrease_penalty:.4f}, \"\n",
    "          f\"Gap_penalty={gap_penalty:.4f}, Score={final_score:.4f}\")\n",
    "\n",
    "    return final_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_fixed_optimization():\n",
    "    print(\"\\n=== FIXED LSTM OPTIMIZATION ===\")\n",
    "    print(\"Target: Fix declining F1/AUC performance\")\n",
    "\n",
    "    required_vars = ['X_train_seq', 'y_train_seq', 'X_val_seq', 'y_val_seq', 'X_test_seq', 'y_test_seq']\n",
    "    miss = [v for v in required_vars if v not in globals()]\n",
    "    if miss:\n",
    "        print(f\"Missing variables: {miss}\")\n",
    "        print(\"Please ensure your data is loaded and named correctly.\")\n",
    "        return None\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        direction='maximize',\n",
    "        pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10),\n",
    "        study_name=f\"fixed_lstm_{int(time.time())}\"\n",
    "    )\n",
    "\n",
    "    print(\"Optimizing parameters...\")\n",
    "    try:\n",
    "        study.optimize(fixed_objective, n_trials=25, timeout=2400)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nOptimization interrupted\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nOptimization failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    best_trial = study.best_trial\n",
    "    print(f\"\\nBest score: {best_trial.value:.4f}\")\n",
    "    print(\"Best parameters:\")\n",
    "    for k, v in best_trial.params.items():\n",
    "        print(f\"   {k}: {v}\")\n",
    "\n",
    "    # Log study-level hyperparameter summary to W&B\n",
    "    study_summary_run = wandb.init(\n",
    "        project=\"lstm-cybersecurity-fixed\",\n",
    "        name=\"study_summary\",\n",
    "        tags=[\"study_summary\", \"best_hyperparams\"],\n",
    "        reinit=True\n",
    "    )\n",
    "    \n",
    "    # Log best hyperparameters with clear naming\n",
    "    best_hyperparams_log = {}\n",
    "    for k, v in best_trial.params.items():\n",
    "        best_hyperparams_log[f\"best_hyperparams/{k}\"] = v\n",
    "    \n",
    "    best_hyperparams_log.update({\n",
    "        \"study/best_score\": best_trial.value,\n",
    "        \"study/n_trials\": len(study.trials),\n",
    "        \"study/study_name\": study.study_name,\n",
    "        \"study/optimization_complete\": 1\n",
    "    })\n",
    "    \n",
    "    wandb.log(best_hyperparams_log)\n",
    "    wandb.finish()\n",
    "\n",
    "    print(\"\\n=== TRAINING FINAL FIXED MODEL ===\")\n",
    "    best_params = dict(best_trial.params)\n",
    "    best_params.update({'early_stopping_patience': 15, 'min_delta': 1e-5, 'wandb_project': \"lstm-cybersecurity-fixed\"})\n",
    "\n",
    "    seq_len = best_params['sequence_length']\n",
    "\n",
    "    def adjust_seq(x, target_len):\n",
    "        cur = x.shape[1]\n",
    "        if target_len == cur:\n",
    "            return x\n",
    "        if target_len < cur:\n",
    "            return x[:, -target_len:, :]\n",
    "        pad = target_len - cur\n",
    "        return np.pad(x, ((0,0), (pad,0), (0,0)), mode='constant')\n",
    "\n",
    "    X_train_final = adjust_seq(globals()['X_train_seq'], seq_len)\n",
    "    X_val_final   = adjust_seq(globals()['X_val_seq'],   seq_len)\n",
    "    X_test_final  = adjust_seq(globals()['X_test_seq'],  seq_len)\n",
    "\n",
    "    X_train_c, X_val_c, X_test_c, y_train_c, y_val_c, y_test_c, scaler, pos_weight = preprocess_data_fixed(\n",
    "        X_train_final, X_val_final, X_test_final, globals()['y_train_seq'], globals()['y_val_seq'], globals()['y_test_seq']\n",
    "    )\n",
    "\n",
    "    best_params['pos_weight'] = pos_weight\n",
    "\n",
    "    final_model = FixedLSTM(\n",
    "        input_dim=X_train_c.shape[2],\n",
    "        hidden_dim=best_params['hidden_dim'],\n",
    "        num_layers=best_params['num_layers'],\n",
    "        dropout=best_params['dropout'],\n",
    "        bidirectional=best_params['bidirectional'],\n",
    "        use_attention=best_params['use_attention']\n",
    "    ).to(device)\n",
    "\n",
    "    final_model, final_history = train_lstm_fixed(\n",
    "        X_train_c, y_train_c, X_val_c, y_val_c,\n",
    "        best_params, final_model, epochs=120, run_name=\"final_fixed_model\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n=== FINAL EVALUATION ===\")\n",
    "    final_model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_test_tensor = torch.from_numpy(X_test_c.astype(np.float32)).to(device)\n",
    "        test_logits = final_model(X_test_tensor)\n",
    "        test_probs = torch.sigmoid(test_logits).detach().cpu().numpy()\n",
    "\n",
    "        X_val_tensor = torch.from_numpy(X_val_c.astype(np.float32)).to(device)\n",
    "        val_probs = torch.sigmoid(final_model(X_val_tensor)).detach().cpu().numpy()\n",
    "\n",
    "        best_threshold, best_val_f1_thresh = 0.5, 0.0\n",
    "        for th in np.arange(0.3, 0.7, 0.05):\n",
    "            vpred = (val_probs >= th).astype(int)\n",
    "            vf1 = f1_score(y_val_c, vpred, zero_division=0)\n",
    "            if vf1 > best_val_f1_thresh:\n",
    "                best_val_f1_thresh = vf1\n",
    "                best_threshold = float(th)\n",
    "\n",
    "        test_preds = (test_probs >= best_threshold).astype(int)\n",
    "        test_f1 = f1_score(y_test_c, test_preds, zero_division=0)\n",
    "        test_auc = roc_auc_score(y_test_c, test_probs) if len(np.unique(y_test_c)) > 1 else 0.5\n",
    "        print(f\"Final Test Results:\\n   F1: {test_f1:.4f}\\n   AUC: {test_auc:.4f}\\n   Thr: {best_threshold:.3f}\")\n",
    "\n",
    "    # Log final test results with all hyperparameters\n",
    "    final_results_run = wandb.init(\n",
    "        project=\"lstm-cybersecurity-fixed\",\n",
    "        name=\"final_test_results\",\n",
    "        tags=[\"final_results\", \"test_evaluation\"],\n",
    "        reinit=True\n",
    "    )\n",
    "    \n",
    "    final_results_log = {\n",
    "        \"final_test/f1_score\": test_f1,\n",
    "        \"final_test/auc_score\": test_auc,\n",
    "        \"final_test/best_threshold\": best_threshold,\n",
    "        \"final_test/test_samples\": len(y_test_c),\n",
    "    }\n",
    "    \n",
    "    # Include all best hyperparameters in final results\n",
    "    for k, v in best_params.items():\n",
    "        if isinstance(v, (int, float, bool, str)):\n",
    "            final_results_log[f\"final_hyperparams/{k}\"] = v\n",
    "    \n",
    "    wandb.log(final_results_log)\n",
    "    wandb.finish()\n",
    "\n",
    "    torch.save(final_model.state_dict(), \"fixed_best_model.pth\")\n",
    "    with open(\"fixed_study.pkl\", \"wb\") as f:\n",
    "        pickle.dump(study, f)\n",
    "    with open(\"fixed_best_params.json\", \"w\") as f:\n",
    "        json.dump(best_params, f, indent=2)\n",
    "\n",
    "    return study, final_model, best_params\n",
    "\n",
    "\n",
    "def quick_diagnostic_check(model, X_train, X_val, y_train, y_val):\n",
    "    print(\"\\n=== PERFORMANCE DECLINE DIAGNOSTIC ===\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_tensor = torch.from_numpy(X_train[:min(1000, len(X_train))].astype(np.float32)).to(device)\n",
    "        val_tensor   = torch.from_numpy(X_val.astype(np.float32)).to(device)\n",
    "        train_probs  = torch.sigmoid(model(train_tensor)).detach().cpu().numpy()\n",
    "        val_probs    = torch.sigmoid(model(val_tensor)).detach().cpu().numpy()\n",
    "\n",
    "        tr_range = float(np.max(train_probs) - np.min(train_probs))\n",
    "        va_range = float(np.max(val_probs) - np.min(val_probs))\n",
    "        print(f\"Prediction analysis:\\n   Train prob range: {tr_range:.4f}\\n   Val prob range: {va_range:.4f}\\n\"\n",
    "              f\"   Train prob mean: {np.mean(train_probs):.4f}\\n   Val prob mean: {np.mean(val_probs):.4f}\")\n",
    "\n",
    "        tr_ent = -np.mean(train_probs * np.log(train_probs + 1e-8) + (1-train_probs) * np.log(1-train_probs + 1e-8))\n",
    "        va_ent = -np.mean(val_probs * np.log(val_probs + 1e-8) + (1-val_probs) * np.log(1-val_probs + 1e-8))\n",
    "        print(f\"   Train entropy: {tr_ent:.4f}\\n   Val entropy: {va_ent:.4f}\")\n",
    "\n",
    "    model.train()\n",
    "    dummy_input  = torch.from_numpy(X_train[:32].astype(np.float32)).to(device)\n",
    "    dummy_target = torch.from_numpy(y_train[:32].astype(np.float32)).to(device)\n",
    "    logits = model(dummy_input).view(-1)\n",
    "    loss = nn.BCEWithLogitsLoss()(logits, dummy_target.view(-1))\n",
    "    loss.backward()\n",
    "    grad_norms = [p.grad.norm().item() for p in model.parameters() if p.grad is not None]\n",
    "    if grad_norms:\n",
    "        avg_grad_norm = float(np.mean(grad_norms))\n",
    "        print(f\"   Average gradient norm: {avg_grad_norm:.6f}\")\n",
    "        if avg_grad_norm < 1e-6:\n",
    "            print(\"   WARNING: Very small gradients (vanishing).\")\n",
    "        elif avg_grad_norm > 10:\n",
    "            print(\"   WARNING: Very large gradients (exploding).\")\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# COMPLETE USAGE EXAMPLE\n",
    "# ==============================\n",
    "# Expect the following globals to be already defined in your environment:\n",
    "# X_train_seq, y_train_seq, X_val_seq, y_val_seq, X_test_seq, y_test_seq\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result = run_fixed_optimization()\n",
    "    if result is not None:\n",
    "        study_fixed, model_fixed, params_fixed = result\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"SUMMARY - FIXING DECLINING F1/AUC PERFORMANCE:\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"1) Correct Optuna sampling; 2) W&B always logs; 3) Optional attention/bidirectional wired;\")\n",
    "        print(\"4) Conservative regularization & BCEWithLogitsLoss; 5) Decline detection + early stop.\")\n",
    "        print(\"6) COMPREHENSIVE HYPERPARAMETER LOGGING TO W&B\")\n",
    "        print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0133f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎲 All seeds set to 42 for reproducibility\n",
      "\n",
      "=== ENHANCED STEP 7: Robust LSTM Training with WandB ===\n",
      "Initializing AdvancedLSTM:\n",
      "  Input dim: 50\n",
      "  Hidden dim: 64\n",
      "  Num layers: 5\n",
      "  Dropout: 0.5\n",
      "  Attention: False\n",
      "  Bidirectional: False\n",
      "  Total parameters: 169,217\n",
      "  Trainable parameters: 169,217\n",
      "Model parameters: 169,217\n",
      "Parameters per training sample: 25.98\n",
      "\n",
      "🔍 Debugging tensor shapes...\n",
      "\n",
      "=== TENSOR SHAPE DEBUGGING ===\n",
      "Input X shape: torch.Size([5, 64, 50])\n",
      "Input y shape: torch.Size([5])\n",
      "Raw model output shape: torch.Size([5])\n",
      "Raw model output: tensor([0.0423, 0.0424, 0.0424, 0.0424, 0.0424], device='cuda:0')\n",
      "After squeeze(-1): torch.Size([5])\n",
      "Target tensor final shape: torch.Size([5])\n",
      "Shapes match: True\n",
      "\n",
      "🚀 Starting enhanced robust training with WandB...\n",
      "🎲 All seeds set to 42 for reproducibility\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ WandB login successful - ONLINE MODE ENABLED\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to True."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">robust_training_1756670915</strong> at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-attack-detection-robust/runs/dbvfz2iz' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-attack-detection-robust/runs/dbvfz2iz</a><br> View project at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-attack-detection-robust' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-attack-detection-robust</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250831_230835-dbvfz2iz\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Cannot find valid git repo associated with this directory.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for wandb.init()..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\gokde\\OneDrive\\Masaüstü\\Anomaly-Detection-and-Network-Optimization-in-SDNs\\real_project\\wandb\\run-20250831_230921-d2q81p7m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-attack-detection-robust/runs/d2q81p7m' target=\"_blank\">robust_training_1756670961</a></strong> to <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-attack-detection-robust' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-attack-detection-robust' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-attack-detection-robust</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-attack-detection-robust/runs/d2q81p7m' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-attack-detection-robust/runs/d2q81p7m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: WandB run initialized!\n",
      "Starting training: 100 epochs, patience=15\n",
      "Epoch  1 | Loss: T=0.6312 V=0.5810 | F1: T=0.6282 V=0.7695 | AUC: 0.8621 | Time: 1.0s\n",
      "    ✅ New best F1: 0.7695\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x1000 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (SSLError), entering retry loop.\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# ENHANCED STEP 7: ROBUST TRAINING WITH WANDB + SEEDING\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import random\n",
    "import wandb\n",
    "import time\n",
    "from sklearn.metrics import f1_score, roc_auc_score, classification_report, confusion_matrix, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==============================\n",
    "# RANDOM SEEDING FOR REPRODUCIBILITY\n",
    "# ==============================\n",
    "def set_all_seeds(seed=42):\n",
    "    \"\"\"Set all random seeds for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    print(f\"🎲 All seeds set to {seed} for reproducibility\")\n",
    "\n",
    "# Set seeds at the start\n",
    "set_all_seeds(42)\n",
    "\n",
    "# ==============================\n",
    "# WANDB SETUP WITH FIXED SETTINGS\n",
    "# ==============================\n",
    "WANDB_API_KEY = \"4cac7a348a78f711d2890b70c3252efbe9c16fe5\"  # Your API key\n",
    "\n",
    "def setup_wandb(project_name, run_name, config=None):\n",
    "    \"\"\"Setup WandB with proper settings (no invalid parameters)\"\"\"\n",
    "    # Clear environment and setup\n",
    "    os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n",
    "    os.environ[\"WANDB_MODE\"] = \"online\"\n",
    "    \n",
    "    # Login\n",
    "    try:\n",
    "        wandb.login(key=WANDB_API_KEY, force=True, relogin=True)\n",
    "        print(\"✅ WandB login successful - ONLINE MODE ENABLED\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ WandB login failed: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Initialize with FIXED settings (no _disable_service)\n",
    "    run = wandb.init(\n",
    "        project=project_name,\n",
    "        name=run_name,\n",
    "        config=config,\n",
    "        mode=\"online\",\n",
    "        force=True,\n",
    "        reinit=True,\n",
    "        settings=wandb.Settings(_service_wait=300)  # Only valid parameter\n",
    "    )\n",
    "    \n",
    "    # Check if run is properly initialized\n",
    "    if run is None:\n",
    "        print(\"WARNING: WandB run not properly initialized!\")\n",
    "    else:\n",
    "        print(\"SUCCESS: WandB run initialized!\")\n",
    "    \n",
    "    return run\n",
    "\n",
    "# ==============================\n",
    "# ENHANCED ROBUST TRAINING FUNCTION\n",
    "# ==============================\n",
    "\n",
    "def train_lstm_robust_antioverfitting_wandb(X_train, y_train, X_val, y_val, params, model, epochs=30, use_wandb=True):\n",
    "    \"\"\"\n",
    "    Step 7 robust training with WandB logging and proper seeding\n",
    "    \"\"\"\n",
    "    # Set seeds for this training session\n",
    "    set_all_seeds(params.get('seed', 42))\n",
    "    \n",
    "    # Setup WandB if requested\n",
    "    if use_wandb:\n",
    "        wandb_run = setup_wandb(\n",
    "            project_name=\"lstm-attack-detection-robust\",\n",
    "            run_name=f\"robust_training_{int(time.time())}\",\n",
    "            config=params\n",
    "        )\n",
    "        wandb.watch(model, log=\"all\", log_freq=50)\n",
    "        \n",
    "        # Log model architecture\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        wandb.log({\n",
    "            \"model/total_parameters\": total_params,\n",
    "            \"model/params_per_sample\": total_params/len(X_train)\n",
    "        })\n",
    "    \n",
    "    # Data preparation (exactly as in Step 7)\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    X_val = X_val.astype(np.float32)\n",
    "    y_train = y_train.astype(np.float32)\n",
    "    y_val = y_val.astype(np.float32)\n",
    "\n",
    "    # NO SHUFFLING to preserve chronology (from Step 7)\n",
    "    train_dataset = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "    val_dataset = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
    "\n",
    "    # Fixed DataLoader settings for Windows\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=params['batch_size'], \n",
    "        shuffle=False, pin_memory=False, num_workers=0  # Windows fix\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=params['batch_size'],\n",
    "        shuffle=False, pin_memory=False, num_workers=0  # Windows fix\n",
    "    )\n",
    "\n",
    "    # Optimizer with weight decay (from Step 7)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=params['lr'],\n",
    "        weight_decay=params['l2_reg']\n",
    "    )\n",
    "\n",
    "    # Conservative scheduler (from Step 7)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.6, \n",
    "        patience=params.get('reduce_lr_patience', 3), verbose=True\n",
    "    )\n",
    "\n",
    "    # Loss with class weights\n",
    "    criterion = nn.BCEWithLogitsLoss(\n",
    "        pos_weight=torch.tensor([params['pos_weight']], device=device)\n",
    "    )\n",
    "\n",
    "    # Tracking variables (from Step 7)\n",
    "    train_losses, val_losses = [], []\n",
    "    train_f1s, val_f1s, val_aucs = [], [], []\n",
    "    best_val_f1, patience_counter = 0, 0\n",
    "    best_model_state = None\n",
    "    best_threshold = 0.5\n",
    "\n",
    "    print(f\"Starting training: {epochs} epochs, patience={params.get('early_stopping_patience', 15)}\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # =============\n",
    "        # TRAINING (Step 7 logic with WandB logging)\n",
    "        # =============\n",
    "        model.train()\n",
    "        epoch_loss, train_preds_all, train_targets_all = 0, [], []\n",
    "        total_grad_norm = 0\n",
    "        batch_count = 0\n",
    "\n",
    "        for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            \n",
    "            # ROBUST TENSOR SHAPE HANDLING (from Step 7)\n",
    "            if outputs.dim() > 1:\n",
    "                outputs = outputs.squeeze(-1)  # Remove only last dimension if it exists\n",
    "            if y_batch.dim() > 1:\n",
    "                y_batch = y_batch.squeeze(-1)  # Remove only last dimension if it exists\n",
    "                \n",
    "            # Ensure both tensors have same shape\n",
    "            if outputs.shape != y_batch.shape:\n",
    "                print(f\"Shape mismatch: outputs {outputs.shape}, targets {y_batch.shape}\")\n",
    "                outputs = outputs.view(-1)\n",
    "                y_batch = y_batch.view(-1)\n",
    "            \n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping (from Step 7)\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), params.get('gradient_clip', 1.0))\n",
    "            total_grad_norm += grad_norm\n",
    "            batch_count += 1\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Collect training predictions (from Step 7)\n",
    "            with torch.no_grad():\n",
    "                probs = torch.sigmoid(outputs).cpu().numpy().flatten()\n",
    "                preds = (probs >= 0.5).astype(int)\n",
    "                train_preds_all.extend(preds)\n",
    "                train_targets_all.extend(y_batch.cpu().numpy().flatten())\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Training F1\n",
    "        train_f1 = f1_score(train_targets_all, train_preds_all, zero_division=0)\n",
    "        train_f1s.append(train_f1)\n",
    "        \n",
    "        avg_grad_norm = total_grad_norm / batch_count if batch_count > 0 else 0\n",
    "\n",
    "        # =============\n",
    "        # VALIDATION (Step 7 logic with WandB logging)\n",
    "        # =============\n",
    "        model.eval()\n",
    "        val_loss, val_probs_all, val_targets_all = 0, [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                \n",
    "                # ROBUST TENSOR SHAPE HANDLING (from Step 7)\n",
    "                if outputs.dim() > 1:\n",
    "                    outputs = outputs.squeeze(-1)\n",
    "                if y_batch.dim() > 1:\n",
    "                    y_batch = y_batch.squeeze(-1)\n",
    "                    \n",
    "                # Ensure both tensors have same shape\n",
    "                if outputs.shape != y_batch.shape:\n",
    "                    outputs = outputs.view(-1)\n",
    "                    y_batch = y_batch.view(-1)\n",
    "                \n",
    "                if not torch.isnan(outputs).any():\n",
    "                    val_loss += criterion(outputs, y_batch).item()\n",
    "                    probs = torch.sigmoid(outputs).cpu().numpy().flatten()\n",
    "                    val_probs_all.extend(probs)\n",
    "                    val_targets_all.extend(y_batch.cpu().numpy().flatten())\n",
    "\n",
    "        if len(val_probs_all) > 0:\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            val_losses.append(avg_val_loss)\n",
    "            \n",
    "            val_probs_all = np.array(val_probs_all)\n",
    "            val_targets_all = np.array(val_targets_all)\n",
    "            \n",
    "            # Calculate metrics (from Step 7)\n",
    "            if len(np.unique(val_targets_all)) > 1:\n",
    "                val_auc = roc_auc_score(val_targets_all, val_probs_all)\n",
    "            else:\n",
    "                val_auc = 0.5\n",
    "            val_aucs.append(val_auc)\n",
    "            \n",
    "            # Best F1 across thresholds (from Step 7)\n",
    "            best_val_f1_epoch = 0\n",
    "            best_thresh_epoch = 0.5\n",
    "            threshold_f1_scores = []\n",
    "            for thresh in np.arange(0.1, 0.9, 0.05):\n",
    "                preds = (val_probs_all >= thresh).astype(int)\n",
    "                f1 = f1_score(val_targets_all, preds, zero_division=0)\n",
    "                threshold_f1_scores.append(f1)\n",
    "                if f1 > best_val_f1_epoch:\n",
    "                    best_val_f1_epoch = f1\n",
    "                    best_thresh_epoch = thresh\n",
    "            \n",
    "            val_f1s.append(best_val_f1_epoch)\n",
    "            \n",
    "            # Calculate additional metrics for WandB\n",
    "            val_precision = precision_score(val_targets_all, (val_probs_all >= best_thresh_epoch).astype(int), zero_division=0)\n",
    "            val_recall = recall_score(val_targets_all, (val_probs_all >= best_thresh_epoch).astype(int), zero_division=0)\n",
    "            \n",
    "            # Calculate overfitting metrics\n",
    "            loss_gap = avg_val_loss - avg_train_loss\n",
    "            f1_gap = train_f1 - best_val_f1_epoch if train_f1 > 0 else 0\n",
    "            \n",
    "            epoch_time = time.time() - epoch_start_time\n",
    "            \n",
    "            # LOG EVERYTHING TO WANDB\n",
    "            if use_wandb:\n",
    "                log_dict = {\n",
    "                    # Basic metrics\n",
    "                    \"epoch\": epoch,\n",
    "                    \"train/loss\": avg_train_loss,\n",
    "                    \"train/f1\": train_f1,\n",
    "                    \"val/loss\": avg_val_loss,\n",
    "                    \"val/f1\": best_val_f1_epoch,\n",
    "                    \"val/auc\": val_auc,\n",
    "                    \"val/precision\": val_precision,\n",
    "                    \"val/recall\": val_recall,\n",
    "                    \"val/best_threshold\": best_thresh_epoch,\n",
    "                    \n",
    "                    # Learning dynamics\n",
    "                    \"train/learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "                    \"train/gradient_norm\": avg_grad_norm,\n",
    "                    \"train/epoch_time\": epoch_time,\n",
    "                    \n",
    "                    # Overfitting indicators\n",
    "                    \"overfitting/loss_gap\": loss_gap,\n",
    "                    \"overfitting/f1_gap\": f1_gap,\n",
    "                    \"overfitting/loss_ratio\": avg_val_loss / avg_train_loss if avg_train_loss > 0 else 1,\n",
    "                    \n",
    "                    # Model tracking\n",
    "                    \"model/patience_counter\": patience_counter,\n",
    "                    \"model/best_val_f1\": best_val_f1,\n",
    "                    \"model/is_best_epoch\": best_val_f1_epoch > best_val_f1,\n",
    "                    \n",
    "                    # Data distribution\n",
    "                    \"data/val_pred_mean\": np.mean(val_probs_all),\n",
    "                    \"data/val_pred_std\": np.std(val_probs_all),\n",
    "                    \"data/val_class_balance\": np.mean(val_targets_all),\n",
    "                }\n",
    "                \n",
    "                # Add threshold analysis\n",
    "                thresholds = np.arange(0.1, 0.9, 0.05)\n",
    "                for i, (thresh, f1_score_thresh) in enumerate(zip(thresholds, threshold_f1_scores)):\n",
    "                    log_dict[f\"threshold_analysis/f1_at_{thresh:.2f}\"] = f1_score_thresh\n",
    "                \n",
    "                wandb.log(log_dict)\n",
    "                \n",
    "                # Log histograms every 5 epochs\n",
    "                if (epoch + 1) % 5 == 0:\n",
    "                    wandb.log({\n",
    "                        \"histograms/val_predictions\": wandb.Histogram(val_probs_all),\n",
    "                        \"histograms/val_targets\": wandb.Histogram(val_targets_all)\n",
    "                    })\n",
    "\n",
    "            # Print progress (from Step 7)\n",
    "            print(f\"Epoch {epoch+1:2d} | Loss: T={avg_train_loss:.4f} V={avg_val_loss:.4f} | \"\n",
    "                  f\"F1: T={train_f1:.4f} V={best_val_f1_epoch:.4f} | AUC: {val_auc:.4f} | \"\n",
    "                  f\"Time: {epoch_time:.1f}s\")\n",
    "\n",
    "            # Early stopping (from Step 7)\n",
    "            if best_val_f1_epoch > best_val_f1 + params.get('min_delta', 1e-4):\n",
    "                best_val_f1 = best_val_f1_epoch\n",
    "                best_threshold = best_thresh_epoch\n",
    "                best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "                patience_counter = 0\n",
    "                print(f\"    ✅ New best F1: {best_val_f1:.4f}\")\n",
    "                \n",
    "                if use_wandb:\n",
    "                    wandb.log({\"events/new_best_model\": True, \"epoch\": epoch})\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            # Overfitting detection (from Step 7)\n",
    "            if len(train_losses) > 1 and len(val_losses) > 1:\n",
    "                loss_gap = avg_val_loss - avg_train_loss\n",
    "                if loss_gap > 0.05 and epoch > 5:\n",
    "                    print(f\"    ⚠️ Loss gap detected: {loss_gap:.4f}\")\n",
    "                    if use_wandb:\n",
    "                        wandb.log({\"warnings/overfitting_detected\": True, \"epoch\": epoch})\n",
    "\n",
    "            # Learning rate scheduling (from Step 7)\n",
    "            scheduler.step(best_val_f1_epoch)\n",
    "\n",
    "            # Early stopping check\n",
    "            if patience_counter >= params.get('early_stopping_patience', 15):\n",
    "                print(f\"⏹ Early stopping at epoch {epoch+1}\")\n",
    "                if use_wandb:\n",
    "                    wandb.log({\"events/early_stopping\": True, \"final_epoch\": epoch})\n",
    "                break\n",
    "\n",
    "            # Enhanced plotting every 3 epochs (from Step 7)\n",
    "            if (epoch + 1) % 3 == 0 or epoch == 0:\n",
    "                plt.figure(figsize=(18, 10))\n",
    "\n",
    "                # 1. Loss with log scale\n",
    "                plt.subplot(2, 4, 1)\n",
    "                plt.plot(train_losses, 'b-o', label='Train', markersize=3)\n",
    "                plt.plot(val_losses, 'r-s', label='Validation', markersize=3)\n",
    "                plt.title(\"Loss (Log Scale)\")\n",
    "                plt.xlabel(\"Epoch\")\n",
    "                plt.ylabel(\"Loss\")\n",
    "                plt.yscale('log')\n",
    "                plt.legend()\n",
    "                plt.grid(True, alpha=0.3)\n",
    "\n",
    "                # 2. F1 Score Comparison  \n",
    "                plt.subplot(2, 4, 2)\n",
    "                plt.plot(train_f1s, 'g-o', label='Train F1', markersize=3)\n",
    "                plt.plot(val_f1s, 'm-s', label='Val F1', markersize=3)\n",
    "                plt.title(\"F1 Score Tracking\")\n",
    "                plt.xlabel(\"Epoch\")\n",
    "                plt.ylabel(\"F1 Score\")\n",
    "                plt.ylim(0, 1)\n",
    "                plt.legend()\n",
    "                plt.grid(True, alpha=0.3)\n",
    "\n",
    "                # 3. Validation AUC\n",
    "                plt.subplot(2, 4, 3)\n",
    "                plt.plot(val_aucs, 'brown', marker='o', markersize=3)\n",
    "                plt.title(\"Validation AUC\")\n",
    "                plt.xlabel(\"Epoch\")\n",
    "                plt.ylabel(\"AUC\")\n",
    "                plt.ylim(0.4, 1.0)\n",
    "                plt.grid(True, alpha=0.3)\n",
    "\n",
    "                # 4. Overfitting indicator\n",
    "                plt.subplot(2, 4, 4)\n",
    "                if len(val_losses) == len(train_losses):\n",
    "                    gap = np.array(val_losses) - np.array(train_losses)\n",
    "                    plt.plot(gap, 'red', marker='o', markersize=3)\n",
    "                    plt.title(\"Overfitting Gap (Val-Train Loss)\")\n",
    "                    plt.xlabel(\"Epoch\")\n",
    "                    plt.ylabel(\"Loss Difference\")\n",
    "                    plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "                    plt.grid(True, alpha=0.3)\n",
    "\n",
    "                # 5. Learning Rate\n",
    "                plt.subplot(2, 4, 5)\n",
    "                current_lr = optimizer.param_groups[0]['lr']\n",
    "                lr_history = [current_lr] * (epoch + 1)  # Simplified\n",
    "                plt.plot(lr_history, 'orange', marker='o', markersize=3)\n",
    "                plt.title(\"Learning Rate\")\n",
    "                plt.xlabel(\"Epoch\")\n",
    "                plt.ylabel(\"LR\")\n",
    "                plt.yscale('log')\n",
    "                plt.grid(True, alpha=0.3)\n",
    "\n",
    "                # 6. Prediction Distribution\n",
    "                plt.subplot(2, 4, 6)\n",
    "                if len(val_probs_all) > 0:\n",
    "                    plt.hist(val_probs_all, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "                plt.title(\"Val Prediction Distribution\")\n",
    "                plt.xlabel(\"Predicted Probability\")\n",
    "                plt.ylabel(\"Frequency\")\n",
    "                plt.grid(True, alpha=0.3)\n",
    "\n",
    "                # 7. Class Balance\n",
    "                plt.subplot(2, 4, 7)\n",
    "                train_unique, train_counts = np.unique(train_targets_all, return_counts=True)\n",
    "                val_unique, val_counts = np.unique(val_targets_all, return_counts=True)\n",
    "                \n",
    "                x_pos = np.arange(len(train_unique))\n",
    "                width = 0.35\n",
    "                plt.bar(x_pos - width/2, train_counts, width, label='Train', alpha=0.7)\n",
    "                plt.bar(x_pos + width/2, val_counts, width, label='Val', alpha=0.7)\n",
    "                plt.title(\"Class Distribution\")\n",
    "                plt.xlabel(\"Class\")\n",
    "                plt.ylabel(\"Count\")\n",
    "                plt.legend()\n",
    "                plt.xticks(x_pos, train_unique)\n",
    "\n",
    "                # 8. F1 vs AUC\n",
    "                plt.subplot(2, 4, 8)\n",
    "                if len(val_f1s) > 0 and len(val_aucs) > 0:\n",
    "                    plt.scatter(val_f1s, val_aucs, c=range(len(val_f1s)), cmap='viridis', s=30)\n",
    "                    plt.colorbar(label='Epoch')\n",
    "                plt.title(\"F1 vs AUC Evolution\")\n",
    "                plt.xlabel(\"F1 Score\")\n",
    "                plt.ylabel(\"AUC\")\n",
    "                plt.grid(True, alpha=0.3)\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                # Log plot to WandB\n",
    "                if use_wandb:\n",
    "                    wandb.log({\"training_plots\": wandb.Image(plt)})\n",
    "                    plt.close()\n",
    "\n",
    "        else:\n",
    "            print(f\"⚠️ No valid validation data at epoch {epoch+1}\")\n",
    "            if use_wandb:\n",
    "                wandb.log({\"events/no_valid_validation_data\": True, \"epoch\": epoch})\n",
    "            break\n",
    "\n",
    "    # Restore best model (from Step 7)\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict({k: v.to(device) for k, v in best_model_state.items()})\n",
    "        print(f\"✅ Restored best model with validation F1: {best_val_f1:.4f}\")\n",
    "    else:\n",
    "        print(\"⚠️ No improvement found, using final model\")\n",
    "\n",
    "    # Log final training summary to WandB\n",
    "    if use_wandb:\n",
    "        wandb.log({\n",
    "            \"final/best_val_f1\": best_val_f1,\n",
    "            \"final/best_threshold\": best_threshold,\n",
    "            \"final/total_epochs\": len(train_losses),\n",
    "            \"final/converged\": patience_counter < params.get('early_stopping_patience', 15)\n",
    "        })\n",
    "        \n",
    "        # Log training curves as summary\n",
    "        wandb.run.summary[\"train_loss_curve\"] = train_losses\n",
    "        wandb.run.summary[\"val_loss_curve\"] = val_losses\n",
    "        wandb.run.summary[\"train_f1_curve\"] = train_f1s\n",
    "        wandb.run.summary[\"val_f1_curve\"] = val_f1s\n",
    "        wandb.run.summary[\"val_auc_curve\"] = val_aucs\n",
    "\n",
    "    return model, {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_f1s': train_f1s,\n",
    "        'val_f1s': val_f1s,\n",
    "        'val_aucs': val_aucs,\n",
    "        'best_val_f1': best_val_f1,\n",
    "        'best_threshold': best_threshold\n",
    "    }\n",
    "\n",
    "# ==============================\n",
    "# DEBUG FUNCTION FROM STEP 7\n",
    "# ==============================\n",
    "\n",
    "def debug_tensor_shapes(model, X_sample, y_sample):\n",
    "    \"\"\"Debug function to check tensor shapes (from Step 7)\"\"\"\n",
    "    print(\"\\n=== TENSOR SHAPE DEBUGGING ===\")\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_tensor = torch.from_numpy(X_sample[:5].astype(np.float32)).to(device)\n",
    "        y_tensor = torch.from_numpy(y_sample[:5].astype(np.float32)).to(device)\n",
    "        \n",
    "        print(f\"Input X shape: {X_tensor.shape}\")\n",
    "        print(f\"Input y shape: {y_tensor.shape}\")\n",
    "        \n",
    "        outputs = model(X_tensor)\n",
    "        print(f\"Raw model output shape: {outputs.shape}\")\n",
    "        print(f\"Raw model output: {outputs}\")\n",
    "        \n",
    "        # Test different squeeze operations\n",
    "        squeezed_once = outputs.squeeze(-1) if outputs.dim() > 1 else outputs\n",
    "        print(f\"After squeeze(-1): {squeezed_once.shape}\")\n",
    "        \n",
    "        # What we need for loss function\n",
    "        if y_tensor.dim() > 1:\n",
    "            y_tensor_fixed = y_tensor.squeeze(-1)\n",
    "        else:\n",
    "            y_tensor_fixed = y_tensor\n",
    "        print(f\"Target tensor final shape: {y_tensor_fixed.shape}\")\n",
    "        \n",
    "        # Test if they match now\n",
    "        print(f\"Shapes match: {squeezed_once.shape == y_tensor_fixed.shape}\")\n",
    "        \n",
    "        return squeezed_once.shape, y_tensor_fixed.shape\n",
    "\n",
    "# ==============================\n",
    "# MAIN EXECUTION WITH STEP 7 LOGIC + WANDB\n",
    "# ==============================\n",
    "\n",
    "print(\"\\n=== ENHANCED STEP 7: Robust LSTM Training with WandB ===\")\n",
    "\n",
    "# Create model (use your existing AdvancedLSTM class)\n",
    "final_model = AdvancedLSTM(\n",
    "    input_dim=X_train_seq.shape[2],\n",
    "    hidden_dim=best_params['hidden_dim'],\n",
    "    num_layers=best_params['num_layers'],\n",
    "    dropout=best_params['dropout'],\n",
    "    use_attention=best_params['use_attention'],\n",
    "    bidirectional=best_params['bidirectional']\n",
    ").to(device)\n",
    "\n",
    "# Add seeding to best_params\n",
    "enhanced_params = best_params.copy()\n",
    "enhanced_params.update({\n",
    "    'seed': 42,\n",
    "    'early_stopping_patience': 15,\n",
    "    'gradient_clip': 1.0,\n",
    "    'reduce_lr_patience': 3,\n",
    "    'min_delta': 1e-4,\n",
    "    'epochs': 100\n",
    "})\n",
    "\n",
    "total_params = sum(p.numel() for p in final_model.parameters())\n",
    "print(f\"Model parameters: {total_params:,}\")\n",
    "print(f\"Parameters per training sample: {total_params/len(X_train_seq):.2f}\")\n",
    "\n",
    "# Debug tensor shapes before training (from Step 7)\n",
    "print(\"\\n🔍 Debugging tensor shapes...\")\n",
    "debug_tensor_shapes(final_model, X_train_seq, y_train_seq)\n",
    "\n",
    "# Execute enhanced robust training with WandB\n",
    "print(\"\\n🚀 Starting enhanced robust training with WandB...\")\n",
    "final_model, training_history = train_lstm_robust_antioverfitting_wandb(\n",
    "    X_train_seq, y_train_seq, X_val_seq, y_val_seq,\n",
    "    enhanced_params, final_model, epochs=enhanced_params['epochs'], use_wandb=True\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Training completed!\")\n",
    "if training_history['val_f1s']:\n",
    "    print(f\"Best validation F1: {max(training_history['val_f1s']):.4f}\")\n",
    "    print(f\"Best validation AUC: {max(training_history['val_aucs']):.4f}\")\n",
    "\n",
    "# ==============================\n",
    "# FINAL TEST EVALUATION WITH WANDB (Step 7 logic)\n",
    "# ==============================\n",
    "print(\"\\n=== Final Test Evaluation ===\")\n",
    "\n",
    "final_model.eval()\n",
    "with torch.no_grad():\n",
    "    X_test_tensor = torch.from_numpy(X_test_seq.astype(np.float32)).to(device)\n",
    "    test_outputs = final_model(X_test_tensor)\n",
    "    \n",
    "    # ROBUST SHAPE HANDLING (from Step 7)\n",
    "    if test_outputs.dim() > 1:\n",
    "        test_outputs = test_outputs.squeeze(-1)\n",
    "    \n",
    "    test_probs = torch.sigmoid(test_outputs).cpu().numpy().flatten()\n",
    "\n",
    "# Use the optimal threshold found during validation\n",
    "test_preds = (test_probs >= training_history['best_threshold']).astype(int)\n",
    "\n",
    "# Calculate comprehensive test metrics\n",
    "test_f1 = f1_score(y_test_seq, test_preds, zero_division=0)\n",
    "if len(np.unique(y_test_seq)) > 1:\n",
    "    test_auc = roc_auc_score(y_test_seq, test_probs)\n",
    "else:\n",
    "    test_auc = 0.5\n",
    "\n",
    "test_precision = precision_score(y_test_seq, test_preds, zero_division=0)\n",
    "test_recall = recall_score(y_test_seq, test_preds, zero_division=0)\n",
    "test_cm = confusion_matrix(y_test_seq, test_preds)\n",
    "\n",
    "# Log comprehensive test results to WandB\n",
    "tn, fp, fn, tp = test_cm.ravel()\n",
    "wandb.log({\n",
    "    \"test/f1\": test_f1,\n",
    "    \"test/auc\": test_auc,\n",
    "    \"test/precision\": test_precision,\n",
    "    \"test/recall\": test_recall,\n",
    "    \"test/threshold\": training_history['best_threshold'],\n",
    "    \"test/true_negatives\": tn,\n",
    "    \"test/false_positives\": fp,\n",
    "    \"test/false_negatives\": fn,\n",
    "    \"test/true_positives\": tp,\n",
    "    \"test/false_negative_rate\": fn/(fn+tp) if (fn+tp) > 0 else 0,\n",
    "    \"test/false_positive_rate\": fp/(fp+tn) if (fp+tn) > 0 else 0,\n",
    "    \"test/specificity\": tn/(tn+fp) if (tn+fp) > 0 else 0,\n",
    "    \"test/sensitivity\": tp/(tp+fn) if (tp+fn) > 0 else 0\n",
    "})\n",
    "\n",
    "# Log confusion matrix as heatmap\n",
    "wandb.log({\n",
    "    \"test/confusion_matrix\": wandb.plot.confusion_matrix(\n",
    "        preds=test_preds,\n",
    "        y_true=y_test_seq,\n",
    "        class_names=[\"Benign\", \"Attack\"]\n",
    "    )\n",
    "})\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 50)\n",
    "print(\"FINAL TEST RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"F1 Score: {test_f1:.4f}\")\n",
    "print(f\"AUC Score: {test_auc:.4f}\")\n",
    "print(f\"Precision: {test_precision:.4f}\")\n",
    "print(f\"Recall: {test_recall:.4f}\")\n",
    "print(f\"Threshold: {training_history['best_threshold']:.3f}\")\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"               Predicted\")\n",
    "print(f\"           Benign  Attack\")\n",
    "print(f\"Actual Benign   {test_cm[0,0]:6d}  {test_cm[0,1]:6d}\")\n",
    "print(f\"       Attack   {test_cm[1,0]:6d}  {test_cm[1,1]:6d}\")\n",
    "\n",
    "# ==============================\n",
    "# OVERFITTING ANALYSIS WITH WANDB (from Step 7)\n",
    "# ==============================\n",
    "print(\"\\n=== Overfitting Analysis ===\")\n",
    "\n",
    "if len(training_history['train_losses']) > 5 and len(training_history['val_losses']) > 5:\n",
    "    final_train_loss = training_history['train_losses'][-1]\n",
    "    final_val_loss = training_history['val_losses'][-1]\n",
    "    min_train_loss = min(training_history['train_losses'])\n",
    "    \n",
    "    loss_ratio = final_val_loss / final_train_loss\n",
    "    print(f\"Final loss ratio (Val/Train): {loss_ratio:.2f}\")\n",
    "    \n",
    "    # Log overfitting analysis to WandB\n",
    "    wandb.log({\n",
    "        \"analysis/loss_ratio\": loss_ratio,\n",
    "        \"analysis/min_train_loss\": min_train_loss,\n",
    "        \"analysis/final_train_loss\": final_train_loss,\n",
    "        \"analysis/final_val_loss\": final_val_loss\n",
    "    })\n",
    "    \n",
    "    if loss_ratio > 3:\n",
    "        print(\"🚨 HIGH overfitting detected (Val loss >> Train loss)\")\n",
    "        wandb.log({\"analysis/overfitting_level\": \"HIGH\"})\n",
    "    elif loss_ratio > 1.5:\n",
    "        print(\"⚠️ Moderate overfitting detected\")\n",
    "        wandb.log({\"analysis/overfitting_level\": \"MODERATE\"})\n",
    "    else:\n",
    "        print(\"✅ Good loss ratio - minimal overfitting\")\n",
    "        wandb.log({\"analysis/overfitting_level\": \"MINIMAL\"})\n",
    "    \n",
    "    # Check for training loss collapse\n",
    "    if min_train_loss < 0.01:\n",
    "        print(\"⚠️ Training loss very low - possible memorization\")\n",
    "        wandb.log({\"analysis/possible_memorization\": True})\n",
    "    \n",
    "    # F1 gap analysis (from Step 7)\n",
    "    if training_history['train_f1s'] and training_history['val_f1s']:\n",
    "        f1_gap = max(training_history['train_f1s']) - max(training_history['val_f1s'])\n",
    "        print(f\"F1 gap (Train - Val): {f1_gap:.4f}\")\n",
    "        \n",
    "        wandb.log({\"analysis/f1_gap\": f1_gap})\n",
    "        \n",
    "        if f1_gap > 0.2:\n",
    "            print(\"🚨 Large F1 gap - significant overfitting\")\n",
    "            wandb.log({\"analysis/f1_overfitting\": \"SIGNIFICANT\"})\n",
    "        elif f1_gap > 0.1:\n",
    "            print(\"⚠️ Moderate F1 gap\")\n",
    "            wandb.log({\"analysis/f1_overfitting\": \"MODERATE\"})\n",
    "        else:\n",
    "            print(\"✅ Good F1 generalization\")\n",
    "            wandb.log({\"analysis/f1_overfitting\": \"MINIMAL\"})\n",
    "\n",
    "# Training summary (from Step 7)\n",
    "print(\"\\n=== Training Summary ===\")\n",
    "print(\"If you're still seeing overfitting:\")\n",
    "print(\"1. Reduce hidden_dim further (try 32 or 16)\")\n",
    "print(\"2. Increase dropout to 0.6-0.8\") \n",
    "print(\"3. Reduce sequence length\")\n",
    "print(\"4. Add more data augmentation\")\n",
    "print(\"5. Consider simpler models (non-LSTM)\")\n",
    "\n",
    "# ==============================\n",
    "# SAVE MODEL AND RESULTS WITH WANDB\n",
    "# ==============================\n",
    "\n",
    "# Save model (from Step 7)\n",
    "torch.save(final_model.state_dict(), \"enhanced_robust_lstm_model.pth\")\n",
    "print(\"💾 Model saved as enhanced_robust_lstm_model.pth\")\n",
    "\n",
    "# Prepare comprehensive results\n",
    "results = {\n",
    "    'training_history': training_history,\n",
    "    'test_metrics': {\n",
    "        'f1': test_f1,\n",
    "        'auc': test_auc,\n",
    "        'precision': test_precision,\n",
    "        'recall': test_recall,\n",
    "        'threshold': training_history['best_threshold'],\n",
    "        'confusion_matrix': test_cm.tolist()\n",
    "    },\n",
    "    'model_params': enhanced_params,\n",
    "    'overfitting_analysis': {\n",
    "        'loss_ratio': loss_ratio if 'loss_ratio' in locals() else None,\n",
    "        'f1_gap': f1_gap if 'f1_gap' in locals() else None\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save results locally\n",
    "import json\n",
    "with open(\"enhanced_robust_lstm_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2, default=str)\n",
    "\n",
    "# Log artifacts to WandB\n",
    "wandb.save(\"enhanced_robust_lstm_model.pth\")\n",
    "wandb.save(\"enhanced_robust_lstm_results.json\")\n",
    "\n",
    "# Final summary to WandB\n",
    "wandb.run.summary.update({\n",
    "    \"final_test_f1\": test_f1,\n",
    "    \"final_test_auc\": test_auc,\n",
    "    \"final_test_precision\": test_precision,\n",
    "    \"final_test_recall\": test_recall,\n",
    "    \"final_threshold\": training_history['best_threshold'],\n",
    "    \"model_parameters\": total_params,\n",
    "    \"reproducible_seed\": enhanced_params['seed']\n",
    "})\n",
    "\n",
    "print(f\"\\n💾 All files saved and logged to WandB!\")\n",
    "print(f\"✅ Enhanced Step 7 training completed successfully!\")\n",
    "print(f\"🔗 Check WandB dashboard for comprehensive training analytics!\")\n",
    "\n",
    "# Finish WandB run\n",
    "wandb.finish()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ENHANCED STEP 7 COMPLETE - FILES SAVED:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"📁 Model: enhanced_robust_lstm_model.pth\")\n",
    "print(\"📁 Results: enhanced_robust_lstm_results.json\") \n",
    "print(\"🔗 WandB: Check your dashboard for detailed training logs!\")\n",
    "print(\"🎲 All training was seeded for reproducibility\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5a5fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting systematic debugging...\n",
      "================================================================================\n",
      "DEBUGGING DATA DIFFERENCES\n",
      "================================================================================\n",
      "\n",
      "Method 1: Full Dataset Approach\n",
      "----------------------------------------\n",
      "Full dataset raw shape: (553749, 50)\n",
      "Full dataset sequences: (553740, 10, 50)\n",
      "Full dataset labels: (553740,)\n",
      "Label distribution: [264817 288923]\n",
      "Attack percentage: 0.5218\n",
      "First 10 labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Last 10 labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Method 2: Chronological Split Approach\n",
      "----------------------------------------\n",
      "Test split raw shape: (83063, 50)\n",
      "Test split sequences: (83054, 10, 50)\n",
      "Test split labels: (83054,)\n",
      "Label distribution: [83054]\n",
      "Attack percentage: 0.0000\n",
      "First 10 labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Last 10 labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "COMPARISON\n",
      "----------------------------------------\n",
      "Size difference: 553740 vs 83054\n",
      "Attack rate difference: 0.5218 vs 0.0000\n",
      "❌ Different dataset sizes - this explains the discrepancy!\n",
      "\n",
      "================================================================================\n",
      "TESTING SAME MODEL ON BOTH DATASETS\n",
      "================================================================================\n",
      "\n",
      "Testing on Full Dataset...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 146\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;66;03m# Step 2: Test same model on both datasets\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mmodel2\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m():\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     preds_1, labels_1, probs_1, preds_2, labels_2, probs_2 = \u001b[43mtest_model_on_both_datasets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_seq_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_seq_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_seq_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_seq_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m80\u001b[39m)\n\u001b[32m    151\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSUMMARY OF DIFFERENCES\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 133\u001b[39m, in \u001b[36mtest_model_on_both_datasets\u001b[39m\u001b[34m(model, X_seq_1, y_seq_1, X_seq_2, y_seq_2, device)\u001b[39m\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m all_preds, all_labels, all_probs\n\u001b[32m    132\u001b[39m \u001b[38;5;66;03m# Test on both datasets\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m preds_1, labels_1, probs_1 = \u001b[43mevaluate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_seq_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_seq_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFull Dataset\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m preds_2, labels_2, probs_2 = evaluate_dataset(X_seq_2, y_seq_2, \u001b[33m\"\u001b[39m\u001b[33mChronological Test Split\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m preds_1, labels_1, probs_1, preds_2, labels_2, probs_2\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 107\u001b[39m, in \u001b[36mtest_model_on_both_datasets.<locals>.evaluate_dataset\u001b[39m\u001b[34m(X_seq, y_seq, dataset_name)\u001b[39m\n\u001b[32m    104\u001b[39m all_probs = []\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gokde\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gokde\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    756\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    759\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gokde\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gokde\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[39m, in \u001b[36mdefault_collate\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault_collate\u001b[39m(batch):\n\u001b[32m    338\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    339\u001b[39m \u001b[33;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[32m    340\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m \u001b[33;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gokde\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:211\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    208\u001b[39m transposed = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(*batch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtransposed\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gokde\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:212\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    208\u001b[39m transposed = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(*batch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[32m    214\u001b[39m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gokde\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gokde\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:272\u001b[39m, in \u001b[36mcollate_tensor_fn\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    270\u001b[39m     storage = elem._typed_storage()._new_shared(numel, device=elem.device)\n\u001b[32m    271\u001b[39m     out = elem.new(storage).resize_(\u001b[38;5;28mlen\u001b[39m(batch), *\u001b[38;5;28mlist\u001b[39m(elem.size()))\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "def debug_data_differences():\n",
    "    \"\"\"\n",
    "    Debug function to identify exactly what's different between the two testing approaches.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"DEBUGGING DATA DIFFERENCES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Method 1: Full dataset (like your \"corrected model loading and testing v3\")\n",
    "    print(\"\\nMethod 1: Full Dataset Approach\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    X_full_1 = datadf.drop(columns=[\"Label\"]).to_numpy(dtype=np.float32)\n",
    "    y_full_1 = datadf[\"Label\"].to_numpy(dtype=np.int32)\n",
    "    \n",
    "    X_test_seq_1, y_test_seq_1 = create_sequences(\n",
    "        X_full_1, \n",
    "        y_full_1, \n",
    "        sequence_length=seq_length,\n",
    "        label_strategy='last'\n",
    "    )\n",
    "    \n",
    "    print(f\"Full dataset raw shape: {X_full_1.shape}\")\n",
    "    print(f\"Full dataset sequences: {X_test_seq_1.shape}\")\n",
    "    print(f\"Full dataset labels: {y_test_seq_1.shape}\")\n",
    "    print(f\"Label distribution: {np.bincount(y_test_seq_1.astype(int))}\")\n",
    "    print(f\"Attack percentage: {np.mean(y_test_seq_1):.4f}\")\n",
    "    print(f\"First 10 labels: {y_test_seq_1[:10]}\")\n",
    "    print(f\"Last 10 labels: {y_test_seq_1[-10:]}\")\n",
    "    \n",
    "    # Method 2: Chronological split (like universal tester)\n",
    "    print(\"\\nMethod 2: Chronological Split Approach\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    (X_train, y_train), (X_val, y_val), (X_test, y_test) = chrono_split_train_val_test(\n",
    "        datadf, \n",
    "        label_col=\"Label\", \n",
    "        train_ratio=0.7, \n",
    "        val_ratio=0.15\n",
    "    )\n",
    "    \n",
    "    X_test_seq_2, y_test_seq_2 = create_sequences(\n",
    "        X_test, \n",
    "        y_test, \n",
    "        sequence_length=seq_length,\n",
    "        label_strategy='last'\n",
    "    )\n",
    "    \n",
    "    print(f\"Test split raw shape: {X_test.shape}\")\n",
    "    print(f\"Test split sequences: {X_test_seq_2.shape}\")\n",
    "    print(f\"Test split labels: {y_test_seq_2.shape}\")\n",
    "    print(f\"Label distribution: {np.bincount(y_test_seq_2.astype(int))}\")\n",
    "    print(f\"Attack percentage: {np.mean(y_test_seq_2):.4f}\")\n",
    "    print(f\"First 10 labels: {y_test_seq_2[:10]}\")\n",
    "    print(f\"Last 10 labels: {y_test_seq_2[-10:]}\")\n",
    "    \n",
    "    # Compare the data\n",
    "    print(\"\\nCOMPARISON\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Size difference: {X_test_seq_1.shape[0]} vs {X_test_seq_2.shape[0]}\")\n",
    "    print(f\"Attack rate difference: {np.mean(y_test_seq_1):.4f} vs {np.mean(y_test_seq_2):.4f}\")\n",
    "    \n",
    "    if X_test_seq_1.shape[0] == X_test_seq_2.shape[0]:\n",
    "        print(\"Same number of sequences - checking if data is identical...\")\n",
    "        if np.array_equal(X_test_seq_1, X_test_seq_2):\n",
    "            print(\"✅ Features are identical\")\n",
    "        else:\n",
    "            print(\"❌ Features are different!\")\n",
    "            \n",
    "        if np.array_equal(y_test_seq_1, y_test_seq_2):\n",
    "            print(\"✅ Labels are identical\")\n",
    "        else:\n",
    "            print(\"❌ Labels are different!\")\n",
    "    else:\n",
    "        print(\"❌ Different dataset sizes - this explains the discrepancy!\")\n",
    "    \n",
    "    return X_test_seq_1, y_test_seq_1, X_test_seq_2, y_test_seq_2\n",
    "\n",
    "def test_model_on_both_datasets(model, X_seq_1, y_seq_1, X_seq_2, y_seq_2, device):\n",
    "    \"\"\"\n",
    "    Test the same model on both datasets to see prediction differences.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TESTING SAME MODEL ON BOTH DATASETS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    def evaluate_dataset(X_seq, y_seq, dataset_name):\n",
    "        print(f\"\\nTesting on {dataset_name}...\")\n",
    "        \n",
    "        X_tensor = torch.tensor(X_seq, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y_seq, dtype=torch.float32)\n",
    "        \n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "        \n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        all_probs = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                probs = torch.sigmoid(outputs)\n",
    "                preds = torch.round(probs)\n",
    "                \n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(y_batch.cpu().numpy())\n",
    "                all_probs.extend(probs.cpu().numpy())\n",
    "        \n",
    "        all_preds = np.array(all_preds)\n",
    "        all_labels = np.array(all_labels)\n",
    "        all_probs = np.array(all_probs)\n",
    "        \n",
    "        accuracy = np.mean(all_preds == all_labels)\n",
    "        \n",
    "        print(f\"  Samples: {len(all_labels):,}\")\n",
    "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"  Attack rate (true): {np.mean(all_labels):.4f}\")\n",
    "        print(f\"  Attack rate (pred): {np.mean(all_preds):.4f}\")\n",
    "        print(f\"  Mean probability: {np.mean(all_probs):.4f}\")\n",
    "        print(f\"  Prob std: {np.std(all_probs):.4f}\")\n",
    "        \n",
    "        return all_preds, all_labels, all_probs\n",
    "    \n",
    "    # Test on both datasets\n",
    "    preds_1, labels_1, probs_1 = evaluate_dataset(X_seq_1, y_seq_1, \"Full Dataset\")\n",
    "    preds_2, labels_2, probs_2 = evaluate_dataset(X_seq_2, y_seq_2, \"Chronological Test Split\")\n",
    "    \n",
    "    return preds_1, labels_1, probs_1, preds_2, labels_2, probs_2\n",
    "\n",
    "# Run the debugging\n",
    "print(\"Starting systematic debugging...\")\n",
    "\n",
    "# Step 1: Compare the datasets\n",
    "X_seq_1, y_seq_1, X_seq_2, y_seq_2 = debug_data_differences()\n",
    "\n",
    "# Step 2: Test same model on both datasets\n",
    "if 'model2' in globals():\n",
    "    preds_1, labels_1, probs_1, preds_2, labels_2, probs_2 = test_model_on_both_datasets(\n",
    "        model2, X_seq_1, y_seq_1, X_seq_2, y_seq_2, device\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SUMMARY OF DIFFERENCES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\nDataset sizes:\")\n",
    "    print(f\"  Full dataset: {len(y_seq_1):,} sequences\")\n",
    "    print(f\"  Test split: {len(y_seq_2):,} sequences\")\n",
    "    print(f\"  Ratio: {len(y_seq_2)/len(y_seq_1):.3f}\")\n",
    "    \n",
    "    print(f\"\\nClass distributions:\")\n",
    "    print(f\"  Full dataset - Attack rate: {np.mean(y_seq_1):.4f}\")\n",
    "    print(f\"  Test split - Attack rate: {np.mean(y_seq_2):.4f}\")\n",
    "    \n",
    "    print(f\"\\nModel performance:\")\n",
    "    print(f\"  Full dataset - Accuracy: {np.mean(preds_1 == labels_1):.4f}\")\n",
    "    print(f\"  Test split - Accuracy: {np.mean(preds_2 == labels_2):.4f}\")\n",
    "    \n",
    "    print(f\"\\nPrediction characteristics:\")\n",
    "    print(f\"  Full dataset - Mean pred prob: {np.mean(probs_1):.4f}\")\n",
    "    print(f\"  Test split - Mean pred prob: {np.mean(probs_2):.4f}\")\n",
    "    \n",
    "    # Check temporal distribution\n",
    "    if len(y_seq_1) > len(y_seq_2):\n",
    "        test_start_idx = len(y_seq_1) - len(y_seq_2)\n",
    "        print(f\"\\nTemporal check:\")\n",
    "        print(f\"  Test split should be the LAST {len(y_seq_2)} sequences from full dataset\")\n",
    "        print(f\"  Checking if test split matches full dataset[{test_start_idx}:]...\")\n",
    "        \n",
    "        # Compare the tail of full dataset with test split\n",
    "        if np.array_equal(y_seq_1[test_start_idx:], y_seq_2):\n",
    "            print(\"  ✅ Test split correctly matches the tail of full dataset\")\n",
    "        else:\n",
    "            print(\"  ❌ Test split does NOT match the tail of full dataset\")\n",
    "            print(\"     This indicates different preprocessing or splitting!\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ model2 not found. Please load your model first.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONCLUSION:\")\n",
    "print(\"The results should be different because:\")\n",
    "print(\"1. Full dataset includes training data (data leakage)\")\n",
    "print(\"2. Test split uses only unseen chronological test data\")\n",
    "print(\"3. Different class distributions between full vs test\")\n",
    "print(\"\\nThe UNIVERSAL TESTER result is correct for model evaluation.\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "efdf0a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAINING FINAL MODEL WITH SPECIFIC HYPERPARAMETERS ===\n",
      "Hyperparameters:\n",
      "   epochs: 100\n",
      "   sequence_length: 128\n",
      "   pos_weight: 0.88654\n",
      "   num_layers: 1\n",
      "   min_delta: 1.4444e-06\n",
      "   lr: 0.00011505\n",
      "   l2_reg: 3.4745e-05\n",
      "   l1_reg: 7.2167e-05\n",
      "   early_stopping_patience: 35\n",
      "   hidden_dim: 4\n",
      "   gradient_clip: 4.21752\n",
      "   dropout: 0.29761\n",
      "   batch_size: 512\n",
      "   bidirectional: True\n",
      "   use_attention: True\n",
      "   wandb_project: lstm-cybersecurity-fixed\n",
      "\n",
      "Data shapes after sequence adjustment:\n",
      "Train: (13675, 128, 50), Val: (7264, 128, 50), Test: (7264, 128, 50)\n",
      "=== FIXED Data Preprocessing ===\n",
      "After cleaning - Train: 13675, Val: 7264, Test: 7264\n",
      "Class balance - Train: 0.560, Val: 0.365\n",
      "Calculated pos_weight: 0.89\n",
      "Using specified pos_weight: 0.88654 (calculated: 0.88654)\n",
      "Initializing AdvancedLSTM:\n",
      "  Input dim: 50\n",
      "  Hidden dim: 4\n",
      "  Num layers: 1\n",
      "  Dropout: 0.29761\n",
      "  Attention: True\n",
      "  Bidirectional: True\n",
      "  Total parameters: 2,137\n",
      "  Trainable parameters: 2,137\n",
      "\n",
      "Model created with 2137 parameters\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to True."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">experiment_run</strong> at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/AnomalyDetectionInSDNs/runs/gniy7pjk' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/AnomalyDetectionInSDNs/runs/gniy7pjk</a><br> View project at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/AnomalyDetectionInSDNs' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/AnomalyDetectionInSDNs</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250902_114617-gniy7pjk\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "creating run (0.1s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\gokde\\OneDrive\\Masaüstü\\Anomaly-Detection-and-Network-Optimization-in-SDNs\\real_project\\wandb\\run-20250902_114811-lokxz6u8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/lokxz6u8' target=\"_blank\">best_model_final_test</a></strong> to <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/lokxz6u8' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/lokxz6u8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training - logging to W&B project: lstm-cybersecurity-fixed\n",
      "Run name: best_model_final_test\n",
      "Epoch  1/100 | Loss: T=0.7131 V=0.6426 Gap=-0.0705\n",
      "         | F1: T=0.0000 V=0.5347 | AUC: 0.6060 | Perf: 0.3802\n",
      "    ✓ New best F1: 0.5347\n",
      "Epoch  2/100 | Loss: T=0.7070 V=0.6419 Gap=-0.0651\n",
      "         | F1: T=0.0000 V=0.5427 | AUC: 0.6947 | Perf: 0.4125\n",
      "    ✓ New best F1: 0.5427\n",
      "Epoch  3/100 | Loss: T=0.7015 V=0.6378 Gap=-0.0637\n",
      "         | F1: T=0.0000 V=0.6229 | AUC: 0.7962 | Perf: 0.4730\n",
      "    ✓ New best F1: 0.6229\n",
      "Epoch  4/100 | Loss: T=0.6957 V=0.6329 Gap=-0.0628\n",
      "         | F1: T=0.0000 V=0.7153 | AUC: 0.8322 | Perf: 0.5158\n",
      "    ✓ New best F1: 0.7153\n",
      "Epoch  5/100 | Loss: T=0.6885 V=0.6279 Gap=-0.0606\n",
      "         | F1: T=0.0000 V=0.7681 | AUC: 0.8531 | Perf: 0.5404\n",
      "    ✓ New best F1: 0.7681\n",
      "Epoch  6/100 | Loss: T=0.6824 V=0.6234 Gap=-0.0589\n",
      "         | F1: T=0.0000 V=0.7936 | AUC: 0.8640 | Perf: 0.5525\n",
      "    ✓ New best F1: 0.7936\n",
      "Epoch  7/100 | Loss: T=0.6776 V=0.6194 Gap=-0.0581\n",
      "         | F1: T=0.0000 V=0.8055 | AUC: 0.8674 | Perf: 0.5576\n",
      "    ✓ New best F1: 0.8055\n",
      "Epoch  8/100 | Loss: T=0.6738 V=0.6160 Gap=-0.0577\n",
      "         | F1: T=0.0000 V=0.8154 | AUC: 0.8676 | Perf: 0.5610\n",
      "    ✓ New best F1: 0.8154\n",
      "Epoch  9/100 | Loss: T=0.6710 V=0.6130 Gap=-0.0580\n",
      "         | F1: T=0.0000 V=0.8228 | AUC: 0.8675 | Perf: 0.5634\n",
      "    ✓ New best F1: 0.8228\n",
      "Epoch 10/100 | Loss: T=0.6679 V=0.6093 Gap=-0.0587\n",
      "         | F1: T=0.0000 V=0.8288 | AUC: 0.8681 | Perf: 0.5656\n",
      "    ✓ New best F1: 0.8288\n",
      "Epoch 11/100 | Loss: T=0.6655 V=0.6061 Gap=-0.0595\n",
      "         | F1: T=0.0000 V=0.8322 | AUC: 0.8706 | Perf: 0.5676\n",
      "    ✓ New best F1: 0.8322\n",
      "Epoch 12/100 | Loss: T=0.6625 V=0.6033 Gap=-0.0592\n",
      "         | F1: T=0.0000 V=0.8343 | AUC: 0.8701 | Perf: 0.5681\n",
      "    ✓ New best F1: 0.8343\n",
      "Epoch 13/100 | Loss: T=0.6601 V=0.6008 Gap=-0.0593\n",
      "         | F1: T=0.0000 V=0.8360 | AUC: 0.8716 | Perf: 0.5692\n",
      "    ✓ New best F1: 0.8360\n",
      "Epoch 14/100 | Loss: T=0.6571 V=0.5976 Gap=-0.0595\n",
      "         | F1: T=0.0000 V=0.8376 | AUC: 0.8735 | Perf: 0.5704\n",
      "    ✓ New best F1: 0.8376\n",
      "Epoch 15/100 | Loss: T=0.6550 V=0.5947 Gap=-0.0603\n",
      "         | F1: T=0.0000 V=0.8388 | AUC: 0.8756 | Perf: 0.5715\n",
      "    ✓ New best F1: 0.8388\n",
      "Epoch 16/100 | Loss: T=0.6523 V=0.5926 Gap=-0.0597\n",
      "         | F1: T=0.0000 V=0.8396 | AUC: 0.8767 | Perf: 0.5721\n",
      "    ✓ New best F1: 0.8396\n",
      "Epoch 17/100 | Loss: T=0.6489 V=0.5887 Gap=-0.0603\n",
      "         | F1: T=0.0000 V=0.8399 | AUC: 0.8861 | Perf: 0.5753\n",
      "    ✓ New best F1: 0.8399\n",
      "Epoch 18/100 | Loss: T=0.6461 V=0.5864 Gap=-0.0597\n",
      "         | F1: T=0.0000 V=0.8410 | AUC: 0.8909 | Perf: 0.5773\n",
      "    ✓ New best F1: 0.8410\n",
      "Epoch 19/100 | Loss: T=0.6437 V=0.5830 Gap=-0.0608\n",
      "         | F1: T=0.0000 V=0.8406 | AUC: 0.8944 | Perf: 0.5783\n",
      "Epoch 20/100 | Loss: T=0.6406 V=0.5795 Gap=-0.0611\n",
      "         | F1: T=0.0000 V=0.8410 | AUC: 0.9022 | Perf: 0.5810\n",
      "Epoch 21/100 | Loss: T=0.6371 V=0.5768 Gap=-0.0603\n",
      "         | F1: T=0.0000 V=0.8411 | AUC: 0.9037 | Perf: 0.5816\n",
      "    ✓ New best F1: 0.8411\n",
      "Epoch 22/100 | Loss: T=0.6355 V=0.5742 Gap=-0.0613\n",
      "         | F1: T=0.0000 V=0.8411 | AUC: 0.9048 | Perf: 0.5820\n",
      "Epoch 23/100 | Loss: T=0.6308 V=0.5708 Gap=-0.0600\n",
      "         | F1: T=0.0000 V=0.8416 | AUC: 0.9044 | Perf: 0.5820\n",
      "    ✓ New best F1: 0.8416\n",
      "Epoch 24/100 | Loss: T=0.6275 V=0.5673 Gap=-0.0602\n",
      "         | F1: T=0.0000 V=0.8417 | AUC: 0.9048 | Perf: 0.5822\n",
      "    ✓ New best F1: 0.8417\n",
      "Epoch 25/100 | Loss: T=0.6255 V=0.5635 Gap=-0.0620\n",
      "         | F1: T=0.0000 V=0.8417 | AUC: 0.9081 | Perf: 0.5833\n",
      "    ✓ New best F1: 0.8417\n",
      "Epoch 26/100 | Loss: T=0.6224 V=0.5608 Gap=-0.0615\n",
      "         | F1: T=0.0000 V=0.8424 | AUC: 0.9035 | Perf: 0.5819\n",
      "    ✓ New best F1: 0.8424\n",
      "Epoch 27/100 | Loss: T=0.6183 V=0.5579 Gap=-0.0604\n",
      "         | F1: T=0.0000 V=0.8427 | AUC: 0.9026 | Perf: 0.5818\n",
      "    ✓ New best F1: 0.8427\n",
      "Epoch 28/100 | Loss: T=0.6164 V=0.5548 Gap=-0.0615\n",
      "         | F1: T=0.0000 V=0.8436 | AUC: 0.9008 | Perf: 0.5815\n",
      "    ✓ New best F1: 0.8436\n",
      "Epoch 29/100 | Loss: T=0.6125 V=0.5503 Gap=-0.0622\n",
      "         | F1: T=0.0000 V=0.8434 | AUC: 0.9002 | Perf: 0.5812\n",
      "Epoch 30/100 | Loss: T=0.6103 V=0.5466 Gap=-0.0637\n",
      "         | F1: T=0.0000 V=0.8437 | AUC: 0.9018 | Perf: 0.5818\n",
      "    ✓ New best F1: 0.8437\n",
      "Epoch 31/100 | Loss: T=0.6071 V=0.5440 Gap=-0.0631\n",
      "         | F1: T=0.0000 V=0.8437 | AUC: 0.9009 | Perf: 0.5816\n",
      "Epoch 32/100 | Loss: T=0.6024 V=0.5418 Gap=-0.0605\n",
      "         | F1: T=0.0000 V=0.8432 | AUC: 0.8996 | Perf: 0.5809\n",
      "Epoch 33/100 | Loss: T=0.5992 V=0.5373 Gap=-0.0618\n",
      "         | F1: T=0.0000 V=0.8431 | AUC: 0.9004 | Perf: 0.5812\n",
      "Epoch 34/100 | Loss: T=0.5965 V=0.5335 Gap=-0.0630\n",
      "         | F1: T=0.0000 V=0.8433 | AUC: 0.9019 | Perf: 0.5818\n",
      "Epoch 35/100 | Loss: T=0.5948 V=0.5327 Gap=-0.0621\n",
      "         | F1: T=0.0000 V=0.8433 | AUC: 0.9013 | Perf: 0.5815\n",
      "Epoch 36/100 | Loss: T=0.5910 V=0.5298 Gap=-0.0612\n",
      "         | F1: T=0.0000 V=0.8436 | AUC: 0.9019 | Perf: 0.5818\n",
      "Epoch 37/100 | Loss: T=0.5878 V=0.5279 Gap=-0.0600\n",
      "         | F1: T=0.0000 V=0.8437 | AUC: 0.9026 | Perf: 0.5821\n",
      "Epoch 38/100 | Loss: T=0.5862 V=0.5257 Gap=-0.0605\n",
      "         | F1: T=0.0000 V=0.8437 | AUC: 0.9031 | Perf: 0.5823\n",
      "Epoch 39/100 | Loss: T=0.5846 V=0.5238 Gap=-0.0608\n",
      "         | F1: T=0.0000 V=0.8437 | AUC: 0.9029 | Perf: 0.5822\n",
      "Epoch 40/100 | Loss: T=0.5828 V=0.5213 Gap=-0.0615\n",
      "         | F1: T=0.0000 V=0.8435 | AUC: 0.9036 | Perf: 0.5824\n",
      "Epoch 41/100 | Loss: T=0.5808 V=0.5191 Gap=-0.0618\n",
      "         | F1: T=0.0000 V=0.8437 | AUC: 0.9032 | Perf: 0.5823\n",
      "Epoch 42/100 | Loss: T=0.5804 V=0.5171 Gap=-0.0632\n",
      "         | F1: T=0.0000 V=0.8436 | AUC: 0.9034 | Perf: 0.5824\n",
      "Epoch 43/100 | Loss: T=0.5774 V=0.5161 Gap=-0.0613\n",
      "         | F1: T=0.0000 V=0.8435 | AUC: 0.9033 | Perf: 0.5823\n",
      "Epoch 44/100 | Loss: T=0.5757 V=0.5135 Gap=-0.0622\n",
      "         | F1: T=0.0000 V=0.8437 | AUC: 0.9052 | Perf: 0.5830\n",
      "Epoch 45/100 | Loss: T=0.5739 V=0.5127 Gap=-0.0612\n",
      "         | F1: T=0.0000 V=0.8441 | AUC: 0.9054 | Perf: 0.5832\n",
      "    ✓ New best F1: 0.8441\n",
      "Epoch 46/100 | Loss: T=0.5740 V=0.5116 Gap=-0.0623\n",
      "         | F1: T=0.0000 V=0.8438 | AUC: 0.9052 | Perf: 0.5830\n",
      "Epoch 47/100 | Loss: T=0.5722 V=0.5099 Gap=-0.0623\n",
      "         | F1: T=0.0000 V=0.8440 | AUC: 0.9057 | Perf: 0.5832\n",
      "Epoch 48/100 | Loss: T=0.5712 V=0.5083 Gap=-0.0629\n",
      "         | F1: T=0.0000 V=0.8442 | AUC: 0.9059 | Perf: 0.5834\n",
      "    ✓ New best F1: 0.8442\n",
      "Epoch 49/100 | Loss: T=0.5676 V=0.5060 Gap=-0.0616\n",
      "         | F1: T=0.0000 V=0.8443 | AUC: 0.9072 | Perf: 0.5838\n",
      "    ✓ New best F1: 0.8443\n",
      "Epoch 50/100 | Loss: T=0.5673 V=0.5058 Gap=-0.0615\n",
      "         | F1: T=0.3768 V=0.8444 | AUC: 0.9076 | Perf: 0.7096\n",
      "    ✓ New best F1: 0.8444\n",
      "Epoch 51/100 | Loss: T=0.5667 V=0.5053 Gap=-0.0614\n",
      "         | F1: T=0.7524 V=0.8442 | AUC: 0.9068 | Perf: 0.8345\n",
      "Epoch 52/100 | Loss: T=0.5648 V=0.5029 Gap=-0.0619\n",
      "         | F1: T=0.7618 V=0.8444 | AUC: 0.9087 | Perf: 0.8383\n",
      "Epoch 53/100 | Loss: T=0.5656 V=0.5002 Gap=-0.0655\n",
      "         | F1: T=0.7591 V=0.8447 | AUC: 0.9087 | Perf: 0.8375\n",
      "    ✓ New best F1: 0.8447\n",
      "Epoch 54/100 | Loss: T=0.5621 V=0.4987 Gap=-0.0634\n",
      "         | F1: T=0.7607 V=0.8446 | AUC: 0.9134 | Perf: 0.8396\n",
      "Epoch 55/100 | Loss: T=0.5614 V=0.4983 Gap=-0.0630\n",
      "         | F1: T=0.7627 V=0.8446 | AUC: 0.9132 | Perf: 0.8402\n",
      "Epoch 56/100 | Loss: T=0.5596 V=0.4964 Gap=-0.0632\n",
      "         | F1: T=0.7697 V=0.8444 | AUC: 0.9144 | Perf: 0.8428\n",
      "Epoch 57/100 | Loss: T=0.5595 V=0.4951 Gap=-0.0645\n",
      "         | F1: T=0.7711 V=0.8446 | AUC: 0.9115 | Perf: 0.8424\n",
      "Epoch 58/100 | Loss: T=0.5580 V=0.4931 Gap=-0.0649\n",
      "         | F1: T=0.7758 V=0.8444 | AUC: 0.9134 | Perf: 0.8445\n",
      "Epoch 59/100 | Loss: T=0.5546 V=0.4925 Gap=-0.0622\n",
      "         | F1: T=0.7840 V=0.8445 | AUC: 0.9174 | Perf: 0.8486\n",
      "Epoch 60/100 | Loss: T=0.5548 V=0.4916 Gap=-0.0632\n",
      "         | F1: T=0.7811 V=0.8447 | AUC: 0.9161 | Perf: 0.8473\n",
      "Epoch 61/100 | Loss: T=0.5529 V=0.4898 Gap=-0.0632\n",
      "         | F1: T=0.7874 V=0.8443 | AUC: 0.9183 | Perf: 0.8500\n",
      "Epoch 62/100 | Loss: T=0.5526 V=0.4880 Gap=-0.0646\n",
      "         | F1: T=0.7868 V=0.8444 | AUC: 0.9201 | Perf: 0.8504\n",
      "Epoch 63/100 | Loss: T=0.5506 V=0.4881 Gap=-0.0625\n",
      "         | F1: T=0.7953 V=0.8445 | AUC: 0.9163 | Perf: 0.8520\n",
      "Epoch 64/100 | Loss: T=0.5510 V=0.4858 Gap=-0.0653\n",
      "         | F1: T=0.7983 V=0.8444 | AUC: 0.9179 | Perf: 0.8536\n",
      "Epoch 65/100 | Loss: T=0.5494 V=0.4847 Gap=-0.0647\n",
      "         | F1: T=0.7932 V=0.8444 | AUC: 0.9177 | Perf: 0.8518\n",
      "Epoch 66/100 | Loss: T=0.5479 V=0.4846 Gap=-0.0633\n",
      "         | F1: T=0.7994 V=0.8445 | AUC: 0.9151 | Perf: 0.8530\n",
      "Epoch 67/100 | Loss: T=0.5451 V=0.4832 Gap=-0.0619\n",
      "         | F1: T=0.8012 V=0.8444 | AUC: 0.9181 | Perf: 0.8546\n",
      "Epoch 68/100 | Loss: T=0.5456 V=0.4801 Gap=-0.0655\n",
      "         | F1: T=0.8060 V=0.8444 | AUC: 0.9205 | Perf: 0.8569\n",
      "Epoch 69/100 | Loss: T=0.5425 V=0.4806 Gap=-0.0619\n",
      "         | F1: T=0.8112 V=0.8445 | AUC: 0.9164 | Perf: 0.8574\n",
      "Epoch 70/100 | Loss: T=0.5442 V=0.4789 Gap=-0.0653\n",
      "         | F1: T=0.8125 V=0.8444 | AUC: 0.9194 | Perf: 0.8588\n",
      "Epoch 71/100 | Loss: T=0.5424 V=0.4754 Gap=-0.0670\n",
      "         | F1: T=0.8131 V=0.8443 | AUC: 0.9233 | Perf: 0.8602\n",
      "Epoch 72/100 | Loss: T=0.5396 V=0.4765 Gap=-0.0631\n",
      "         | F1: T=0.8162 V=0.8444 | AUC: 0.9210 | Perf: 0.8605\n",
      "Epoch 73/100 | Loss: T=0.5399 V=0.4769 Gap=-0.0631\n",
      "         | F1: T=0.8205 V=0.8445 | AUC: 0.9132 | Perf: 0.8594\n",
      "Epoch 74/100 | Loss: T=0.5382 V=0.4754 Gap=-0.0628\n",
      "         | F1: T=0.8197 V=0.8443 | AUC: 0.9155 | Perf: 0.8598\n",
      "Epoch 75/100 | Loss: T=0.5377 V=0.4721 Gap=-0.0657\n",
      "         | F1: T=0.8240 V=0.8443 | AUC: 0.9211 | Perf: 0.8631\n",
      "Epoch 76/100 | Loss: T=0.5357 V=0.4711 Gap=-0.0646\n",
      "         | F1: T=0.8254 V=0.8443 | AUC: 0.9208 | Perf: 0.8635\n",
      "Epoch 77/100 | Loss: T=0.5359 V=0.4703 Gap=-0.0656\n",
      "         | F1: T=0.8262 V=0.8443 | AUC: 0.9217 | Perf: 0.8640\n",
      "Epoch 78/100 | Loss: T=0.5345 V=0.4735 Gap=-0.0609\n",
      "         | F1: T=0.8306 V=0.8439 | AUC: 0.9083 | Perf: 0.8609\n",
      "Epoch 79/100 | Loss: T=0.5333 V=0.4691 Gap=-0.0641\n",
      "         | F1: T=0.8432 V=0.8438 | AUC: 0.9079 | Perf: 0.8649\n",
      "Epoch 80/100 | Loss: T=0.5295 V=0.4678 Gap=-0.0617\n",
      "         | F1: T=0.8433 V=0.8439 | AUC: 0.9075 | Perf: 0.8649\n",
      "Epoch 81/100 | Loss: T=0.5317 V=0.4667 Gap=-0.0650\n",
      "         | F1: T=0.8438 V=0.8438 | AUC: 0.9062 | Perf: 0.8646\n",
      "Epoch 82/100 | Loss: T=0.5310 V=0.4644 Gap=-0.0666\n",
      "         | F1: T=0.8406 V=0.8440 | AUC: 0.9056 | Perf: 0.8634\n",
      "Epoch 83/100 | Loss: T=0.5283 V=0.4638 Gap=-0.0644\n",
      "         | F1: T=0.8445 V=0.8441 | AUC: 0.9051 | Perf: 0.8646\n",
      "Epoch 84/100 | Loss: T=0.5261 V=0.4631 Gap=-0.0630\n",
      "         | F1: T=0.8543 V=0.8442 | AUC: 0.9050 | Perf: 0.8678\n",
      "Epoch 85/100 | Loss: T=0.5245 V=0.4613 Gap=-0.0631\n",
      "         | F1: T=0.8522 V=0.8443 | AUC: 0.9050 | Perf: 0.8672\n",
      "Epoch 86/100 | Loss: T=0.5222 V=0.4622 Gap=-0.0600\n",
      "         | F1: T=0.8595 V=0.8440 | AUC: 0.9046 | Perf: 0.8694\n",
      "Epoch 87/100 | Loss: T=0.5233 V=0.4624 Gap=-0.0609\n",
      "         | F1: T=0.8577 V=0.8440 | AUC: 0.9045 | Perf: 0.8687\n",
      "Epoch 88/100 | Loss: T=0.5224 V=0.4594 Gap=-0.0631\n",
      "         | F1: T=0.8626 V=0.8441 | AUC: 0.9044 | Perf: 0.8704\n",
      "Early stopping after 35 epochs without improvement\n",
      "\n",
      "Restored best model with validation F1: 0.8447\n",
      "\n",
      "=== FINAL TEST EVALUATION ===\n",
      "Final Test Results:\n",
      "   F1 Score: 0.4812\n",
      "   AUC Score: 0.7503\n",
      "   Best Threshold: 0.450\n",
      "   Test Samples: 7264\n",
      "\n",
      "✓ Training completed and logged to W&B run: best_model_final_test\n",
      "✓ All results saved to W&B project: lstm-cybersecurity-fixed\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▇▇▇██</td></tr><tr><td>best_model_saved</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>best_val_f1</td><td>▁▁▃▅▆▇▇▇███████████████████████</td></tr><tr><td>combined_performance</td><td>▁▁▃▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄███████████████████</td></tr><tr><td>early_stop</td><td>▁</td></tr><tr><td>early_stop_epoch</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>final_test/auc_score</td><td>▁</td></tr><tr><td>final_test/best_threshold</td><td>▁</td></tr><tr><td>final_test/completed</td><td>▁</td></tr><tr><td>+34</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>53</td></tr><tr><td>best_model_saved</td><td>1</td></tr><tr><td>best_val_f1</td><td>0.84467</td></tr><tr><td>combined_performance</td><td>0.87036</td></tr><tr><td>early_stop</td><td>1</td></tr><tr><td>early_stop_epoch</td><td>88</td></tr><tr><td>epoch</td><td>88</td></tr><tr><td>final_test/auc_score</td><td>0.75034</td></tr><tr><td>final_test/best_threshold</td><td>0.45</td></tr><tr><td>final_test/completed</td><td>1</td></tr><tr><td>+37</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">best_model_final_test</strong> at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/lokxz6u8' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed/runs/lokxz6u8</a><br> View project at: <a href='https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed' target=\"_blank\">https://wandb.ai/derelioglugokdeniz-bilkent-university/lstm-cybersecurity-fixed</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250902_114811-lokxz6u8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Model saved locally as: saved_model.pth\n",
      "✓ Hyperparameters saved locally as: saved_parameters.json\n",
      "\n",
      "============================================================\n",
      "FINAL MODEL TRAINING COMPLETED\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# FINAL LSTM MODEL TRAINING WITH SPECIFIC HYPERPARAMETERS\n",
    "# ==============================\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "import wandb\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Your specific hyperparameters\n",
    "final_params = {\n",
    "    'epochs': 100,\n",
    "    'sequence_length': 128,\n",
    "    'pos_weight': 0.88654,\n",
    "    'num_layers': 1,\n",
    "    'min_delta': 0.0000014444,\n",
    "    'lr': 0.00011505,\n",
    "    'l2_reg': 0.000034745,\n",
    "    'l1_reg': 0.000072167,\n",
    "    'early_stopping_patience': 35,\n",
    "    'hidden_dim': 4,\n",
    "    'gradient_clip': 4.21752,\n",
    "    'dropout': 0.29761,\n",
    "    'batch_size': 512,\n",
    "    'bidirectional': True,  # Default choice\n",
    "    'use_attention': True,  # Default choice\n",
    "    'wandb_project': \"lstm-cybersecurity-fixed\"\n",
    "}\n",
    "\n",
    "print(\"=== TRAINING FINAL MODEL WITH SPECIFIC HYPERPARAMETERS ===\")\n",
    "print(\"Hyperparameters:\")\n",
    "for k, v in final_params.items():\n",
    "    print(f\"   {k}: {v}\")\n",
    "\n",
    "# Prepare data with the specified sequence length\n",
    "def adjust_seq(x, target_len):\n",
    "    \"\"\"Adjust sequence length by padding or truncating\"\"\"\n",
    "    cur = x.shape[1]\n",
    "    if target_len == cur:\n",
    "        return x\n",
    "    if target_len < cur:\n",
    "        return x[:, -target_len:, :]  # Take last target_len timesteps\n",
    "    pad = target_len - cur\n",
    "    return np.pad(x, ((0,0), (pad,0), (0,0)), mode='constant')  # Pad at beginning\n",
    "\n",
    "# Adjust sequence lengths for all datasets\n",
    "seq_len = final_params['sequence_length']\n",
    "X_train_final = adjust_seq(X_train_seq, seq_len)\n",
    "X_val_final = adjust_seq(X_val_seq, seq_len)\n",
    "X_test_final = adjust_seq(X_test_seq, seq_len)\n",
    "\n",
    "print(f\"\\nData shapes after sequence adjustment:\")\n",
    "print(f\"Train: {X_train_final.shape}, Val: {X_val_final.shape}, Test: {X_test_final.shape}\")\n",
    "\n",
    "def preprocess_data_fixed(X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "    \"\"\"\n",
    "    FIXED: More conservative preprocessing to preserve signal\n",
    "    \"\"\"\n",
    "    print(\"=== FIXED Data Preprocessing ===\")\n",
    "    \n",
    "    # Convert to numpy if needed\n",
    "    X_train = np.array(X_train, dtype=np.float32)\n",
    "    X_val = np.array(X_val, dtype=np.float32)\n",
    "    X_test = np.array(X_test, dtype=np.float32)\n",
    "    y_train = np.array(y_train, dtype=np.float32)\n",
    "    y_val = np.array(y_val, dtype=np.float32)\n",
    "    y_test = np.array(y_test, dtype=np.float32)\n",
    "    \n",
    "    # Clean data\n",
    "    def clean_sequences(X, y):\n",
    "        valid_mask = ~(np.isnan(X).any(axis=(1,2)) | np.isinf(X).any(axis=(1,2)) | \n",
    "                      np.isnan(y) | np.isinf(y))\n",
    "        return X[valid_mask], y[valid_mask]\n",
    "    \n",
    "    X_train, y_train = clean_sequences(X_train, y_train)\n",
    "    X_val, y_val = clean_sequences(X_val, y_val)\n",
    "    X_test, y_test = clean_sequences(X_test, y_test)\n",
    "    \n",
    "    print(f\"After cleaning - Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "    \n",
    "    # CONSERVATIVE scaling - preserve more signal\n",
    "    original_shape_train = X_train.shape\n",
    "    original_shape_val = X_val.shape\n",
    "    original_shape_test = X_test.shape\n",
    "    \n",
    "    # Reshape to (samples * timesteps, features)\n",
    "    X_train_flat = X_train.reshape(-1, X_train.shape[-1])\n",
    "    X_val_flat = X_val.reshape(-1, X_val.shape[-1])\n",
    "    X_test_flat = X_test.reshape(-1, X_test.shape[-1])\n",
    "    \n",
    "    # Use StandardScaler (less aggressive than RobustScaler)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_flat = scaler.fit_transform(X_train_flat)\n",
    "    X_val_flat = scaler.transform(X_val_flat)\n",
    "    X_test_flat = scaler.transform(X_test_flat)\n",
    "    \n",
    "    pickle.dump(scaler, open('saved_scaler.pkl', 'wb'))\n",
    "\n",
    "    # Reshape back\n",
    "    X_train = X_train_flat.reshape(original_shape_train)\n",
    "    X_val = X_val_flat.reshape(original_shape_val)\n",
    "    X_test = X_test_flat.reshape(original_shape_test)\n",
    "    \n",
    "    # REMOVED: Aggressive clipping that can remove signal\n",
    "    # Only clip extreme outliers (>6 sigma)\n",
    "    clip_value = 6.0\n",
    "    X_train = np.clip(X_train, -clip_value, clip_value)\n",
    "    X_val = np.clip(X_val, -clip_value, clip_value)\n",
    "    X_test = np.clip(X_test, -clip_value, clip_value)\n",
    "    \n",
    "    # Ensure labels are properly shaped\n",
    "    y_train = y_train.reshape(-1)\n",
    "    y_val = y_val.reshape(-1)\n",
    "    y_test = y_test.reshape(-1)\n",
    "    \n",
    "    # More conservative pos_weight calculation\n",
    "    train_pos_ratio = np.mean(y_train)\n",
    "    val_pos_ratio = np.mean(y_val)\n",
    "    \n",
    "    print(f\"Class balance - Train: {train_pos_ratio:.3f}, Val: {val_pos_ratio:.3f}\")\n",
    "    \n",
    "    # Less aggressive pos_weight\n",
    "    if train_pos_ratio > 0:\n",
    "        pos_weight = np.sqrt((1 - train_pos_ratio) / train_pos_ratio)  # Square root for less aggressive weighting\n",
    "        pos_weight = np.clip(pos_weight, 0.8, 3.0)  # Much tighter bounds\n",
    "    else:\n",
    "        pos_weight = 1.0\n",
    "    \n",
    "    print(f\"Calculated pos_weight: {pos_weight:.2f}\")\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, scaler, pos_weight\n",
    "\n",
    "# Preprocess data\n",
    "X_train_c, X_val_c, X_test_c, y_train_c, y_val_c, y_test_c, scaler, calculated_pos_weight = preprocess_data_fixed(\n",
    "    X_train_final, X_val_final, X_test_final, y_train_seq, y_val_seq, y_test_seq\n",
    ")\n",
    "\n",
    "print(f\"Using specified pos_weight: {final_params['pos_weight']} (calculated: {calculated_pos_weight:.5f})\")\n",
    "\n",
    "# Create the model\n",
    "final_model = AdvancedLSTM(\n",
    "    input_dim=X_train_c.shape[2],\n",
    "    hidden_dim=final_params['hidden_dim'],\n",
    "    num_layers=final_params['num_layers'],\n",
    "    dropout=final_params['dropout'],\n",
    "    bidirectional=final_params['bidirectional'],\n",
    "    use_attention=final_params['use_attention']\n",
    ").to(device)\n",
    "\n",
    "print(f\"\\nModel created with {sum(p.numel() for p in final_model.parameters())} parameters\")\n",
    "\n",
    "# Initialize W&B with comprehensive config\n",
    "wandb_config = {\n",
    "    # Specified hyperparameters\n",
    "    'sequence_length': final_params['sequence_length'],\n",
    "    'pos_weight': final_params['pos_weight'],\n",
    "    'num_layers': final_params['num_layers'],\n",
    "    'lr': final_params['lr'],\n",
    "    'l2_reg': final_params['l2_reg'],\n",
    "    'l1_reg': final_params['l1_reg'],\n",
    "    'hidden_dim': final_params['hidden_dim'],\n",
    "    'gradient_clip': final_params['gradient_clip'],\n",
    "    'early_stopping_patience': final_params['early_stopping_patience'],\n",
    "    'dropout': final_params['dropout'],\n",
    "    'batch_size': final_params['batch_size'],\n",
    "    'bidirectional': final_params['bidirectional'],\n",
    "    'use_attention': final_params['use_attention'],\n",
    "    'epochs': final_params['epochs'],\n",
    "    \n",
    "    # Model and data info\n",
    "    'model_params': sum(p.numel() for p in final_model.parameters()),\n",
    "    'device': str(device),\n",
    "    'train_samples': len(X_train_c),\n",
    "    'val_samples': len(X_val_c),\n",
    "    'test_samples': len(X_test_c),\n",
    "    'input_features': X_train_c.shape[2],\n",
    "    'sequence_length_actual': X_train_c.shape[1],\n",
    "    'train_pos_ratio': float(np.mean(y_train_c)),\n",
    "    'val_pos_ratio': float(np.mean(y_val_c)),\n",
    "    'test_pos_ratio': float(np.mean(y_test_c)),\n",
    "    \n",
    "    # Training setup\n",
    "    'model_type': 'FixedLSTM',\n",
    "    'optimizer_type': 'Adam',\n",
    "    'loss_function': 'BCEWithLogitsLoss',\n",
    "    'scheduler_type': 'ReduceLROnPlateau',\n",
    "    'data_normalization': 'StandardScaler',\n",
    "    'training_mode': 'final_model_specific_hyperparams'\n",
    "}\n",
    "\n",
    "# Initialize W&B run\n",
    "run = wandb.init(\n",
    "    project=final_params[\"wandb_project\"],\n",
    "    name=\"best_model_final_test\",\n",
    "    config=wandb_config,\n",
    "    tags=[\"final_model\", \"specific_hyperparams\", \"best_model_final_test\"],\n",
    "    reinit=True\n",
    ")\n",
    "\n",
    "wandb.watch(final_model, log=\"all\", log_freq=100)\n",
    "\n",
    "print(f\"\\nStarting training - logging to W&B project: {final_params['wandb_project']}\")\n",
    "print(f\"Run name: best_model_final_test\")\n",
    "\n",
    "try:\n",
    "    # Prepare data loaders\n",
    "    X_train_c = X_train_c.astype(np.float32)\n",
    "    X_val_c = X_val_c.astype(np.float32)\n",
    "    y_train_c = y_train_c.astype(np.float32)\n",
    "    y_val_c = y_val_c.astype(np.float32)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        TensorDataset(torch.from_numpy(X_train_c), torch.from_numpy(y_train_c)),\n",
    "        batch_size=final_params['batch_size'], \n",
    "        shuffle=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        TensorDataset(torch.from_numpy(X_val_c), torch.from_numpy(y_val_c)),\n",
    "        batch_size=final_params['batch_size'], \n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # Setup optimizer and scheduler\n",
    "    optimizer = torch.optim.Adam(\n",
    "        final_model.parameters(),\n",
    "        lr=final_params['lr'],\n",
    "        weight_decay=final_params['l2_reg'],\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.7, patience=7, verbose=True, min_lr=5e-6\n",
    "    )\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss(\n",
    "        pos_weight=torch.tensor([final_params['pos_weight']], device=device)\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    train_losses, val_losses = [], []\n",
    "    train_f1s, val_f1s, val_aucs = [], [], []\n",
    "    best_val_f1, patience_counter = 0.0, 0\n",
    "    best_model_state = None\n",
    "    performance_history = []\n",
    "\n",
    "    final_model.to(device)\n",
    "\n",
    "    for epoch in range(final_params['epochs']):\n",
    "        # --- TRAINING PHASE ---\n",
    "        final_model.train()\n",
    "        epoch_loss = 0.0\n",
    "        train_preds_all, train_targets_all = [], []\n",
    "\n",
    "        for batch_idx, (Xb, yb) in enumerate(train_loader):\n",
    "            Xb = Xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = final_model(Xb).view(-1)\n",
    "            loss = criterion(logits, yb.view(-1))\n",
    "            \n",
    "            # Add L1 regularization if specified\n",
    "            if final_params['l1_reg'] > 0:\n",
    "                l1_penalty = sum(p.abs().sum() for p in final_model.parameters())\n",
    "                loss = loss + final_params['l1_reg'] * l1_penalty\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(final_model.parameters(), final_params['gradient_clip'])\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Collect predictions for F1 calculation\n",
    "            with torch.no_grad():\n",
    "                probs = torch.sigmoid(logits).detach().cpu().numpy().flatten()\n",
    "                preds = (probs >= 0.5).astype(int)\n",
    "                train_preds_all.extend(preds.tolist())\n",
    "                train_targets_all.extend(yb.detach().cpu().numpy().flatten().tolist())\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Calculate training F1\n",
    "        if len(set(train_targets_all)) > 1:\n",
    "            train_f1 = f1_score(train_targets_all, train_preds_all, zero_division=0)\n",
    "        else:\n",
    "            train_f1 = 0.0\n",
    "        train_f1s.append(train_f1)\n",
    "\n",
    "        # --- VALIDATION PHASE ---\n",
    "        final_model.eval()\n",
    "        val_loss_sum = 0.0\n",
    "        val_probs_all, val_targets_all = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for Xb, yb in val_loader:\n",
    "                Xb = Xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "                logits = final_model(Xb).view(-1)\n",
    "                val_loss_sum += criterion(logits, yb.view(-1)).item()\n",
    "                \n",
    "                probs = torch.sigmoid(logits).detach().cpu().numpy().flatten()\n",
    "                val_probs_all.extend(probs.tolist())\n",
    "                val_targets_all.extend(yb.detach().cpu().numpy().flatten().tolist())\n",
    "\n",
    "        avg_val_loss = val_loss_sum / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        # Calculate validation metrics\n",
    "        if len(np.unique(val_targets_all)) > 1:\n",
    "            val_auc = roc_auc_score(val_targets_all, val_probs_all)\n",
    "        else:\n",
    "            val_auc = 0.5\n",
    "        val_aucs.append(val_auc)\n",
    "\n",
    "        # Find best validation F1 across thresholds\n",
    "        best_val_f1_epoch = 0.0\n",
    "        vp = np.array(val_probs_all)\n",
    "        vt = np.array(val_targets_all)\n",
    "        for thresh in np.arange(0.3, 0.7, 0.05):\n",
    "            preds = (vp >= thresh).astype(int)\n",
    "            f1 = f1_score(vt, preds, zero_division=0)\n",
    "            if f1 > best_val_f1_epoch:\n",
    "                best_val_f1_epoch = f1\n",
    "        val_f1s.append(best_val_f1_epoch)\n",
    "\n",
    "        # Combined performance metric\n",
    "        current_perf = (train_f1 + best_val_f1_epoch + val_auc) / 3.0\n",
    "        performance_history.append(current_perf)\n",
    "\n",
    "        # Calculate metrics for logging\n",
    "        loss_gap = avg_val_loss - avg_train_loss\n",
    "        grad_norms = [p.grad.norm().item() for p in final_model.parameters() if p.grad is not None]\n",
    "        avg_grad_norm = np.mean(grad_norms) if grad_norms else 0.0\n",
    "        max_grad_norm = np.max(grad_norms) if grad_norms else 0.0\n",
    "\n",
    "        # Log to console\n",
    "        print(f\"Epoch {epoch+1:2d}/{final_params['epochs']} | \"\n",
    "              f\"Loss: T={avg_train_loss:.4f} V={avg_val_loss:.4f} Gap={loss_gap:.4f}\")\n",
    "        print(f\"         | F1: T={train_f1:.4f} V={best_val_f1_epoch:.4f} | \"\n",
    "              f\"AUC: {val_auc:.4f} | Perf: {current_perf:.4f}\")\n",
    "\n",
    "        # Comprehensive logging to W&B\n",
    "        log_dict = {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"val_loss\": avg_val_loss,\n",
    "            \"loss_gap\": loss_gap,\n",
    "            \"train_f1\": train_f1,\n",
    "            \"val_f1\": best_val_f1_epoch,\n",
    "            \"val_auc\": val_auc,\n",
    "            \"combined_performance\": current_perf,\n",
    "            \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "            \"patience_counter\": patience_counter,\n",
    "            \"gradients/avg_norm\": avg_grad_norm,\n",
    "            \"gradients/max_norm\": max_grad_norm,\n",
    "            \"training/overfitting_gap\": loss_gap,\n",
    "            \"model_state/weight_norm\": sum(p.norm().item() for p in final_model.parameters()),\n",
    "            \"predictions/val_prob_mean\": np.mean(val_probs_all),\n",
    "            \"predictions/val_prob_std\": np.std(val_probs_all),\n",
    "        }\n",
    "        wandb.log(log_dict)\n",
    "\n",
    "        # Early stopping and best model tracking\n",
    "        if best_val_f1_epoch > best_val_f1:\n",
    "            best_val_f1 = best_val_f1_epoch\n",
    "            best_model_state = {k: v.detach().cpu().clone() for k, v in final_model.state_dict().items()}\n",
    "            patience_counter = 0\n",
    "            print(f\"    ✓ New best F1: {best_val_f1:.4f}\")\n",
    "            wandb.log({\n",
    "                \"best_val_f1\": best_val_f1,\n",
    "                \"best_epoch\": epoch + 1,\n",
    "                \"best_model_saved\": 1\n",
    "            })\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        # Early stopping check\n",
    "        if patience_counter >= final_params['early_stopping_patience']:\n",
    "            print(f\"Early stopping after {patience_counter} epochs without improvement\")\n",
    "            wandb.log({\"early_stop\": 1, \"early_stop_epoch\": epoch + 1})\n",
    "            break\n",
    "\n",
    "        scheduler.step(current_perf)\n",
    "\n",
    "    # Restore best model\n",
    "    if best_model_state is not None:\n",
    "        final_model.load_state_dict({k: v.to(device) for k, v in best_model_state.items()})\n",
    "        print(f\"\\nRestored best model with validation F1: {best_val_f1:.4f}\")\n",
    "\n",
    "    # --- FINAL TEST EVALUATION ---\n",
    "    print(\"\\n=== FINAL TEST EVALUATION ===\")\n",
    "    final_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Get test predictions\n",
    "        X_test_tensor = torch.from_numpy(X_test_c.astype(np.float32)).to(device)\n",
    "        test_logits = final_model(X_test_tensor)\n",
    "        test_probs = torch.sigmoid(test_logits).detach().cpu().numpy().flatten()\n",
    "\n",
    "        # Get validation predictions for threshold optimization\n",
    "        X_val_tensor = torch.from_numpy(X_val_c.astype(np.float32)).to(device)\n",
    "        val_probs = torch.sigmoid(final_model(X_val_tensor)).detach().cpu().numpy().flatten()\n",
    "\n",
    "        # Find best threshold on validation set\n",
    "        best_threshold, best_val_f1_thresh = 0.5, 0.0\n",
    "        for th in np.arange(0.3, 0.7, 0.05):\n",
    "            vpred = (val_probs >= th).astype(int)\n",
    "            vf1 = f1_score(y_val_c, vpred, zero_division=0)\n",
    "            if vf1 > best_val_f1_thresh:\n",
    "                best_val_f1_thresh = vf1\n",
    "                best_threshold = float(th)\n",
    "\n",
    "        # Apply best threshold to test set\n",
    "        test_preds = (test_probs >= best_threshold).astype(int)\n",
    "        test_f1 = f1_score(y_test_c, test_preds, zero_division=0)\n",
    "        test_auc = roc_auc_score(y_test_c, test_probs) if len(np.unique(y_test_c)) > 1 else 0.5\n",
    "\n",
    "        print(f\"Final Test Results:\")\n",
    "        print(f\"   F1 Score: {test_f1:.4f}\")\n",
    "        print(f\"   AUC Score: {test_auc:.4f}\")\n",
    "        print(f\"   Best Threshold: {best_threshold:.3f}\")\n",
    "        print(f\"   Test Samples: {len(y_test_c)}\")\n",
    "\n",
    "        # Log final test results\n",
    "        final_test_results = {\n",
    "            \"final_test/f1_score\": test_f1,\n",
    "            \"final_test/auc_score\": test_auc,\n",
    "            \"final_test/best_threshold\": best_threshold,\n",
    "            \"final_test/test_samples\": len(y_test_c),\n",
    "            \"final_test/test_pos_ratio\": float(np.mean(y_test_c)),\n",
    "            \"final_test/completed\": 1,\n",
    "            \"summary/total_epochs_trained\": epoch + 1,\n",
    "            \"summary/best_val_f1\": best_val_f1,\n",
    "            \"summary/early_stopped\": int(patience_counter >= final_params['early_stopping_patience']),\n",
    "            \"summary/final_lr\": optimizer.param_groups[0]['lr']\n",
    "        }\n",
    "\n",
    "        # Include all hyperparameters in final results\n",
    "        for k, v in final_params.items():\n",
    "            if isinstance(v, (int, float, bool, str)):\n",
    "                final_test_results[f\"hyperparams/{k}\"] = v\n",
    "\n",
    "        wandb.log(final_test_results)\n",
    "\n",
    "        print(f\"\\n✓ Training completed and logged to W&B run: best_model_final_test\")\n",
    "        print(f\"✓ All results saved to W&B project: {final_params['wandb_project']}\")\n",
    "except Exception as e: \n",
    "        print(f\"Error during training: {e}\") \n",
    "        wandb.log({\"training_error\": str(e)}) \n",
    "        raise \n",
    "finally: \n",
    "    wandb.finish()\n",
    "    # --- SAVE MODEL AND HYPERPARAMETERS LOCALLY ---\n",
    "    # Save model\n",
    "    torch.save(final_model.state_dict(), \"saved_model.pth\")\n",
    "    print(f\"\\n✓ Model saved locally as: saved_model.pth\")\n",
    "\n",
    "    # Save hyperparameters\n",
    "    import json\n",
    "    with open(\"saved_parameters.json\", \"w\") as f:\n",
    "        json.dump(final_params, f, indent=4)\n",
    "    print(\"✓ Hyperparameters saved locally as: saved_parameters.json\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL MODEL TRAINING COMPLETED\")\n",
    "    print(\"=\"*60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
