{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16b1d2b6",
   "metadata": {},
   "source": [
    "# Making a DQN for our SDN system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8a161d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    def compute_reward(packet, delivered=True):\n",
    "        if packet.dropped:\n",
    "            return -1.0  # penalty for dropping benign packet\n",
    "        elif packet.suspicion >= upper_thresh:\n",
    "            return 1.0   # reward for correctly blocking malicious packet\n",
    "        else:\n",
    "            # reward = negative latency to encourage fast delivery\n",
    "            return -packet.finish_t + packet.spawn_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e1082d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNRouter:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        # define Q-network here\n",
    "        pass\n",
    "    \n",
    "    def select_path(self, topology_state, packet, mode=\"normal\"):\n",
    "        \"\"\"\n",
    "        mode = \"normal\" → choose optimal route (minimize latency, load balance, etc.)\n",
    "        mode = \"quarantine\" → choose path to isolation node/subnet\n",
    "        \"\"\"\n",
    "        state = self.encode_state(topology_state, packet)\n",
    "        action = self.q_network(state).argmax().item()\n",
    "\n",
    "        # decode action into actual path\n",
    "        if mode == \"normal\":\n",
    "            return self.decode_action_to_path(action, quarantine=False)\n",
    "        else:\n",
    "            return self.decode_action_to_path(action, quarantine=True)\n",
    "\n",
    "    def encode_state(self, topology_state, packet):\n",
    "        # turn topology state + packet features into NN input vector\n",
    "        pass\n",
    "\n",
    "    def decode_action_to_path(self, action, quarantine=False):\n",
    "        # map action index → path through topology\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073e4b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "\n",
    "# ============================\n",
    "# Replay Buffer\n",
    "# ============================\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.array, zip(*batch))\n",
    "        return (\n",
    "            torch.FloatTensor(state),\n",
    "            torch.LongTensor(action),\n",
    "            torch.FloatTensor(reward),\n",
    "            torch.FloatTensor(next_state),\n",
    "            torch.FloatTensor(done)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# ============================\n",
    "# DQN Model\n",
    "# ============================\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=128):\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# ============================\n",
    "# Agent\n",
    "# ============================\n",
    "class DQNAgent:\n",
    "    def __init__(self, input_dim, output_dim, lr=1e-3, gamma=0.99, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.q_network = DQN(input_dim, output_dim).to(self.device)\n",
    "        self.target_network = DQN(input_dim, output_dim).to(self.device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = eps_start\n",
    "        self.eps_end = eps_end\n",
    "        self.eps_decay = eps_decay\n",
    "        self.memory = ReplayBuffer()\n",
    "        self.steps_done = 0\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randrange(self.output_dim)\n",
    "        else:\n",
    "            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                q_values = self.q_network(state)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "    def optimize_model(self, batch_size=64):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(batch_size)\n",
    "        states, actions, rewards, next_states, dones = (\n",
    "            states.to(self.device), actions.to(self.device), rewards.to(self.device),\n",
    "            next_states.to(self.device), dones.to(self.device)\n",
    "        )\n",
    "\n",
    "        q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        next_q_values = self.target_network(next_states).max(1)[0]\n",
    "        expected_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "\n",
    "        loss = nn.MSELoss()(q_values, expected_q_values.detach())\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Epsilon decay\n",
    "        self.epsilon = max(self.eps_end, self.epsilon * self.eps_decay)\n",
    "\n",
    "    def update_target(self):\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "# ============================\n",
    "# CSV Stream Environment\n",
    "# ============================\n",
    "class CSVStreamEnvironment:\n",
    "    def __init__(self, csv_path):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.index = 0\n",
    "        self.max_index = len(self.data)\n",
    "\n",
    "    def reset(self):\n",
    "        self.index = 0\n",
    "        return self._get_state()\n",
    "\n",
    "    def step(self, action):\n",
    "        # Example reward logic:\n",
    "        # 0 = route, 1 = quarantine, 2 = drop\n",
    "        current_state = self._get_state()\n",
    "        suspicion_score = current_state[-1]  # assuming last feature = suspicion\n",
    "\n",
    "        if action == 0:  # route\n",
    "            reward = 1.0 if suspicion_score < 0.5 else -1.0\n",
    "        elif action == 1:  # quarantine\n",
    "            reward = 0.5 if suspicion_score >= 0.5 else -0.5\n",
    "        else:  # drop\n",
    "            reward = -0.2 if suspicion_score < 0.5 else 0.5\n",
    "\n",
    "        self.index += 1\n",
    "        done = self.index >= self.max_index\n",
    "        next_state = self._get_state() if not done else np.zeros_like(current_state)\n",
    "\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def _get_state(self):\n",
    "        row = self.data.iloc[self.index].values\n",
    "        return row.astype(np.float32)\n",
    "\n",
    "# ============================\n",
    "# Glue Training Loop\n",
    "# ============================\n",
    "def train_dqn(csv_path, episodes=10, batch_size=64, target_update=10):\n",
    "    env = CSVStreamEnvironment(csv_path)\n",
    "    state_dim = env.reset().shape[0]\n",
    "    action_dim = 3  # route, quarantine, drop\n",
    "    agent = DQNAgent(state_dim, action_dim)\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.memory.push(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            agent.optimize_model(batch_size)\n",
    "\n",
    "        if ep % target_update == 0:\n",
    "            agent.update_target()\n",
    "\n",
    "        print(f\"Episode {ep+1}/{episodes}, Total Reward: {total_reward:.2f}, Epsilon: {agent.epsilon:.3f}\")\n",
    "\n",
    "    torch.save(agent.q_network.state_dict(), \"dqn_model.pth\")\n",
    "    print(\"Model saved as dqn_model.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
