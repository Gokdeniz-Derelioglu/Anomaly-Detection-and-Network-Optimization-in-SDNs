{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df655de2",
   "metadata": {},
   "source": [
    "# Hopefully the FINAL notebook I'll need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b528c5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151b693f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def merge_csv_files(folder_path, output_file=\"merged.csv\"):\n",
    "    csv_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "    dfs = []\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)\n",
    "        df.columns = [col.strip() for col in df.columns]\n",
    "        dfs.append(df)\n",
    "    merged_df = pd.concat(dfs, ignore_index=True)\n",
    "    merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "# Use correct relative path for CICIDS2017 folder\n",
    "merge_csv_files(\"../../CICIDS2017\", \"merged.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c96479",
   "metadata": {},
   "source": [
    "need to get this part to only include the top 50 features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbb6e12",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# ----------------------------\n",
    "# SETTINGS\n",
    "# ----------------------------\n",
    "file_path = \"new_merged.csv\"\n",
    "fraction = 0.20     # % of data to sample overall (keeps earliest fraction, preserves timeline)\n",
    "seq_length = 10     # default LSTM sequence length (adjust as needed)\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Load in chunks (preserve order)\n",
    "# ----------------------------\n",
    "chunks = []\n",
    "chunk_size = 10_000\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "    # Strip whitespace from column names\n",
    "    chunk.columns = chunk.columns.str.strip()\n",
    "\n",
    "    # Downcast numerics\n",
    "    for col in chunk.select_dtypes(include=['int', 'float']).columns:\n",
    "        if pd.api.types.is_integer_dtype(chunk[col]):\n",
    "            chunk[col] = pd.to_numeric(chunk[col], downcast='integer')\n",
    "        else:\n",
    "            chunk[col] = pd.to_numeric(chunk[col], downcast='float')\n",
    "\n",
    "    # Replace NaN/Inf inside chunk\n",
    "    chunk = chunk.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "    chunks.append(chunk)\n",
    "\n",
    "data = pd.concat(chunks, ignore_index=True)\n",
    "del chunks  # free memory\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Label to binary\n",
    "# ----------------------------\n",
    "if \"Label\" not in data.columns:\n",
    "    raise KeyError(\"Expected a 'Label' column.\")\n",
    "\n",
    "data[\"Label\"] = data[\"Label\"].apply(\n",
    "    lambda x: 0 if str(x).strip().upper() == \"BENIGN\" else 1\n",
    ").astype(np.int32)\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Optional overall fraction (preserves order)\n",
    "# ----------------------------\n",
    "if fraction < 1.0:\n",
    "    n_sample = max(1, int(len(data) * fraction))\n",
    "    data = data.iloc[:n_sample].copy()\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Time-aware benign downsampling (keeps chronology)\n",
    "# ----------------------------\n",
    "def time_aware_downsample_benign(df, label_col=\"Label\", max_benign_ratio=0.55):\n",
    "    keep_mask = np.zeros(len(df), dtype=bool)\n",
    "    benign_kept = 0\n",
    "    attack_kept = 0\n",
    "\n",
    "    label = df[label_col].to_numpy()\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        if label[i] == 1:\n",
    "            keep_mask[i] = True\n",
    "            attack_kept += 1\n",
    "        else:\n",
    "            proposed_benign = benign_kept + 1\n",
    "            proposed_total  = proposed_benign + attack_kept\n",
    "            if proposed_total == 0:\n",
    "                keep_mask[i] = True\n",
    "                benign_kept += 1\n",
    "            else:\n",
    "                if (proposed_benign / proposed_total) <= max_benign_ratio:\n",
    "                    keep_mask[i] = True\n",
    "                    benign_kept += 1\n",
    "\n",
    "    kept = df.loc[keep_mask]\n",
    "    return kept\n",
    "\n",
    "# Apply chronology-safe downsampling\n",
    "data = time_aware_downsample_benign(data, label_col=\"Label\", max_benign_ratio=0.55).reset_index(drop=True)\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Cap extreme values (exclude the label)\n",
    "# ----------------------------\n",
    "numeric_cols = [c for c in data.select_dtypes(include=[np.number]).columns if c != \"Label\"]\n",
    "for col in numeric_cols:\n",
    "    cap_value = data[col].quantile(0.999)\n",
    "    data[col] = np.clip(data[col], a_min=None, a_max=cap_value)\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Feature selection: keep only TOP 50 features\n",
    "# ----------------------------\n",
    "X = data.drop(columns=[\"Label\"])\n",
    "y = data[\"Label\"]\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    class_weight=\"balanced\"\n",
    ")\n",
    "rf.fit(X, y)\n",
    "\n",
    "importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "top_features = importances.sort_values(ascending=False).head(50).index.tolist()\n",
    "\n",
    "print(\"\\nTop 50 selected features:\")\n",
    "print(top_features)\n",
    "\n",
    "data = data[top_features + [\"Label\"]]\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Dtypes for ML (features float32, label int32)\n",
    "# ----------------------------\n",
    "for col in top_features:\n",
    "    data[col] = data[col].astype(np.float32)\n",
    "data[\"Label\"] = data[\"Label\"].astype(np.int32)\n",
    "\n",
    "# ----------------------------\n",
    "# Save final reduced dataset\n",
    "# ----------------------------\n",
    "data.to_csv(\"new_merged.csv\", index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# Final assignment for downstream code\n",
    "# ----------------------------\n",
    "datadf = data.copy()\n",
    "ftnames = [c.strip() for c in datadf.columns if c.strip() != \"Label\"]\n",
    "\n",
    "print(f\"\\nFinal shape: {datadf.shape}\")\n",
    "print(f\"Number of features: {len(ftnames)}\")\n",
    "print(\"First few feature names:\", ftnames[:10])\n",
    "print(\"Label distribution:\\n\", datadf['Label'].value_counts())\n",
    "\n",
    "# ======================================================================\n",
    "# Chronology-safe sequence utilities (NO shuffling, NO stratify anywhere)\n",
    "# ======================================================================\n",
    "\n",
    "def create_sequences(X, y, sequence_length=10, label_strategy='last'):\n",
    "    \"\"\"\n",
    "    Create overlapping sequences without breaking chronology.\n",
    "    label_strategy: 'last' | 'majority' | 'any_attack'\n",
    "    \"\"\"\n",
    "    if len(X) < sequence_length:\n",
    "        sequence_length = len(X)\n",
    "        print(f\"[create_sequences] Adjusted sequence_length to {sequence_length}\")\n",
    "\n",
    "    X_sequences, y_sequences = [], []\n",
    "    for i in range(len(X) - sequence_length + 1):\n",
    "        X_seq = X[i:i+sequence_length]\n",
    "        y_seq = y[i:i+sequence_length]\n",
    "        X_sequences.append(X_seq)\n",
    "\n",
    "        if label_strategy == 'last':\n",
    "            y_sequences.append(y_seq[-1])\n",
    "        elif label_strategy == 'majority':\n",
    "            y_sequences.append(1 if np.sum(y_seq) > (len(y_seq) // 2) else 0)\n",
    "        elif label_strategy == 'any_attack':\n",
    "            y_sequences.append(1 if (y_seq == 1).any() else 0)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown label_strategy: {label_strategy}\")\n",
    "\n",
    "    X_sequences = np.asarray(X_sequences, dtype=np.float32)\n",
    "    y_sequences = np.asarray(y_sequences, dtype=np.float32)\n",
    "\n",
    "    return X_sequences, y_sequences\n",
    "\n",
    "def chrono_split_train_val_test(df, label_col=\"Label\", train_ratio=0.7, val_ratio=0.15):\n",
    "    \"\"\"\n",
    "    Chronological split on rows (NO shuffle). Test gets the tail.\n",
    "    \"\"\"\n",
    "    n = len(df)\n",
    "    n_train = int(n * train_ratio)\n",
    "    n_val = int(n * val_ratio)\n",
    "    n_test = n - n_train - n_val\n",
    "\n",
    "    train_df = df.iloc[:n_train]\n",
    "    val_df   = df.iloc[n_train:n_train+n_val]\n",
    "    test_df  = df.iloc[n_train+n_val:]\n",
    "\n",
    "    def xy(d):\n",
    "        X = d.drop(columns=[label_col]).to_numpy(dtype=np.float32)\n",
    "        y = d[label_col].to_numpy(dtype=np.int32)\n",
    "        return X, y\n",
    "\n",
    "    return xy(train_df), xy(val_df), xy(test_df)\n",
    "\n",
    "def build_chrono_sequences(df, label_col=\"Label\", sequence_length=10, label_strategy='last'):\n",
    "    \"\"\"\n",
    "    Split chronologically on raw rows, then build sequences inside each split so\n",
    "    no sequence crosses split boundaries.\n",
    "    \"\"\"\n",
    "    (X_tr, y_tr), (X_va, y_va), (X_te, y_te) = chrono_split_train_val_test(df, label_col=label_col)\n",
    "\n",
    "    X_train_seq, y_train_seq = create_sequences(X_tr, y_tr, sequence_length, label_strategy)\n",
    "    X_val_seq,   y_val_seq   = create_sequences(X_va, y_va, sequence_length, label_strategy)\n",
    "    X_test_seq,  y_test_seq  = create_sequences(X_te, y_te, sequence_length, label_strategy)\n",
    "\n",
    "    # Quick sanity print\n",
    "    def _dist(y):\n",
    "        c = Counter(y.astype(int).tolist())\n",
    "        total = len(y)\n",
    "        if total == 0:\n",
    "            return {}\n",
    "        return {k: f\"{v} ({v/total:.2%})\" for k, v in sorted(c.items())}\n",
    "\n",
    "    print(\"\\n=== Chronological split (sequence-level) ===\")\n",
    "    print(f\"Train seq: {X_train_seq.shape}, dist: { _dist(y_train_seq) }\")\n",
    "    print(f\"Val   seq: {X_val_seq.shape}, dist: { _dist(y_val_seq) }\")\n",
    "    print(f\"Test  seq: {X_test_seq.shape}, dist: { _dist(y_test_seq) }\")\n",
    "\n",
    "    return X_train_seq, y_train_seq, X_val_seq, y_val_seq, X_test_seq, y_test_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bb2633",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, average_precision_score, roc_curve, precision_recall_curve,\n",
    "    classification_report, accuracy_score\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import optuna\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b267374b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def print_data_info(X, y, stage_name):\n",
    "    \"\"\"Print comprehensive data information\"\"\"\n",
    "    print(f\"\\n=== {stage_name} Data Info ===\")\n",
    "    print(f\"Feature matrix shape: {X.shape}\")\n",
    "    print(f\"Labels shape: {y.shape}\")\n",
    "    print(f\"Feature data type: {X.dtype}\")\n",
    "    print(f\"Label data type: {y.dtype}\")\n",
    "    \n",
    "    # Check for NaN/inf values\n",
    "    nan_count = np.isnan(X).sum()\n",
    "    inf_count = np.isinf(X).sum()\n",
    "    print(f\"NaN values in features: {nan_count}\")\n",
    "    print(f\"Inf values in features: {inf_count}\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"Feature matrix - Min: {X.min():.4f}, Max: {X.max():.4f}, Mean: {X.mean():.4f}\")\n",
    "    \n",
    "    # Label distribution\n",
    "    unique_labels, counts = np.unique(y, return_counts=True)\n",
    "    print(f\"Label distribution:\")\n",
    "    for label, count in zip(unique_labels, counts):\n",
    "        percentage = (count / len(y)) * 100\n",
    "        print(f\"  Class {int(label)}: {count} samples ({percentage:.2f}%)\")\n",
    "    \n",
    "    # Sample some labels\n",
    "    print(f\"First 20 labels: {y[:20]}\")\n",
    "    print(f\"Last 20 labels: {y[-20:]}\")\n",
    "\n",
    "def validate_data_splits(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    \"\"\"Validate that data splits preserve class distribution\"\"\"\n",
    "    print(\"\\n=== Data Split Validation ===\")\n",
    "    \n",
    "    # Check shapes\n",
    "    print(f\"Original total samples: {len(X_train) + len(X_val) + len(X_test)}\")\n",
    "    print(f\"Train: {X_train.shape[0]} ({X_train.shape[0]/(len(X_train) + len(X_val) + len(X_test))*100:.1f}%)\")\n",
    "    print(f\"Val: {X_val.shape[0]} ({X_val.shape[0]/(len(X_train) + len(X_val) + len(X_test))*100:.1f}%)\")\n",
    "    print(f\"Test: {X_test.shape[0]} ({X_test.shape[0]/(len(X_train) + len(X_val) + len(X_test))*100:.1f}%)\")\n",
    "    \n",
    "    # Check class distributions\n",
    "    datasets = [(\"Train\", y_train), (\"Val\", y_val), (\"Test\", y_test)]\n",
    "    \n",
    "    print(\"\\nClass distributions across splits:\")\n",
    "    for name, y_split in datasets:\n",
    "        unique_labels, counts = np.unique(y_split, return_counts=True)\n",
    "        print(f\"{name}:\")\n",
    "        for label, count in zip(unique_labels, counts):\n",
    "            percentage = (count / len(y_split)) * 100\n",
    "            print(f\"  Class {int(label)}: {count} samples ({percentage:.2f}%)\")\n",
    "    \n",
    "    # Check for data leakage indicators\n",
    "    print(f\"\\nFeature statistics consistency check:\")\n",
    "    print(f\"Train mean: {X_train.mean():.6f}\")\n",
    "    print(f\"Val mean: {X_val.mean():.6f}\")\n",
    "    print(f\"Test mean: {X_test.mean():.6f}\")\n",
    "\n",
    "def check_preprocessing_integrity(X_before, y_before, X_after, y_after, stage_name):\n",
    "    \"\"\"Check if preprocessing preserved data integrity\"\"\"\n",
    "    print(f\"\\n=== {stage_name} Preprocessing Integrity Check ===\")\n",
    "    \n",
    "    # Shape consistency\n",
    "    assert X_before.shape[0] == X_after.shape[0], f\"Sample count mismatch: {X_before.shape[0]} vs {X_after.shape[0]}\"\n",
    "    assert len(y_before) == len(y_after), f\"Label count mismatch: {len(y_before)} vs {len(y_after)}\"\n",
    "    print(\"✓ Sample counts preserved\")\n",
    "    \n",
    "    # Label consistency\n",
    "    assert np.array_equal(y_before, y_after), \"Labels were modified during preprocessing!\"\n",
    "    print(\"✓ Labels preserved\")\n",
    "    \n",
    "    # Feature scaling check\n",
    "    if stage_name == \"Scaling\":\n",
    "        print(f\"Before scaling - Min: {X_before.min():.4f}, Max: {X_before.max():.4f}\")\n",
    "        print(f\"After scaling - Min: {X_after.min():.4f}, Max: {X_after.max():.4f}\")\n",
    "        print(f\"After scaling - Mean: {X_after.mean():.6f}, Std: {X_after.std():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ed8c7a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class BaselineEvaluator:\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "    \n",
    "    def add_dummy_classifier(self, X_train, y_train):\n",
    "        \"\"\"Add majority class predictor\"\"\"\n",
    "        print(\"Adding Majority Class Predictor...\")\n",
    "        self.models['majority_class'] = DummyClassifier(strategy='most_frequent', random_state=42)\n",
    "        self.models['majority_class'].fit(X_train, y_train)\n",
    "    \n",
    "    def add_logistic_regression(self, X_train, y_train):\n",
    "        \"\"\"Add logistic regression baseline\"\"\"\n",
    "        print(\"Adding Logistic Regression...\")\n",
    "        self.models['logistic_regression'] = LogisticRegression(\n",
    "            random_state=42, \n",
    "            max_iter=1000,\n",
    "            class_weight='balanced'\n",
    "        )\n",
    "        self.models['logistic_regression'].fit(X_train, y_train)\n",
    "    \n",
    "    def add_random_forest(self, X_train, y_train):\n",
    "        \"\"\"Add random forest baseline\"\"\"\n",
    "        print(\"Adding Random Forest...\")\n",
    "        self.models['random_forest'] = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            random_state=42,\n",
    "            class_weight='balanced',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        self.models['random_forest'].fit(X_train, y_train)\n",
    "    \n",
    "    def add_knn(self, X_train, y_train):\n",
    "        \"\"\"Add KNN baseline\"\"\"\n",
    "        print(\"Adding KNN...\")\n",
    "        # Use smaller sample for KNN if dataset is too large\n",
    "        if len(X_train) > 10000:\n",
    "            print(f\"Using subset of {min(5000, len(X_train))} samples for KNN training...\")\n",
    "            indices = np.random.choice(len(X_train), min(5000, len(X_train)), replace=False)\n",
    "            X_train_knn = X_train[indices]\n",
    "            y_train_knn = y_train[indices]\n",
    "        else:\n",
    "            X_train_knn = X_train\n",
    "            y_train_knn = y_train\n",
    "            \n",
    "        self.models['knn'] = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
    "        self.models['knn'].fit(X_train_knn, y_train_knn)\n",
    "    \n",
    "    def evaluate_all(self, X_test, y_test, model_name_prefix=\"Baseline\"):\n",
    "        \"\"\"Evaluate all baseline models\"\"\"\n",
    "        print(f\"\\n=== {model_name_prefix} Model Evaluation ===\")\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            print(f\"\\nEvaluating {name.replace('_', ' ').title()}...\")\n",
    "            \n",
    "            # Predictions\n",
    "            try:\n",
    "                y_pred = model.predict(X_test)\n",
    "                y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else y_pred\n",
    "            except Exception as e:\n",
    "                print(f\"Error with {name}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "            recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "            f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "            \n",
    "            # AUC calculation\n",
    "            try:\n",
    "                if len(np.unique(y_test)) > 1:\n",
    "                    auc = roc_auc_score(y_test, y_prob)\n",
    "                    pr_auc = average_precision_score(y_test, y_prob)\n",
    "                else:\n",
    "                    auc = pr_auc = 0.5\n",
    "            except:\n",
    "                auc = pr_auc = 0.5\n",
    "            \n",
    "            # Store results\n",
    "            self.results[name] = {\n",
    "                'accuracy': accuracy,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'auc': auc,\n",
    "                'pr_auc': pr_auc,\n",
    "                'y_pred': y_pred,\n",
    "                'y_prob': y_prob\n",
    "            }\n",
    "            \n",
    "            # Print results\n",
    "            print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "            print(f\"  Precision: {precision:.4f}\")\n",
    "            print(f\"  Recall:    {recall:.4f}\")\n",
    "            print(f\"  F1 Score:  {f1:.4f}\")\n",
    "            print(f\"  AUC:       {auc:.4f}\")\n",
    "            print(f\"  PR-AUC:    {pr_auc:.4f}\")\n",
    "    \n",
    "    def plot_confusion_matrices(self, y_test):\n",
    "        \"\"\"Plot confusion matrices for all models with robust error handling\"\"\"\n",
    "        n_models = len(self.results)  # Use results, not models\n",
    "        if n_models == 0:\n",
    "            print(\"No model results available for confusion matrix plotting\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Plotting confusion matrices for {n_models} models...\")\n",
    "        \n",
    "        # Try the subplot approach first\n",
    "        try:\n",
    "            cols = min(3, n_models)\n",
    "            rows = (n_models + cols - 1) // cols\n",
    "            \n",
    "            fig, axes = plt.subplots(rows, cols, figsize=(5*cols, 4*rows))\n",
    "            \n",
    "            # Convert to 2D array for consistent indexing\n",
    "            if n_models == 1:\n",
    "                axes = np.array([[axes]])\n",
    "            elif rows == 1:\n",
    "                axes = axes.reshape(1, -1)\n",
    "            elif cols == 1:\n",
    "                axes = axes.reshape(-1, 1)\n",
    "            \n",
    "            plot_idx = 0\n",
    "            for name, results in self.results.items():\n",
    "                try:\n",
    "                    row = plot_idx // cols\n",
    "                    col = plot_idx % cols\n",
    "                    \n",
    "                    ax = axes[row, col]\n",
    "                    \n",
    "                    cm = confusion_matrix(y_test, results['y_pred'])\n",
    "                    sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap='Blues')\n",
    "                    ax.set_title(f'{name.replace(\"_\", \" \").title()}\\nF1: {results[\"f1\"]:.3f}, AUC: {results[\"auc\"]:.3f}')\n",
    "                    ax.set_xlabel('Predicted')\n",
    "                    ax.set_ylabel('Actual')\n",
    "                    \n",
    "                    plot_idx += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error plotting confusion matrix for {name}: {e}\")\n",
    "                    plot_idx += 1\n",
    "                    continue\n",
    "            \n",
    "            # Hide empty subplots\n",
    "            for idx in range(n_models, rows * cols):\n",
    "                try:\n",
    "                    row = idx // cols\n",
    "                    col = idx % cols\n",
    "                    axes[row, col].axis('off')\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            print(\"✅ Confusion matrices plotted successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Subplot approach failed: {e}\")\n",
    "            print(\"Falling back to individual plots...\")\n",
    "            \n",
    "            # Fallback: individual plots\n",
    "            try:\n",
    "                for name, results in self.results.items():\n",
    "                    try:\n",
    "                        plt.figure(figsize=(6, 4))\n",
    "                        cm = confusion_matrix(y_test, results['y_pred'])\n",
    "                        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "                        plt.title(f'{name.replace(\"_\", \" \").title()}\\nF1: {results[\"f1\"]:.3f}, AUC: {results[\"auc\"]:.3f}')\n",
    "                        plt.xlabel('Predicted')\n",
    "                        plt.ylabel('Actual')\n",
    "                        plt.tight_layout()\n",
    "                        plt.show()\n",
    "                        print(f\"✅ Confusion matrix for {name} plotted\")\n",
    "                    except Exception as e2:\n",
    "                        print(f\"❌ Failed to plot confusion matrix for {name}: {e2}\")\n",
    "                        \n",
    "            except Exception as e3:\n",
    "                print(f\"❌ All plotting approaches failed: {e3}\")\n",
    "                print(\"Skipping confusion matrix plots...\")\n",
    "    \n",
    "    def get_results_summary(self):\n",
    "        \"\"\"Get summary of all baseline results\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No results available for summary\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "        summary_data = []\n",
    "        for name, results in self.results.items():\n",
    "            summary_data.append({\n",
    "                'Model': name.replace('_', ' ').title(),\n",
    "                'Accuracy': results['accuracy'],\n",
    "                'Precision': results['precision'],\n",
    "                'Recall': results['recall'],\n",
    "                'F1': results['f1'],\n",
    "                'AUC': results['auc'],\n",
    "                'PR-AUC': results['pr_auc']\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(summary_data).round(4)\n",
    "    \n",
    "    # Replace your plot_confusion_matrices method with this simple version\n",
    "    def plot_confusion_matrices(self, y_test):\n",
    "        \"\"\"Plot confusion matrices for all models - Simple version\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No results available for plotting\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Plotting confusion matrices for {len(self.results)} models...\")\n",
    "        \n",
    "        # Use simple individual plots to avoid matplotlib axes issues\n",
    "        for name, results in self.results.items():\n",
    "            try:\n",
    "                print(f\"Plotting confusion matrix for {name}...\")\n",
    "                \n",
    "                # Create a new figure for each model\n",
    "                plt.figure(figsize=(6, 5))\n",
    "                \n",
    "                # Calculate confusion matrix\n",
    "                cm = confusion_matrix(y_test, results['y_pred'])\n",
    "                \n",
    "                # Use matplotlib directly instead of seaborn to avoid axes issues\n",
    "                plt.imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "                plt.colorbar()\n",
    "                \n",
    "                # Add text annotations\n",
    "                for i in range(cm.shape[0]):\n",
    "                    for j in range(cm.shape[1]):\n",
    "                        plt.text(j, i, str(cm[i, j]), \n",
    "                                ha='center', va='center', \n",
    "                                color='white' if cm[i, j] > cm.max() / 2 else 'black',\n",
    "                                fontsize=14, fontweight='bold')\n",
    "                \n",
    "                # Labels and title\n",
    "                plt.title(f'{name.replace(\"_\", \" \").title()}\\nF1: {results[\"f1\"]:.3f}, AUC: {results[\"auc\"]:.3f}', \n",
    "                        fontsize=12, pad=20)\n",
    "                plt.xlabel('Predicted Label', fontsize=11)\n",
    "                plt.ylabel('True Label', fontsize=11)\n",
    "                \n",
    "                # Set tick labels\n",
    "                tick_labels = ['Benign', 'Attack']\n",
    "                plt.xticks(range(len(tick_labels)), tick_labels)\n",
    "                plt.yticks(range(len(tick_labels)), tick_labels)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error plotting confusion matrix for {name}: {e}\")\n",
    "                # Still try to show basic metrics\n",
    "                try:\n",
    "                    cm = confusion_matrix(y_test, results['y_pred'])\n",
    "                    print(f\"  Confusion Matrix for {name}:\")\n",
    "                    print(f\"    {cm}\")\n",
    "                except:\n",
    "                    print(f\"  Could not generate any visualization for {name}\")\n",
    "        \n",
    "        print(\"✅ Confusion matrix plotting completed\")\n",
    "    \n",
    "    def get_results_summary(self):\n",
    "        \"\"\"Get summary of all baseline results\"\"\"\n",
    "        if not self.results:\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "        summary_data = []\n",
    "        for name, results in self.results.items():\n",
    "            summary_data.append({\n",
    "                'Model': name.replace('_', ' ').title(),\n",
    "                'Accuracy': results['accuracy'],\n",
    "                'Precision': results['precision'],\n",
    "                'Recall': results['recall'],\n",
    "                'F1': results['f1'],\n",
    "                'AUC': results['auc'],\n",
    "                'PR-AUC': results['pr_auc']\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(summary_data).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62df0cc0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7071be",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# ==========================================================\n",
    "# Chronology-safe prepare_lstm_sequences_fixed\n",
    "# ==========================================================\n",
    "def prepare_lstm_sequences_fixed(X, y, sequence_length=SEQ_LENGTH, label_strategy='majority'):\n",
    "    \"\"\"\n",
    "    Convert tabular data to sequences for LSTM training with better label handling.\n",
    "    Chronology preserved (NO shuffling).\n",
    "    \"\"\"\n",
    "    print(f\"Creating sequences of length {sequence_length} with {label_strategy} labeling...\")\n",
    "\n",
    "    if len(X) < sequence_length:\n",
    "        sequence_length = len(X)\n",
    "        print(f\"Adjusted sequence length to {sequence_length} due to limited data\")\n",
    "\n",
    "    X_sequences = []\n",
    "    y_sequences = []\n",
    "\n",
    "    for i in range(len(X) - sequence_length + 1):\n",
    "        X_seq = X[i:i + sequence_length]\n",
    "        y_seq = y[i:i + sequence_length]\n",
    "        X_sequences.append(X_seq)\n",
    "\n",
    "        if label_strategy == 'last':\n",
    "            y_sequences.append(y_seq[-1])\n",
    "        elif label_strategy == 'majority':\n",
    "            y_sequences.append(1 if np.sum(y_seq) > len(y_seq) // 2 else 0)\n",
    "        elif label_strategy == 'any_attack':\n",
    "            y_sequences.append(1 if np.any(y_seq == 1) else 0)\n",
    "\n",
    "    X_sequences = np.array(X_sequences, dtype=np.float32)\n",
    "    y_sequences = np.array(y_sequences, dtype=np.int32)\n",
    "\n",
    "    print(f\"Created {len(X_sequences)} sequences\")\n",
    "    print(f\"Sequence shape: {X_sequences.shape}\")\n",
    "\n",
    "    # Distribution debug\n",
    "    original_dist = Counter(y)\n",
    "    sequence_dist = Counter(y_sequences)\n",
    "    print(f\"Original distribution: {dict(original_dist)}\")\n",
    "    print(f\"Sequence distribution: {dict(sequence_dist)}\")\n",
    "\n",
    "    return X_sequences, y_sequences\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# Chronology-safe create_stratified_sequences\n",
    "# (renamed internally but kept external name)\n",
    "# ==========================================================\n",
    "def create_stratified_sequences(X, y, sequence_length=SEQ_LENGTH, test_size=0.4, val_split=0.5, random_state=42):\n",
    "    \"\"\"\n",
    "    Chronology-safe split of sequences (replaces stratified split).\n",
    "    \"\"\"\n",
    "    print(\"Creating chronology-safe sequences (no stratify)...\")\n",
    "\n",
    "    # Step 1: build sequences\n",
    "    X_seq, y_seq = prepare_lstm_sequences_fixed(X, y, sequence_length, label_strategy='majority')\n",
    "\n",
    "    # Step 2: split chronologically\n",
    "    n_total = len(X_seq)\n",
    "    n_test = int(n_total * test_size)\n",
    "    n_val = int((n_total - n_test) * val_split)\n",
    "\n",
    "    train_end = n_total - n_test - n_val\n",
    "    val_end   = n_total - n_test\n",
    "\n",
    "    X_train_seq, y_train_seq = X_seq[:train_end], y_seq[:train_end]\n",
    "    X_val_seq,   y_val_seq   = X_seq[train_end:val_end], y_seq[train_end:val_end]\n",
    "    X_test_seq,  y_test_seq  = X_seq[val_end:], y_seq[val_end:]\n",
    "\n",
    "    validate_sequence_splits(X_train_seq, y_train_seq, X_val_seq, y_val_seq, X_test_seq, y_test_seq)\n",
    "\n",
    "    return X_train_seq, y_train_seq, X_val_seq, y_val_seq, X_test_seq, y_test_seq\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# validate_sequence_splits (unchanged, still works)\n",
    "# ==========================================================\n",
    "def validate_sequence_splits(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    \"\"\"Validate sequence splits maintain reasonable class distribution\"\"\"\n",
    "    print(\"\\n=== Sequence Split Validation ===\")\n",
    "    splits = [(\"Train\", y_train), (\"Val\", y_val), (\"Test\", y_test)]\n",
    "\n",
    "    for name, y_split in splits:\n",
    "        if len(y_split) > 0:\n",
    "            attack_ratio = np.sum(y_split) / len(y_split)\n",
    "            benign_count = len(y_split) - np.sum(y_split)\n",
    "            attack_count = np.sum(y_split)\n",
    "            print(f\"{name}: {benign_count} benign, {attack_count} attack (ratio: {attack_ratio:.3f})\")\n",
    "        else:\n",
    "            print(f\"{name}: Empty split!\")\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# get_balanced_class_weights (no change needed)\n",
    "# ==========================================================\n",
    "def get_balanced_class_weights(y_train):\n",
    "    \"\"\"Calculate balanced class weights with safety checks\"\"\"\n",
    "    class_counts = Counter(y_train)\n",
    "    print(f\"Training class counts: {dict(class_counts)}\")\n",
    "\n",
    "    if len(class_counts) < 2:\n",
    "        print(\"⚠️  WARNING: Only one class in training data!\")\n",
    "        return {0: 1.0, 1: 1.0}\n",
    "\n",
    "    total_samples = len(y_train)\n",
    "    n_classes = len(class_counts)\n",
    "    class_weights = {cls: total_samples / (n_classes * count) for cls, count in class_counts.items()}\n",
    "\n",
    "    print(f\"Calculated class weights: {class_weights}\")\n",
    "    return class_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f5d8e3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class AdvancedLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, num_layers=2, dropout=0.3, \n",
    "                 use_attention=True, bidirectional=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.use_attention = use_attention\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        print(f\"Initializing AdvancedLSTM:\")\n",
    "        print(f\"  Input dim: {input_dim}\")\n",
    "        print(f\"  Hidden dim: {hidden_dim}\")\n",
    "        print(f\"  Num layers: {num_layers}\")\n",
    "        print(f\"  Dropout: {dropout}\")\n",
    "        print(f\"  Attention: {use_attention}\")\n",
    "        print(f\"  Bidirectional: {bidirectional}\")\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        \n",
    "        lstm_output_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        \n",
    "        # Attention mechanism\n",
    "        if use_attention:\n",
    "            self.attention = nn.MultiheadAttention(\n",
    "                embed_dim=lstm_output_dim,\n",
    "                num_heads=4,\n",
    "                dropout=dropout,\n",
    "                batch_first=True\n",
    "            )\n",
    "        \n",
    "        # Classification head with residual connection\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(lstm_output_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "        \n",
    "        # Count parameters\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        print(f\"  Total parameters: {total_params:,}\")\n",
    "        print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "        \n",
    "    def forward(self, x, return_features=False):\n",
    "        # LSTM forward pass\n",
    "        lstm_out, (hidden, cell) = self.lstm(x)\n",
    "        \n",
    "        if self.use_attention:\n",
    "            # Apply attention to all time steps\n",
    "            attended_out, attention_weights = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "            # Use mean of attended outputs\n",
    "            features = attended_out.mean(dim=1)\n",
    "        else:\n",
    "            # Use last time step\n",
    "            features = lstm_out[:, -1, :]\n",
    "\n",
    "        if return_features:\n",
    "            return features  # Return the hidden layer values (pre-classifier)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(features)\n",
    "        \n",
    "        return logits.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6475f94e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def train_advanced_lstm_enhanced(X_train, y_train, X_val, y_val, params, model, epochs=30, min_delta=1e-4):\n",
    "    \"\"\"\n",
    "    Enhanced training with better overfitting prevention and chronology preservation.\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "    import numpy as np\n",
    "    from collections import Counter\n",
    "    from sklearn.metrics import roc_auc_score, f1_score\n",
    "\n",
    "    # Ensure float32 for GPU efficiency\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    X_val = X_val.astype(np.float32)\n",
    "    y_train = y_train.astype(np.float32)\n",
    "    y_val = y_val.astype(np.float32)\n",
    "\n",
    "    # Prepare datasets with NO shuffling (chronology preserved)\n",
    "    train_dataset = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "    val_dataset = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=params['batch_size'],\n",
    "        shuffle=False,  # CRITICAL: NO shuffling to preserve chronology\n",
    "        pin_memory=True,\n",
    "        num_workers=2,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=params['batch_size'],\n",
    "        shuffle=False,  # CRITICAL: NO shuffling\n",
    "        pin_memory=True,\n",
    "        num_workers=2,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "\n",
    "    # Optimizer with stronger weight decay if specified\n",
    "    weight_decay = params.get('l2_reg', 1e-5)\n",
    "    if params.get('optimizer') == 'adamw':\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(), \n",
    "            lr=params['lr'], \n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(), \n",
    "            lr=params['lr'],\n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "\n",
    "    # Conservative learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='max',  # Monitor validation F1 (maximize)\n",
    "        factor=0.5,  # Reduce LR by half\n",
    "        patience=params.get('reduce_lr_patience', 3),\n",
    "        verbose=True,\n",
    "        min_lr=params.get('min_lr', 1e-6)\n",
    "    )\n",
    "\n",
    "    # Loss function with class balancing\n",
    "    device = next(model.parameters()).device\n",
    "    class_counts = Counter(y_train)\n",
    "    \n",
    "    if len(class_counts) > 1:\n",
    "        pos_weight = torch.tensor([class_counts[0] / class_counts[1]], device=device)\n",
    "        print(f\"Using pos_weight: {pos_weight.item():.3f} (Benign/Attack ratio)\")\n",
    "    else:\n",
    "        pos_weight = torch.tensor([1.0], device=device)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    # Training monitoring\n",
    "    train_losses, val_aucs, val_f1s = [], [], []\n",
    "    best_val_f1 = 0\n",
    "    best_model_state = None\n",
    "    patience = params.get('early_stopping_patience', 3)\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # Overfitting detection variables\n",
    "    consecutive_loss_drops = 0\n",
    "    loss_drop_threshold = 0.9  # If loss drops by 90% in one epoch, flag it\n",
    "\n",
    "    print(f\"Training for max {epochs} epochs with early stopping (patience={patience})\")\n",
    "    print(f\"Batch size: {params['batch_size']}, Learning rate: {params['lr']}\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        batch_count = 0\n",
    "\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "\n",
    "            # Primary loss\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            # L2 regularization (if not using weight_decay in optimizer)\n",
    "            if params.get('l2_reg', 0) > 0 and params.get('optimizer') != 'adamw':\n",
    "                l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "                loss += params['l2_reg'] * l2_norm\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            clip_value = params.get('gradient_clip', 1.0)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_value)\n",
    "            \n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            batch_count += 1\n",
    "\n",
    "        avg_train_loss = epoch_loss / batch_count\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Overfitting detection: Check for suspiciously fast loss drops\n",
    "        if epoch > 0:\n",
    "            loss_drop_ratio = (train_losses[-2] - avg_train_loss) / train_losses[-2]\n",
    "            if loss_drop_ratio > loss_drop_threshold:\n",
    "                consecutive_loss_drops += 1\n",
    "                print(f\"WARNING: Large loss drop detected: {loss_drop_ratio:.1%}\")\n",
    "                if consecutive_loss_drops >= 2:\n",
    "                    print(\"ALERT: Multiple consecutive large loss drops - possible overfitting!\")\n",
    "            else:\n",
    "                consecutive_loss_drops = 0\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_probs, val_targets = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X = batch_X.to(device)\n",
    "                outputs = model(batch_X)\n",
    "                \n",
    "                # Check for NaN outputs\n",
    "                if torch.isnan(outputs).any():\n",
    "                    print(f\"WARNING: NaN detected in outputs at epoch {epoch+1}\")\n",
    "                    continue\n",
    "                \n",
    "                val_probs.extend(torch.sigmoid(outputs).cpu().numpy())\n",
    "                val_targets.extend(batch_y.numpy())\n",
    "\n",
    "        if len(val_probs) == 0:\n",
    "            print(f\"ERROR: No valid validation predictions at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "        val_probs = np.array(val_probs)\n",
    "        val_targets = np.array(val_targets)\n",
    "\n",
    "        # Calculate validation metrics\n",
    "        if len(np.unique(val_targets)) > 1:\n",
    "            val_auc = roc_auc_score(val_targets, val_probs)\n",
    "        else:\n",
    "            val_auc = 0.5\n",
    "            print(f\"WARNING: Only one class in validation set at epoch {epoch+1}\")\n",
    "\n",
    "        # Find best F1 threshold\n",
    "        thresholds = np.linspace(0.1, 0.9, 50)\n",
    "        f1_scores = []\n",
    "        for t in thresholds:\n",
    "            try:\n",
    "                f1 = f1_score(val_targets, (val_probs >= t).astype(int), zero_division=0)\n",
    "                f1_scores.append(f1)\n",
    "            except:\n",
    "                f1_scores.append(0.0)\n",
    "        \n",
    "        best_f1 = max(f1_scores) if f1_scores else 0.0\n",
    "        val_aucs.append(val_auc)\n",
    "        val_f1s.append(best_f1)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if best_f1 > best_val_f1 + min_delta:\n",
    "            best_val_f1 = best_f1\n",
    "            best_model_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        # Step the scheduler (monitoring validation F1)\n",
    "        scheduler.step(best_f1)\n",
    "\n",
    "        # Early stopping check\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1} (no improvement for {patience} epochs)\")\n",
    "            break\n",
    "\n",
    "        # Progress reporting\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch {epoch+1:2d}/{epochs} | \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "              f\"Val F1: {best_f1:.4f} | \"\n",
    "              f\"Val AUC: {val_auc:.4f} | \"\n",
    "              f\"LR: {current_lr:.2e}\")\n",
    "\n",
    "        # Additional overfitting warnings\n",
    "        if epoch >= 5 and best_f1 > 0.995:\n",
    "            print(\"WARNING: Suspiciously high validation F1 (>99.5%) - check for data leakage!\")\n",
    "        \n",
    "        if epoch >= 3 and avg_train_loss < 0.01:\n",
    "            print(\"WARNING: Very low training loss - possible overfitting\")\n",
    "\n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict({k: v.to(device) for k, v in best_model_state.items()})\n",
    "        print(f\"Loaded best model with validation F1: {best_val_f1:.4f}\")\n",
    "    else:\n",
    "        print(\"WARNING: No improvement found, using final epoch model\")\n",
    "\n",
    "    return model, train_losses, val_aucs, val_f1s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf59756",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def advanced_objective(trial, X_train_seq, y_train_seq):\n",
    "    params = {\n",
    "        'hidden_dim': trial.suggest_categorical('hidden_dim', [64, 128, 256]),\n",
    "        'num_layers': trial.suggest_int('num_layers', 2, 4),\n",
    "        'dropout': trial.suggest_float('dropout', 0.2, 0.5),\n",
    "        'lr': trial.suggest_float('lr', 5e-5, 5e-3, log=True),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [32, 64, 128]),\n",
    "        'use_attention': trial.suggest_categorical('use_attention', [True, False]),\n",
    "        'bidirectional': trial.suggest_categorical('bidirectional', [True, False]),\n",
    "        'optimizer': trial.suggest_categorical('optimizer', ['adam', 'adamw']),\n",
    "        'l2_reg': trial.suggest_float('l2_reg', 1e-6, 1e-3, log=True),\n",
    "    }\n",
    "    \n",
    "    if params['optimizer'] == 'adamw':\n",
    "        params['weight_decay'] = trial.suggest_float('weight_decay', 1e-5, 1e-3, log=True)\n",
    "    \n",
    "    try:\n",
    "        # Use cross-validation for more robust evaluation\n",
    "        kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        cv_scores = []\n",
    "        \n",
    "        for train_idx, val_idx in kf.split(X_train_seq, y_train_seq):\n",
    "            X_cv_train, X_cv_val = X_train_seq[train_idx], X_train_seq[val_idx]\n",
    "            y_cv_train, y_cv_val = y_train_seq[train_idx], y_train_seq[val_idx]\n",
    "\n",
    "            # 🔹 Build a fresh model for each CV split\n",
    "            input_dim = X_train_seq.shape[2]\n",
    "            num_classes = len(np.unique(y_train_seq))\n",
    "            \n",
    "            model = AdvancedLSTM(\n",
    "                input_dim=input_dim,\n",
    "                hidden_dim=params['hidden_dim'],\n",
    "                num_layers=params['num_layers'],\n",
    "                dropout=params['dropout'],\n",
    "                use_attention=params['use_attention'],\n",
    "                bidirectional=params['bidirectional']\n",
    "            ).to(device)\n",
    "\n",
    "\n",
    "            _, _, _, val_f1s = train_advanced_lstm_enhanced(\n",
    "                X_cv_train, y_cv_train,\n",
    "                X_cv_val, y_cv_val,\n",
    "                params,\n",
    "                model,\n",
    "                epochs=15\n",
    "            )\n",
    "            \n",
    "            cv_scores.append(max(val_f1s))\n",
    "        \n",
    "        return np.mean(cv_scores)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Trial failed: {e}\")\n",
    "        return 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e628c45",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def analyze_feature_importance(model, X_sample, feature_names):\n",
    "    \"\"\"Analyze which features are most important for predictions\"\"\"\n",
    "    # Store original model state\n",
    "    original_training_state = model.training\n",
    "    \n",
    "    # Set model to training mode for gradient computation\n",
    "    model.train()\n",
    "    \n",
    "    try:\n",
    "        # Use gradient-based feature importance\n",
    "        X_tensor = torch.FloatTensor(X_sample[:100]).to(device)  # Use subset for speed\n",
    "        X_tensor.requires_grad_(True)\n",
    "        \n",
    "        outputs = model(X_tensor)\n",
    "        \n",
    "        # Calculate gradients\n",
    "        outputs.sum().backward()\n",
    "        gradients = X_tensor.grad.abs().mean(dim=[0, 1]).cpu().numpy()\n",
    "        \n",
    "        # Create feature importance DataFrame\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': gradients\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        return importance_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in feature importance analysis: {e}\")\n",
    "        # Return dummy DataFrame in case of error\n",
    "        return pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': np.zeros(len(feature_names))\n",
    "        })\n",
    "    \n",
    "    finally:\n",
    "        # Restore original model state\n",
    "        model.train(original_training_state)\n",
    "\n",
    "def plot_advanced_results(train_losses, val_aucs, val_f1s, test_results):\n",
    "    \"\"\"Create comprehensive result plots\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    \n",
    "    # Training history\n",
    "    axes[0, 0].plot(train_losses)\n",
    "    axes[0, 0].set_title('Training Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[0, 1].plot(val_aucs, label='AUC', color='blue')\n",
    "    axes[0, 1].plot(val_f1s, label='F1', color='red')\n",
    "    axes[0, 1].set_title('Validation Metrics')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Score')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # ROC and PR curves\n",
    "    fpr, tpr, _ = roc_curve(test_results['y_true'], test_results['y_prob'])\n",
    "    axes[0, 2].plot(fpr, tpr, label=f'AUC = {test_results[\"auc\"]:.4f}')\n",
    "    axes[0, 2].plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "    axes[0, 2].set_title('ROC Curve')\n",
    "    axes[0, 2].set_xlabel('False Positive Rate')\n",
    "    axes[0, 2].set_ylabel('True Positive Rate')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    precision, recall, _ = precision_recall_curve(test_results['y_true'], test_results['y_prob'])\n",
    "    axes[1, 0].plot(recall, precision, label=f'PR-AUC = {test_results[\"pr_auc\"]:.4f}')\n",
    "    axes[1, 0].set_title('Precision-Recall Curve')\n",
    "    axes[1, 0].set_xlabel('Recall')\n",
    "    axes[1, 0].set_ylabel('Precision')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(test_results['y_true'], test_results['y_pred'])\n",
    "    sns.heatmap(cm, annot=True, fmt='d', ax=axes[1, 1], cmap='Blues')\n",
    "    axes[1, 1].set_title('Confusion Matrix')\n",
    "    axes[1, 1].set_xlabel('Predicted')\n",
    "    axes[1, 1].set_ylabel('Actual')\n",
    "    \n",
    "    # Threshold analysis\n",
    "    thresholds = np.linspace(0.1, 0.9, 100)\n",
    "    f1_scores = []\n",
    "    for t in thresholds:\n",
    "        try:\n",
    "            f1 = f1_score(test_results['y_true'], (test_results['y_prob'] >= t).astype(int))\n",
    "            f1_scores.append(f1)\n",
    "        except:\n",
    "            f1_scores.append(0.0)\n",
    "    \n",
    "    axes[1, 2].plot(thresholds, f1_scores)\n",
    "    axes[1, 2].axvline(x=test_results['best_threshold'], color='red', linestyle='--')\n",
    "    axes[1, 2].set_title('F1 Score vs Threshold')\n",
    "    axes[1, 2].set_xlabel('Threshold')\n",
    "    axes[1, 2].set_ylabel('F1 Score')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_model_comparison(baseline_results_df, lstm_results):\n",
    "    \"\"\"Plot comparison between baseline models and LSTM\"\"\"\n",
    "    # Add LSTM results to comparison\n",
    "    lstm_row = pd.DataFrame({\n",
    "        'Model': ['Advanced LSTM'],\n",
    "        'Accuracy': [lstm_results.get('accuracy', 0)],\n",
    "        'Precision': [lstm_results['precision']],\n",
    "        'Recall': [lstm_results['recall']],\n",
    "        'F1': [lstm_results['f1']],\n",
    "        'AUC': [lstm_results['auc']],\n",
    "        'PR-AUC': [lstm_results['pr_auc']]\n",
    "    })\n",
    "    \n",
    "    comparison_df = pd.concat([baseline_results_df, lstm_row], ignore_index=True)\n",
    "    \n",
    "    # Create comparison plots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    \n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1', 'AUC', 'PR-AUC']\n",
    "    colors = ['skyblue', 'lightgreen', 'lightcoral', 'gold', 'plum', 'lightpink']\n",
    "    \n",
    "    for i, (metric, color) in enumerate(zip(metrics, colors)):\n",
    "        row, col = i // 3, i % 3\n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        bars = ax.bar(comparison_df['Model'], comparison_df[metric], color=color, alpha=0.7)\n",
    "        ax.set_title(f'{metric} Comparison')\n",
    "        ax.set_ylabel(metric)\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Highlight best model\n",
    "        best_idx = comparison_df[metric].idxmax()\n",
    "        bars[best_idx].set_color('red')\n",
    "        bars[best_idx].set_alpha(0.9)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                   f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9b38f3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from tqdm import tqdm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5f9384",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# STEP 1: Data Loading and Initial Sanity Checks\n",
    "# ==============================\n",
    "print(\"\\n=== STEP 1: Data Loading and Initial Validation ===\")    \n",
    "try:\n",
    "    # Load data\n",
    "    print(\"Loading CICIDS2017 dataset...\")\n",
    "    df, feature_names = datadf, ftnames  # pre-loaded globals\n",
    "        \n",
    "    # Extract features and labels\n",
    "    if 'Label' in df.columns:\n",
    "        X = df.drop('Label', axis=1).values\n",
    "        y = df['Label'].values\n",
    "    elif 'label' in df.columns:\n",
    "        X = df.drop('label', axis=1).values\n",
    "        y = df['label'].values\n",
    "    else:\n",
    "        X = df.iloc[:, :-1].values\n",
    "        y = df.iloc[:, -1].values\n",
    "        \n",
    "    # Convert labels to binary\n",
    "    if y.dtype == 'object' or len(np.unique(y)) > 2:\n",
    "        print(\"Converting labels to binary classification...\")\n",
    "        benign_labels = ['BENIGN', 'Normal', 'normal', 'benign', 0]\n",
    "        y_binary = np.zeros(len(y), dtype=int)\n",
    "            \n",
    "        unique_labels, counts = np.unique(y, return_counts=True)\n",
    "        print(\"Original label distribution:\")\n",
    "        for label, count in zip(unique_labels, counts):\n",
    "            print(f\"  {label}: {count} samples\")\n",
    "            \n",
    "        for i, label in enumerate(y):\n",
    "            if label not in benign_labels:\n",
    "                y_binary[i] = 1\n",
    "            \n",
    "        y = y_binary\n",
    "        print(\"Converted to binary classification (0=Benign, 1=Attack)\")\n",
    "        \n",
    "    # Ensure numeric features\n",
    "    if X.dtype == 'object':\n",
    "        print(\"Converting features to numeric...\")\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        le = LabelEncoder()\n",
    "        for col in range(X.shape[1]):\n",
    "            if df.iloc[:, col].dtype == 'object':\n",
    "                X[:, col] = le.fit_transform(X[:, col].astype(str))\n",
    "        X = X.astype(float)\n",
    "        \n",
    "    # Data quality checks\n",
    "    nan_count = np.isnan(X).sum()\n",
    "    inf_count = np.isinf(X).sum()\n",
    "        \n",
    "    if nan_count > 0:\n",
    "        print(f\"Replacing {nan_count} NaN values with median...\")\n",
    "        from sklearn.impute import SimpleImputer\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        X = imputer.fit_transform(X)\n",
    "            \n",
    "    if inf_count > 0:\n",
    "        print(f\"Replacing {inf_count} infinite values...\")\n",
    "        X = np.nan_to_num(X, nan=0.0, posinf=1e10, neginf=-1e10)\n",
    "        \n",
    "    print(f\"Dataset loaded successfully!\")\n",
    "    print(f\"Total samples: {len(X)}\")\n",
    "    print(f\"Total features: {X.shape[1]}\")\n",
    "        \n",
    "    # Class distribution\n",
    "    unique_labels, counts = np.unique(y, return_counts=True)\n",
    "    attack_ratio = counts[1] / len(y) if len(counts) > 1 else 0\n",
    "    print(f\"Class distribution: Benign={counts[0]}, Attack={counts[1] if len(counts) > 1 else 0}\")\n",
    "    print(f\"Attack ratio: {attack_ratio:.3f}\")\n",
    "        \n",
    "    if attack_ratio < 0.01 or attack_ratio > 0.99:\n",
    "        print(f\"⚠️  SEVERE CLASS IMBALANCE DETECTED! Attack ratio: {attack_ratio:.3f}\")\n",
    "        \n",
    "    # 🔍 EXTRA: Feature-label correlation check\n",
    "    try:\n",
    "        corrs = [np.corrcoef(X[:, i], y)[0, 1] for i in range(X.shape[1])]\n",
    "        max_corr = np.nanmax(np.abs(corrs))\n",
    "        if max_corr > 0.95:\n",
    "            print(f\"🚨 POTENTIAL DATA LEAKAGE: Feature correlates with label at {max_corr:.3f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not compute feature-label correlation: {e}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cf8a74",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# STEP 2: CHRONOLOGICAL SPLITTING (CRITICAL)\n",
    "# ==============================\n",
    "print(\"\\n=== STEP 2: Chronological Data Splitting ===\")\n",
    "print(\"🕒 MAINTAINING TEMPORAL ORDER - NO SHUFFLING!\")\n",
    "    \n",
    "total_samples = len(X)\n",
    "train_end = int(0.6 * total_samples)\n",
    "val_end = int(0.8 * total_samples)\n",
    "    \n",
    "X_train, y_train = X[:train_end], y[:train_end]\n",
    "X_val, y_val     = X[train_end:val_end], y[train_end:val_end]\n",
    "X_test, y_test   = X[val_end:], y[val_end:]\n",
    "    \n",
    "print(f\"Training: {len(X_train)} | Validation: {len(X_val)} | Test: {len(X_test)}\")\n",
    "    \n",
    "# 🚨 EXTRA: Data leakage check\n",
    "train_hash = set(map(tuple, X_train[:100].astype(str)))  # sample-based\n",
    "val_hash = set(map(tuple, X_val[:100].astype(str)))\n",
    "test_hash = set(map(tuple, X_test[:100].astype(str)))\n",
    "    \n",
    "if train_hash & val_hash or val_hash & test_hash or train_hash & test_hash:\n",
    "    print(\"🚨 WARNING: Possible data leakage detected (overlapping samples across splits)\")\n",
    "    \n",
    "# Per-split balance checks\n",
    "for name, y_split in [(\"Train\", y_train), (\"Val\", y_val), (\"Test\", y_test)]:\n",
    "    uniq, counts = np.unique(y_split, return_counts=True)\n",
    "    print(f\"{name} class distribution: {dict(zip(uniq, counts))}\")\n",
    "    \n",
    "# Verify splits in temporal order\n",
    "if np.any(np.diff(np.arange(len(y_train))) < 0):\n",
    "    print(\"🚨 Temporal order broken in train split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b7b78a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# STEP 3: Feature Scaling (Applied Chronologically)\n",
    "# ==============================\n",
    "print(\"\\n=== STEP 3: Chronological Feature Scaling ===\")\n",
    "print(\"📊 Scaling based on training data only (no future information)\")\n",
    "    \n",
    "    # Fit scaler ONLY on training data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)  # Transform only, no fitting\n",
    "X_test_scaled = scaler.transform(X_test)  # Transform only, no fitting\n",
    "    \n",
    "    # DEBUG: Check for scaling issues\n",
    "train_mean = np.mean(X_train_scaled, axis=0)\n",
    "train_std = np.std(X_train_scaled, axis=0)\n",
    "    \n",
    "print(f\"Training data after scaling - Mean range: [{train_mean.min():.6f}, {train_mean.max():.6f}]\")\n",
    "print(f\"Training data after scaling - Std range: [{train_std.min():.6f}, {train_std.max():.6f}]\")\n",
    "    \n",
    "    # Check for constant features (std = 0)\n",
    "constant_features = np.sum(train_std < 1e-8)\n",
    "if constant_features > 0:\n",
    "    print(f\"⚠️  {constant_features} constant features detected - these won't help learning\")\n",
    "    \n",
    "print(\"✅ Feature scaling completed with temporal integrity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c912ea8a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n=== STEP 4: Baseline Model Evaluation ===\")\n",
    "    \n",
    "baseline_eval = BaselineEvaluator()\n",
    "print(\"Training baseline models...\")\n",
    "baseline_eval.add_dummy_classifier(X_train_scaled, y_train)\n",
    "baseline_eval.add_logistic_regression(X_train_scaled, y_train)\n",
    "baseline_eval.add_random_forest(X_train_scaled, y_train)\n",
    "    \n",
    "if len(X_train_scaled) <= 50000:\n",
    "    baseline_eval.add_knn(X_train_scaled, y_train)\n",
    "    \n",
    "baseline_eval.evaluate_all(X_test_scaled, y_test, \"Baseline\")\n",
    "baseline_results_df = baseline_eval.get_results_summary()\n",
    "print(\"\\n=== Baseline Results Summary ===\")\n",
    "print(baseline_results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da1828d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# STEP 5: CHRONOLOGICAL SEQUENCE CREATION (KEY INNOVATION)\n",
    "# ==============================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STEP 5: Chronological LSTM Sequence Creation\")\n",
    "print(\"=\"*50)\n",
    "print(\"🔄 Creating sequences while preserving temporal relationships\")\n",
    "    \n",
    "sequence_length = window_length\n",
    "print(f\"Using sequence length: {sequence_length}\")\n",
    "    \n",
    "def create_chronological_sequences_debugged(X, y, seq_len, min_sequences_per_class=10):\n",
    "    \"\"\"\n",
    "    Create sequences that maintain chronological order and ensure class balance\n",
    "    \"\"\"\n",
    "    print(f\"  Input: {len(X)} samples, sequence length: {seq_len}\")\n",
    "        \n",
    "    if len(X) < seq_len:\n",
    "        print(f\"⚠️  Not enough samples for sequences (need {seq_len}, have {len(X)})\")\n",
    "        return np.array([]), np.array([])\n",
    "        \n",
    "    sequences = []\n",
    "    labels = []\n",
    "        \n",
    "    # Create overlapping sequences with stride=1 to maximize data usage\n",
    "    for i in range(len(X) - seq_len + 1):\n",
    "        sequence = X[i:i + seq_len]\n",
    "            \n",
    "        # CRITICAL FIX: Use better labeling strategy\n",
    "        sequence_labels = y[i:i + seq_len]\n",
    "            \n",
    "        # Strategy 1: Use the LAST label in the sequence (most recent)\n",
    "        sequence_label = sequence_labels[-1]\n",
    "            \n",
    "        # Alternative Strategy 2: Majority voting (uncomment to use)\n",
    "        # sequence_label = 1 if np.sum(sequence_labels) > seq_len // 2 else 0\n",
    "            \n",
    "        # Alternative Strategy 3: ANY attack in sequence (uncomment to use)\n",
    "        # sequence_label = 1 if np.any(sequence_labels) else 0\n",
    "            \n",
    "        sequences.append(sequence)\n",
    "        labels.append(sequence_label)\n",
    "        \n",
    "    sequences = np.array(sequences)\n",
    "    labels = np.array(labels)\n",
    "        \n",
    "    print(f\"  Created {len(sequences)} sequences\")\n",
    "        \n",
    "    # Check class distribution in sequences\n",
    "    unique_seq, counts_seq = np.unique(labels, return_counts=True)\n",
    "    print(f\"    Sequence class distribution: {dict(zip(unique_seq, counts_seq))}\")\n",
    "        \n",
    "    # CRITICAL CHECK: Verify both classes exist\n",
    "    if len(unique_seq) < 2:\n",
    "        print(f\"🚨 CRITICAL: Only one class in sequences! This will break training!\")\n",
    "        print(f\"   Try different sequence labeling strategy\")\n",
    "        return sequences, labels  # Return anyway for debugging\n",
    "        \n",
    "    # Check class balance\n",
    "    if len(unique_seq) > 1:\n",
    "        min_class_count = min(counts_seq)\n",
    "        minority_ratio = min_class_count / len(labels)\n",
    "        print(f\"    Minority class ratio: {minority_ratio:.4f}\")\n",
    "            \n",
    "        if minority_ratio < 0.01:\n",
    "            print(f\"⚠️  SEVERE sequence class imbalance detected!\")\n",
    "            print(f\"   Consider different sequence labeling or balancing techniques\")\n",
    "            \n",
    "        if min_class_count < min_sequences_per_class:\n",
    "            print(f\"    ⚠️  Minority class has only {min_class_count} sequences\")\n",
    "        \n",
    "    return sequences, labels\n",
    "    \n",
    "# Create sequences for each split independently\n",
    "print(\"Creating sequences for training set...\")\n",
    "X_train_seq, y_train_seq = create_chronological_sequences_debugged(\n",
    "    X_train_scaled, y_train, sequence_length\n",
    ")\n",
    "    \n",
    "print(\"Creating sequences for validation set...\")\n",
    "X_val_seq, y_val_seq = create_chronological_sequences_debugged(\n",
    "    X_val_scaled, y_val, sequence_length\n",
    ")\n",
    "    \n",
    "print(\"Creating sequences for test set...\")\n",
    "X_test_seq, y_test_seq = create_chronological_sequences_debugged(\n",
    "    X_test_scaled, y_test, sequence_length\n",
    ")\n",
    "    \n",
    "print(f\"\\nFinal sequence shapes:\")\n",
    "print(f\"  Training: {X_train_seq.shape}\")\n",
    "print(f\"  Validation: {X_val_seq.shape}\")\n",
    "print(f\"  Test: {X_test_seq.shape}\")\n",
    "    \n",
    "    # Validation checks\n",
    "if len(X_train_seq) == 0:\n",
    "    print(\"❌ ERROR: No training sequences created!\")\n",
    "    \n",
    "# CRITICAL: Check for class imbalance in sequences\n",
    "train_seq_unique, train_seq_counts = np.unique(y_train_seq, return_counts=True)\n",
    "val_seq_unique, val_seq_counts = np.unique(y_val_seq, return_counts=True)\n",
    "    \n",
    "if len(train_seq_unique) < 2 or len(val_seq_unique) < 2:\n",
    "    print(\"🚨 CRITICAL: Single class in training or validation sequences!\")\n",
    "    print(\"This will cause stagnant metrics. The model cannot learn without both classes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddfff7e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ==============================\n",
    "# STEP 6: IMPROVED LSTM PARAMETERS (Anti-Overfitting)\n",
    "# ==============================\n",
    "print(\"\\n=== STEP 6: Improved LSTM Configuration ===\")\n",
    "print(\"🛡️  Using anti-overfitting measures with class imbalance handling\")\n",
    "    \n",
    "# Calculate conservative parameters based on data size\n",
    "n_train_sequences = len(X_train_seq)\n",
    "n_features = X_train_seq.shape[2]\n",
    "    \n",
    "    # Calculate class weights for imbalanced data\n",
    "if len(train_seq_unique) > 1:\n",
    "    class_weights = len(y_train_seq) / (len(train_seq_unique) * train_seq_counts)\n",
    "    pos_weight = class_weights[1] / class_weights[0] if len(class_weights) > 1 else 1.0\n",
    "    print(f\"Positive class weight (for BCE loss): {pos_weight:.2f}\")\n",
    "else:\n",
    "    pos_weight = 1.0\n",
    "    \n",
    "def get_improved_params(n_sequences, n_features, pos_weight):\n",
    "    \"\"\"Get improved parameters with class imbalance handling\"\"\"\n",
    "        \n",
    "    # Scale model complexity with data size\n",
    "    if n_sequences < 500:\n",
    "        hidden_dim = min(32, max(32, n_features // 4))  # Increased minimum\n",
    "        num_layers = 1\n",
    "        dropout = 0.5  # Reduced from 0.6\n",
    "    elif n_sequences < 2000:\n",
    "        hidden_dim = min(64, max(64, n_features // 2))  # Increased\n",
    "        num_layers = 2\n",
    "        dropout = 0.4  # Reduced\n",
    "    elif n_sequences < 10000:\n",
    "        hidden_dim = min(256, max(96, n_features))\n",
    "        num_layers = 2\n",
    "        dropout = 0.3\n",
    "    else:\n",
    "        hidden_dim = min(512, max(128, n_features))\n",
    "        num_layers = 3\n",
    "        dropout = 0.2\n",
    "        \n",
    "    return {\n",
    "        'hidden_dim': hidden_dim,\n",
    "        'num_layers': num_layers,\n",
    "        'dropout': dropout,\n",
    "        'lr': 0.001,  # Increased from 1e-4 for better convergence\n",
    "        'batch_size': min(16, max(8, n_sequences // 50)),  # Larger batches\n",
    "        'use_attention': n_sequences > 1000,  # Enable attention for larger datasets\n",
    "        'bidirectional': n_sequences > 500,  # Enable bidirectional for moderate datasets\n",
    "        'optimizer': 'adam',\n",
    "        'l2_reg': 1e-4,  # Reduced from 1e-3\n",
    "        'gradient_clip': 1.0,  # Increased from 0.5            'early_stopping_patience': 10,  # Increased\n",
    "        'reduce_lr_patience': 3,\n",
    "        'min_lr': 1e-6,\n",
    "        'pos_weight': pos_weight,  # For handling class imbalance\n",
    "        'focal_loss': pos_weight > 2.0 or pos_weight < 0.5  # Use focal loss for severe imbalance\n",
    "    }\n",
    "    \n",
    "best_params = get_improved_params(n_train_sequences, n_features, pos_weight)\n",
    "print(f\"Improved parameters: {best_params}\")\n",
    "    \n",
    "    # Check for potential overfitting risks\n",
    "complexity_ratio = (best_params['hidden_dim'] * best_params['num_layers']) / n_train_sequences\n",
    "print(f\"Model complexity ratio: {complexity_ratio:.4f}\")\n",
    "    \n",
    "if complexity_ratio > 0.2:\n",
    "    print(f\"⚠️  High complexity ratio - adjusting parameters\")\n",
    "    best_params['hidden_dim'] = max(32, best_params['hidden_dim'] // 2)\n",
    "    best_params['dropout'] = min(0.6, best_params['dropout'] + 0.1)\n",
    "    print(f\"Adjusted parameters: hidden_dim={best_params['hidden_dim']}, dropout={best_params['dropout']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d872fe5d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# STEP 7: ENHANCED LSTM TRAINING with tqdm + GPU check\n",
    "# ==============================\n",
    "print(\"\\n=== STEP 7: Enhanced LSTM Training with Comprehensive Monitoring ===\")\n",
    "\n",
    "# Create model with debugging\n",
    "final_model = AdvancedLSTM(\n",
    "    input_dim=X_train_seq.shape[2],\n",
    "    hidden_dim=best_params['hidden_dim'],\n",
    "    num_layers=best_params['num_layers'],\n",
    "    dropout=best_params['dropout'],\n",
    "    use_attention=best_params.get('use_attention', False),\n",
    "    bidirectional=best_params.get('bidirectional', False)\n",
    ").to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in final_model.parameters())\n",
    "print(f\"Model parameters: {total_params:,}\")\n",
    "\n",
    "# ✅ GPU sanity check\n",
    "print(f\"Using device: {device}\")\n",
    "try:\n",
    "    test_tensor = torch.rand(10).to(device)\n",
    "    print(f\"Tensor successfully moved to {device}: {test_tensor.device}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Could not allocate tensor on device {device}: {e}\")\n",
    "\n",
    "# Patch train_advanced_lstm_enhanced_debug to include tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_advanced_lstm_enhanced_debug(X_train, y_train, X_val, y_val,\n",
    "                                        params, model, epochs=20, verbose=True):\n",
    "\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    X_val   = X_val.astype(np.float32)\n",
    "    y_train = y_train.astype(np.float32)\n",
    "    y_val   = y_val.astype(np.float32)\n",
    "\n",
    "    train_dataset = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "    val_dataset   = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=params.get('batch_size', 256),\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=params.get('batch_size', 256),\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "    criterion = nn.BCEWithLogitsLoss(\n",
    "        pos_weight=torch.tensor([params.get('pos_weight', 1.0)], device=device)\n",
    "    )\n",
    "\n",
    "    # Metric trackers\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "    val_aucs, val_f1s = [], []\n",
    "\n",
    "    best_val_f1, patience_counter = 0, 0\n",
    "    patience = params.get('early_stopping_patience', 10)\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # ---- Training ----\n",
    "        model.train()\n",
    "        epoch_loss, correct, total = 0.0, 0, 0\n",
    "        for X_batch, y_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            preds = (torch.sigmoid(outputs) >= 0.5).int()\n",
    "            correct += (preds.squeeze() == y_batch.int()).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "        train_accs.append(correct / total if total > 0 else 0)\n",
    "\n",
    "        # ---- Validation ----\n",
    "        model.eval()\n",
    "        val_probs, val_targets = [], []\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "                preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "                val_probs.extend(probs)\n",
    "                val_targets.extend(y_batch.cpu().numpy())\n",
    "\n",
    "                val_correct += (preds.squeeze() == y_batch.cpu().int().numpy()).sum()\n",
    "                val_total += y_batch.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_correct / val_total if val_total > 0 else 0)\n",
    "\n",
    "        val_probs = np.array(val_probs).flatten()\n",
    "        val_targets = np.array(val_targets)\n",
    "\n",
    "        try:\n",
    "            val_auc = roc_auc_score(val_targets, val_probs)\n",
    "        except:\n",
    "            val_auc = 0.5\n",
    "\n",
    "        thresholds = np.linspace(0.1, 0.9, 20)\n",
    "        f1_scores = [f1_score(val_targets, (val_probs >= t).astype(int)) for t in thresholds]\n",
    "        best_f1 = max(f1_scores) if f1_scores else 0\n",
    "\n",
    "        val_aucs.append(val_auc)\n",
    "        val_f1s.append(best_f1)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | \"\n",
    "                  f\"Train Loss={avg_loss:.4f}, Train Acc={train_accs[-1]:.4f} | \"\n",
    "                  f\"Val Loss={val_loss:.4f}, Val Acc={val_accs[-1]:.4f} | \"\n",
    "                  f\"Val F1={best_f1:.4f} | AUC={val_auc:.4f}\")\n",
    "\n",
    "        # ---- Early stopping ----\n",
    "        if best_f1 > best_val_f1:\n",
    "            best_val_f1 = best_f1\n",
    "            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"⏹ Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "        # ---- Plot after each epoch ----\n",
    "        plt.figure(figsize=(15, 10))\n",
    "\n",
    "        metrics = [\n",
    "            (\"Train Loss\", train_losses),\n",
    "            (\"Validation Loss\", val_losses),\n",
    "            (\"Train Accuracy\", train_accs),\n",
    "            (\"Validation Accuracy\", val_accs),\n",
    "            (\"Validation F1\", val_f1s),\n",
    "            (\"Validation AUC\", val_aucs),\n",
    "        ]\n",
    "\n",
    "        for i, (title, values) in enumerate(metrics, 1):\n",
    "            plt.subplot(2, 3, i)\n",
    "            plt.plot(range(1, len(values)+1), values, marker='o')\n",
    "            plt.title(title)\n",
    "            plt.xlabel(\"Epoch\")\n",
    "            plt.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict({k: v.to(device) for k, v in best_state.items()})\n",
    "\n",
    "    return model, train_losses, val_losses, train_accs, val_accs, val_aucs, val_f1s\n",
    "\n",
    "# finally call training\n",
    "final_model, train_losses, val_aucs, val_f1s, training_info = train_advanced_lstm_enhanced_debug(\n",
    "    X_train_seq, y_train_seq, X_val_seq, y_val_seq, best_params, final_model,\n",
    "    epochs=20, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d941aaf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# STEP 8: Comprehensive Evaluation with Debugging\n",
    "# ==============================\n",
    "print(\"\\n=== STEP 8: Final Model Evaluation with Debugging ===\")\n",
    "    \n",
    "try:\n",
    "    # Test evaluation with debugging\n",
    "    test_dataset = TensorDataset(torch.FloatTensor(X_test_seq), torch.FloatTensor(y_test_seq))\n",
    "    test_loader = DataLoader(test_dataset, batch_size=min(64, len(X_test_seq)), shuffle=False)\n",
    "        \n",
    "    final_model.eval()\n",
    "    test_probs = []\n",
    "    test_targets = []\n",
    "    all_outputs = []\n",
    "        \n",
    "    print(\"Evaluating model on test set...\")\n",
    "    with torch.no_grad():\n",
    "        for i, (batch_X, batch_y) in enumerate(test_loader):\n",
    "            batch_X = batch_X.to(device)\n",
    "            outputs = final_model(batch_X)\n",
    "                \n",
    "            # Debug model outputs\n",
    "            if i == 0:  # First batch debugging\n",
    "                print(f\"First batch outputs shape: {outputs.shape}\")\n",
    "                print(f\"Output range: [{outputs.min().item():.4f}, {outputs.max().item():.4f}]\")\n",
    "                print(f\"Output mean: {outputs.mean().item():.4f}\")\n",
    "                print(f\"Output std: {outputs.std().item():.4f}\")\n",
    "                \n",
    "            all_outputs.extend(outputs.cpu().numpy())\n",
    "                \n",
    "            if not torch.isnan(outputs).any() and not torch.isinf(outputs).any():\n",
    "                probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "                test_probs.extend(probs)\n",
    "                test_targets.extend(batch_y.numpy())\n",
    "            else:\n",
    "                print(f\"⚠️  NaN or Inf detected in batch {i}\")\n",
    "        \n",
    "    test_probs = np.array(test_probs).flatten()\n",
    "    test_targets = np.array(test_targets)\n",
    "    all_outputs = np.array(all_outputs).flatten()\n",
    "        \n",
    "    print(f\"\\nModel output analysis:\")\n",
    "    print(f\"Raw outputs - Min: {all_outputs.min():.4f}, Max: {all_outputs.max():.4f}\")\n",
    "    print(f\"Raw outputs - Mean: {all_outputs.mean():.4f}, Std: {all_outputs.std():.4f}\")\n",
    "    print(f\"Probabilities - Min: {test_probs.min():.4f}, Max: {test_probs.max():.4f}\")\n",
    "    print(f\"Probabilities - Mean: {test_probs.mean():.4f}, Std: {test_probs.std():.4f}\")\n",
    "        \n",
    "    # Check for constant predictions\n",
    "    if test_probs.std() < 1e-6:\n",
    "        print(\"🚨 CRITICAL ISSUE: Model is making constant predictions!\")\n",
    "        print(\"   All probabilities are essentially the same\")\n",
    "        print(\"   This explains the stagnant metrics\")\n",
    "        print(\"   Model failed to learn meaningful patterns\")\n",
    "        \n",
    "    # Find optimal threshold\n",
    "    if len(np.unique(test_targets)) > 1 and test_probs.std() > 1e-6:\n",
    "        thresholds = np.linspace(0.01, 0.99, 100)\n",
    "        f1_scores = []\n",
    "        for t in thresholds:\n",
    "            preds = (test_probs >= t).astype(int)\n",
    "            f1 = f1_score(test_targets, preds, zero_division=0)\n",
    "            f1_scores.append(f1)\n",
    "            \n",
    "        best_threshold_idx = np.argmax(f1_scores)\n",
    "        best_threshold = thresholds[best_threshold_idx]\n",
    "        best_f1 = f1_scores[best_threshold_idx]\n",
    "            \n",
    "        print(f\"Threshold optimization:\")\n",
    "        print(f\"  Best threshold: {best_threshold:.4f}\")\n",
    "        print(f\"  Best F1 at threshold: {best_f1:.4f}\")\n",
    "    else:\n",
    "        best_threshold = 0.5\n",
    "        print(\"Using default threshold: 0.5\")\n",
    "        \n",
    "    y_pred_final = (test_probs >= best_threshold).astype(int)\n",
    "        \n",
    "    # Calculate metrics with debugging\n",
    "    print(f\"\\nPrediction analysis:\")\n",
    "    pred_unique, pred_counts = np.unique(y_pred_final, return_counts=True)\n",
    "    print(f\"Predictions distribution: {dict(zip(pred_unique, pred_counts))}\")\n",
    "    target_unique, target_counts = np.unique(test_targets, return_counts=True)\n",
    "    print(f\"Targets distribution: {dict(zip(target_unique, target_counts))}\")\n",
    "        \n",
    "    # Calculate metrics\n",
    "    lstm_results = {\n",
    "        'accuracy': accuracy_score(test_targets, y_pred_final),\n",
    "        'f1': f1_score(test_targets, y_pred_final, zero_division=0),\n",
    "        'precision': precision_score(test_targets, y_pred_final, zero_division=0),\n",
    "        'recall': recall_score(test_targets, y_pred_final, zero_division=0),\n",
    "        'best_threshold': best_threshold\n",
    "    }\n",
    "        \n",
    "    if len(np.unique(test_targets)) > 1 and test_probs.std() > 1e-6:\n",
    "        try:\n",
    "            lstm_results['auc'] = roc_auc_score(test_targets, test_probs)\n",
    "            lstm_results['pr_auc'] = average_precision_score(test_targets, test_probs)\n",
    "        except:\n",
    "            lstm_results['auc'] = 0.5\n",
    "            lstm_results['pr_auc'] = 0.5\n",
    "            print(\"⚠️  Could not calculate AUC scores\")\n",
    "    else:\n",
    "        lstm_results['auc'] = 0.5\n",
    "        lstm_results['pr_auc'] = 0.5\n",
    "        \n",
    "    print(f\"\\n=== Chronological LSTM Results ===\")\n",
    "    print(f\"Accuracy:  {lstm_results['accuracy']:.4f}\")\n",
    "    print(f\"AUC:       {lstm_results['auc']:.4f}\")\n",
    "    print(f\"F1 Score:  {lstm_results['f1']:.4f}\")\n",
    "    print(f\"Precision: {lstm_results['precision']:.4f}\")\n",
    "    print(f\"Recall:    {lstm_results['recall']:.4f}\")\n",
    "        \n",
    "    # Enhanced reality check\n",
    "    if lstm_results['f1'] == 0:\n",
    "        print(\"\\n🚨 CRITICAL ISSUE: F1 Score is 0!\")\n",
    "        print(\"   This indicates complete prediction failure\")\n",
    "        print(\"   Check model architecture and training process\")\n",
    "    elif lstm_results['f1'] < 0.1:\n",
    "        print(\"\\n🚨 Very poor performance - model barely learning\")\n",
    "    elif lstm_results['f1'] > 0.98:\n",
    "        print(\"\\n🚨 SUSPICIOUSLY HIGH PERFORMANCE!\")\n",
    "        print(\"   F1 > 0.98 suggests possible data leakage or unrealistic dataset\")\n",
    "    elif lstm_results['f1'] > 0.95:\n",
    "        print(\"\\n⚠️  Very high performance - double-check for issues\")\n",
    "    else:\n",
    "        print(\"\\n✅ Realistic performance range\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Evaluation failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    lstm_results = {'f1': 0.0, 'auc': 0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faab2f96",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# STEP 9: Enhanced Diagnostics and Recommendations\n",
    "# ==============================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ENHANCED DIAGNOSTICS AND RECOMMENDATIONS\")\n",
    "print(\"=\"*50)\n",
    "    \n",
    "# Diagnose the root cause of stagnant metrics\n",
    "print(\"\\n🔍 ROOT CAUSE ANALYSIS:\")\n",
    "    \n",
    "if lstm_results.get('f1', 0) == 0:\n",
    "    print(\"❌ ISSUE 1: Zero F1 Score\")\n",
    "    print(\"   Causes: Model predicting single class OR complete mismatch\")\n",
    "    print(\"   Solutions:\")\n",
    "    print(\"   - Check class distribution in sequences\")\n",
    "    print(\"   - Adjust loss function (use focal loss or class weights)\")\n",
    "    print(\"   - Try different sequence labeling strategy\")\n",
    "    print(\"   - Increase learning rate\")\n",
    "    \n",
    "elif len(val_f1s) > 5 and np.var(val_f1s) < 1e-6:\n",
    "    print(\"❌ ISSUE 2: Stagnant Validation Metrics\")\n",
    "    print(\"   Causes: Model not learning OR constant predictions\")\n",
    "    print(\"   Solutions:\")\n",
    "    print(\"   - Check gradient flow (gradient norms)\")\n",
    "    print(\"   - Increase learning rate (try 1e-3 or 1e-2)\")\n",
    "    print(\"   - Reduce model complexity if overfitting\")\n",
    "    print(\"   - Check for vanishing gradients\")\n",
    "    print(\"   - Try different activation functions\")\n",
    "    \n",
    "elif 'test_probs' in locals() and test_probs.std() < 1e-6:\n",
    "    print(\"❌ ISSUE 3: Constant Model Predictions\")\n",
    "    print(\"   Causes: Model stuck in local minimum OR poor initialization\")\n",
    "    print(\"   Solutions:\")\n",
    "    print(\"   - Reinitialize model weights\")\n",
    "    print(\"   - Use Xavier/He initialization\")\n",
    "    print(\"   - Try different optimizer (SGD with momentum)\")\n",
    "    print(\"   - Increase batch size\")\n",
    "    print(\"   - Check input data preprocessing\")\n",
    "    \n",
    "# Data-related issues\n",
    "if attack_ratio < 0.01 or attack_ratio > 0.99:\n",
    "    print(\"❌ ISSUE 4: Severe Class Imbalance\")\n",
    "    print(\"   Solutions:\")\n",
    "    print(\"   - Use weighted loss function (implemented)\")\n",
    "    print(\"   - Try focal loss for extreme imbalance\")\n",
    "    print(\"   - Use SMOTE or other resampling techniques\")\n",
    "    print(\"   - Adjust decision threshold\")\n",
    "    \n",
    "# Architecture issues\n",
    "if complexity_ratio > 0.2:\n",
    "    print(\"❌ ISSUE 5: Model Too Complex\")\n",
    "    print(\"   Solutions:\")\n",
    "    print(\"   - Reduce hidden dimensions\")\n",
    "    print(\"   - Reduce number of layers\")\n",
    "    print(\"   - Increase regularization\")\n",
    "    \n",
    "# Compare with baselines\n",
    "lstm_f1 = lstm_results.get('f1', 0)\n",
    "best_baseline_f1 = baseline_results_df['F1'].max() if len(baseline_results_df) > 0 else 0\n",
    "    \n",
    "print(f\"\\n📊 Performance Comparison:\")\n",
    "print(f\"Best Baseline F1: {best_baseline_f1:.4f}\")\n",
    "print(f\"Chronological LSTM F1: {lstm_f1:.4f}\")\n",
    "    \n",
    "if lstm_f1 > 0:\n",
    "    improvement = ((lstm_f1 / best_baseline_f1) - 1) * 100 if best_baseline_f1 > 0 else float('inf')\n",
    "    print(f\"Improvement: {improvement:.1f}%\")\n",
    "else:\n",
    "    print(\"Improvement: N/A (LSTM failed)\")\n",
    "    \n",
    "print(f\"\\n🕒 Temporal Integrity: ✅ MAINTAINED\")\n",
    "print(f\"   - Chronological data splitting\")\n",
    "print(f\"   - No future information leakage\")\n",
    "print(f\"   - Realistic evaluation protocol\")\n",
    "    \n",
    "print(f\"\\n📋 SPECIFIC RECOMMENDATIONS:\")\n",
    "    \n",
    "if lstm_f1 == 0:\n",
    "    print(\"🚨 IMMEDIATE ACTIONS NEEDED:\")\n",
    "    print(\"1. Check data preprocessing - verify both classes exist in sequences\")\n",
    "    print(\"2. Try simpler model first (single LSTM layer, hidden_dim=32)\")\n",
    "    print(\"3. Use class weights in loss function\")\n",
    "    print(\"4. Start with higher learning rate (1e-3)\")\n",
    "    print(\"5. Debug gradient flow - print gradient norms during training\")\n",
    "        \n",
    "elif lstm_f1 < 0.1:\n",
    "    print(\"⚠️  POOR PERFORMANCE - TRY:\")\n",
    "    print(\"1. Increase learning rate to 1e-3 or 5e-3\")\n",
    "    print(\"2. Reduce model complexity\")\n",
    "    print(\"3. Check for gradient vanishing\")\n",
    "    print(\"4. Try different loss function (focal loss)\")\n",
    "    print(\"5. Verify input data normalization\")\n",
    "        \n",
    "elif lstm_f1 < best_baseline_f1:\n",
    "    print(\"📉 UNDERPERFORMING BASELINES:\")\n",
    "    print(\"1. LSTM may be overkill for this dataset\")\n",
    "    print(\"2. Try reducing sequence length\")\n",
    "    print(\"3. Consider using baseline model instead\")\n",
    "    print(\"4. Check if temporal patterns actually exist in data\")\n",
    "        \n",
    "else:\n",
    "    print(\"✅ PERFORMANCE ACCEPTABLE:\")\n",
    "    print(\"1. Monitor for overfitting in production\")\n",
    "    print(\"2. Consider ensemble methods\")\n",
    "    print(\"3. Fine-tune hyperparameters\")\n",
    "        \n",
    "# Additional debugging suggestions\n",
    "print(f\"\\n🛠️  DEBUGGING CHECKLIST:\")\n",
    "print(\"□ Verify both classes exist in training sequences\")\n",
    "print(\"□ Check gradient norms during training\")\n",
    "print(\"□ Monitor loss convergence\")\n",
    "print(\"□ Verify model outputs are changing\")\n",
    "print(\"□ Test with different sequence lengths\")\n",
    "print(\"□ Try different optimizers (Adam vs SGD)\")\n",
    "print(\"□ Check for NaN/Inf in inputs\")\n",
    "print(\"□ Verify loss function implementation\")\n",
    "\n",
    "\n",
    "def train_advanced_lstm_enhanced_debug(X_train, y_train, X_val, y_val, params, model, epochs=50, verbose=True):\n",
    "    \"\"\"\n",
    "    Enhanced training function with comprehensive debugging\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "    from sklearn.metrics import f1_score, roc_auc_score\n",
    "    import numpy as np\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Setup datasets\n",
    "    train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
    "    val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=min(64, len(X_val)), shuffle=False)\n",
    "\n",
    "    # Setup loss function with class weights\n",
    "    pos_weight = torch.tensor(params.get('pos_weight', 1.0)).to(device)\n",
    "\n",
    "    if params.get('focal_loss', False):\n",
    "        # Implement focal loss for severe class imbalance\n",
    "        class FocalLoss(nn.Module):\n",
    "            def __init__(self, alpha=1, gamma=2, pos_weight=1.0):\n",
    "                super().__init__()\n",
    "                self.alpha = alpha\n",
    "                self.gamma = gamma\n",
    "                self.pos_weight = pos_weight\n",
    "                \n",
    "            def forward(self, inputs, targets):\n",
    "                bce_loss = nn.functional.binary_cross_entropy_with_logits(\n",
    "                    inputs, targets, pos_weight=torch.tensor(self.pos_weight).to(inputs.device)\n",
    "                )\n",
    "                pt = torch.exp(-bce_loss)\n",
    "                focal_loss = self.alpha * (1-pt)**self.gamma * bce_loss\n",
    "                return focal_loss.mean()\n",
    "        \n",
    "        criterion = FocalLoss(pos_weight=pos_weight.item())\n",
    "        print(f\"Using Focal Loss with pos_weight={pos_weight.item():.2f}\")\n",
    "    else:\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        print(f\"Using BCE Loss with pos_weight={pos_weight.item():.2f}\")\n",
    "\n",
    "    # Setup optimizer\n",
    "    if params['optimizer'] == 'adam':\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(), \n",
    "            lr=params['lr'], \n",
    "            weight_decay=params.get('l2_reg', 1e-4)\n",
    "        )\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(\n",
    "            model.parameters(), \n",
    "            lr=params['lr'], \n",
    "            momentum=0.9,\n",
    "            weight_decay=params.get('l2_reg', 1e-4)\n",
    "        )\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='max', \n",
    "        factor=0.5, \n",
    "        patience=params.get('reduce_lr_patience', 5),\n",
    "        min_lr=params.get('min_lr', 1e-6),\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # Training tracking\n",
    "    train_losses = []\n",
    "    val_aucs = []\n",
    "    val_f1s = []\n",
    "    best_val_f1 = 0\n",
    "    patience_counter = 0\n",
    "    training_info = {'grad_norms': [], 'lr_history': []}\n",
    "\n",
    "    print(f\"Starting training for {epochs} epochs...\")\n",
    "    print(f\"Training samples: {len(X_train)}, Validation samples: {len(X_val)}\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_losses = []\n",
    "        epoch_grad_norms = []\n",
    "        \n",
    "        for batch_idx, (batch_X, batch_y) in enumerate(train_loader):\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device).float()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            \n",
    "            # Debug: Check for NaN/Inf in outputs\n",
    "            if torch.isnan(outputs).any() or torch.isinf(outputs).any():\n",
    "                print(f\"⚠️  NaN/Inf detected in outputs at epoch {epoch}, batch {batch_idx}\")\n",
    "                continue\n",
    "            \n",
    "            loss = criterion(outputs.squeeze(), batch_y)\n",
    "            \n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                print(f\"⚠️  NaN/Inf detected in loss at epoch {epoch}, batch {batch_idx}\")\n",
    "                continue\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping and monitoring\n",
    "            if params.get('gradient_clip', 0) > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), params['gradient_clip'])\n",
    "            \n",
    "            # Monitor gradient norms\n",
    "            grad_norm = 0\n",
    "            for p in model.parameters():\n",
    "                if p.grad is not None:\n",
    "                    grad_norm += p.grad.data.norm(2).item() ** 2\n",
    "            grad_norm = grad_norm ** 0.5\n",
    "            epoch_grad_norms.append(grad_norm)\n",
    "            \n",
    "            optimizer.step()\n",
    "            epoch_losses.append(loss.item())\n",
    "        \n",
    "        if len(epoch_losses) == 0:\n",
    "            print(f\"⚠️  No valid batches in epoch {epoch}\")\n",
    "            continue\n",
    "            \n",
    "        avg_train_loss = np.mean(epoch_losses)\n",
    "        avg_grad_norm = np.mean(epoch_grad_norms)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        training_info['grad_norms'].append(avg_grad_norm)\n",
    "        training_info['lr_history'].append(optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_probs = []\n",
    "        val_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X = batch_X.to(device)\n",
    "                outputs = model(batch_X)\n",
    "                \n",
    "                if not torch.isnan(outputs).any():\n",
    "                    probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "                    val_probs.extend(probs)\n",
    "                    val_targets.extend(batch_y.numpy())\n",
    "        \n",
    "        if len(val_probs) > 0:\n",
    "            val_probs = np.array(val_probs).flatten()\n",
    "            val_targets = np.array(val_targets)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            val_preds = (val_probs >= 0.5).astype(int)\n",
    "            val_f1 = f1_score(val_targets, val_preds, zero_division=0)\n",
    "            \n",
    "            try:\n",
    "                val_auc = roc_auc_score(val_targets, val_probs) if len(np.unique(val_targets)) > 1 else 0.5\n",
    "            except:\n",
    "                val_auc = 0.5\n",
    "            \n",
    "            val_f1s.append(val_f1)\n",
    "            val_aucs.append(val_auc)\n",
    "            \n",
    "            # Learning rate scheduling\n",
    "            scheduler.step(val_f1)\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_f1 > best_val_f1:\n",
    "                best_val_f1 = val_f1\n",
    "                patience_counter = 0\n",
    "                # Save best model state\n",
    "                best_model_state = model.state_dict().copy()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            if verbose and (epoch + 1) % 5 == 0:\n",
    "                print(f\"Epoch {epoch+1:3d}: Loss={avg_train_loss:.4f}, \"\n",
    "                        f\"Val F1={val_f1:.4f}, Val AUC={val_auc:.4f}, \"\n",
    "                        f\"GradNorm={avg_grad_norm:.4f}, LR={optimizer.param_groups[0]['lr']:.2e}\")\n",
    "            \n",
    "            # Debug stagnant metrics\n",
    "            if epoch >= 10:\n",
    "                recent_f1s = val_f1s[-5:]\n",
    "                if len(recent_f1s) == 5 and np.std(recent_f1s) < 1e-6:\n",
    "                    print(f\"⚠️  Validation F1 stagnant for 5 epochs at {val_f1:.6f}\")\n",
    "                    if val_f1 < 0.1:\n",
    "                        print(\"   Consider increasing learning rate or changing architecture\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if patience_counter >= params.get('early_stopping_patience', 10):\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"⚠️  No valid validation predictions at epoch {epoch}\")\n",
    "\n",
    "    # Load best model\n",
    "    if 'best_model_state' in locals():\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"Loaded best model with validation F1: {best_val_f1:.4f}\")\n",
    "\n",
    "    return model, train_losses, val_aucs, val_f1s, training_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7d0c7b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def main_chr_debugged_v2(window_length):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"DEBUGGED Chronologically-Aware Advanced LSTM for Network Intrusion Detection (v2 with Leak Checks)\")\n",
    "    print(\"=\"*70)\n",
    "# ==============================\n",
    "# Usage\n",
    "# ==============================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 Running DEBUGGED LSTM with comprehensive monitoring...\")\n",
    "    print(\"This version includes:\")\n",
    "    print(\"- Enhanced class imbalance handling\")\n",
    "    print(\"- Gradient monitoring\")\n",
    "    print(\"- Output analysis\")\n",
    "    print(\"- Root cause diagnosis\")\n",
    "    print(\"- Specific recommendations\")\n",
    "    \n",
    "    final_model1, X_test, y_test, probs, targets = main_chr_debugged_v2(1)\n",
    "    \n",
    "    if final_model1 is not None:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"TRYING DIFFERENT SEQUENCE LENGTH...\")\n",
    "        print(\"=\"*50)\n",
    "        final_model2, _, _, _, _ = main_chr_debugged_v2(8)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
